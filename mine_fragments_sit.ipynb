{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain: sit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"die\"\n",
    "#delete_main_node(\"numbness\")\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"die\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lda model began\n",
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5Blemma%3D%22die%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=50\n"
     ]
    }
   ],
   "source": [
    "result = topic_concordancer.main(query,window=50,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.044*\"story\" + 0.039*\"talk\" + 0.039*\"talk_about\" + 0.028*\"speak\" + 0.025*\"change\" + 0.024*\"let_'s\" + 0.023*\"learn\" + 0.021*\"reason\" + 0.021*\"Holocaust\" + 0.020*\"very_much\"\n",
      "\n",
      "\n",
      "1\n",
      "0.083*\"friend\" + 0.038*\"ago\" + 0.036*\"school\" + 0.035*\"meet\" + 0.026*\"book\" + 0.026*\"gon_na\" + 0.024*\"love\" + 0.019*\"very_good\" + 0.019*\"America\" + 0.018*\"wonderful\"\n",
      "\n",
      "\n",
      "2\n",
      "0.082*\"jewish\" + 0.064*\"Jews\" + 0.058*\"german\" + 0.034*\"Poland\" + 0.029*\"fight\" + 0.026*\"jew\" + 0.024*\"city\" + 0.023*\"polish\" + 0.018*\"Germans\" + 0.018*\"Hitler\"\n",
      "\n",
      "\n",
      "3\n",
      "0.137*\"day\" + 0.083*\"Auschwitz\" + 0.059*\"train\" + 0.029*\"how_many\" + 0.028*\"arrive\" + 0.027*\"number\" + 0.022*\"transport\" + 0.018*\"wagon\" + 0.018*\"point\" + 0.018*\"Theresienstadt\"\n",
      "\n",
      "\n",
      "4\n",
      "0.146*\"camp\" + 0.043*\"people\" + 0.035*\"typhus\" + 0.028*\"many_people\" + 0.028*\"prisoner\" + 0.025*\"march\" + 0.025*\"most_of\" + 0.023*\"terrible\" + 0.023*\"starvation\" + 0.023*\"liberate\"\n",
      "\n",
      "\n",
      "5\n",
      "0.108*\"life\" + 0.059*\"thing\" + 0.058*\"good\" + 0.057*\"feel\" + 0.042*\"fact\" + 0.037*\"save\" + 0.033*\"understand\" + 0.030*\"bad\" + 0.027*\"go_through\" + 0.023*\"time\"\n",
      "\n",
      "\n",
      "6\n",
      "0.172*\"do_not\" + 0.045*\"cry\" + 0.044*\"night\" + 0.043*\"water\" + 0.039*\"make\" + 0.034*\"God\" + 0.030*\"anymore\" + 0.026*\"sleep\" + 0.019*\"stop\" + 0.017*\"tape\"\n",
      "\n",
      "\n",
      "7\n",
      "0.127*\"could_not\" + 0.038*\"run\" + 0.033*\"stand\" + 0.030*\"sit\" + 0.029*\"open\" + 0.029*\"afraid\" + 0.019*\"catch\" + 0.019*\"beat\" + 0.017*\"place\" + 0.017*\"close\"\n",
      "\n",
      "\n",
      "8\n",
      "0.099*\"man\" + 0.092*\"call\" + 0.068*\"woman\" + 0.051*\"big\" + 0.035*\"guy\" + 0.028*\"ss\" + 0.026*\"long\" + 0.021*\"heart_attack\" + 0.020*\"or_something\" + 0.020*\"suppose\"\n",
      "\n",
      "\n",
      "9\n",
      "0.077*\"back\" + 0.056*\"hear\" + 0.056*\"walk\" + 0.055*\"start\" + 0.042*\"shoot\" + 0.027*\"hand\" + 0.025*\"head\" + 0.023*\"look_at\" + 0.021*\"fall\" + 0.020*\"hold\"\n",
      "\n",
      "\n",
      "10\n",
      "0.399*\"people\" + 0.120*\"kill\" + 0.110*\"a_lot\" + 0.036*\"Germans\" + 0.035*\"street\" + 0.015*\"group\" + 0.014*\"bomb\" + 0.012*\"wood\" + 0.010*\"all_kind\" + 0.009*\"hunger\"\n",
      "\n",
      "\n",
      "11\n",
      "0.173*\"my_mother\" + 0.156*\"my_father\" + 0.102*\"mother\" + 0.098*\"father\" + 0.052*\"bear\" + 0.042*\"my_grandmother\" + 0.039*\"my_grandfather\" + 0.037*\"parent\" + 0.018*\"grandmother\" + 0.017*\"young\"\n",
      "\n",
      "\n",
      "12\n",
      "0.082*\"put\" + 0.080*\"dead\" + 0.067*\"people\" + 0.037*\"body\" + 0.024*\"every_day\" + 0.024*\"morning\" + 0.023*\"barracks\" + 0.023*\"lay\" + 0.017*\"barrack\" + 0.015*\"burn\"\n",
      "\n",
      "\n",
      "13\n",
      "0.129*\"family\" + 0.097*\"brother\" + 0.091*\"sister\" + 0.052*\"my_sister\" + 0.051*\"my_brother\" + 0.048*\"my_father\" + 0.039*\"my_mother\" + 0.032*\"cousin\" + 0.028*\"uncle\" + 0.026*\"my_uncle\"\n",
      "\n",
      "\n",
      "14\n",
      "0.203*\"live\" + 0.186*\"remember\" + 0.080*\"ghetto\" + 0.070*\"house\" + 0.066*\"stay\" + 0.044*\"move\" + 0.020*\"town\" + 0.017*\"apartment\" + 0.015*\"room\" + 0.015*\"play\"\n",
      "\n",
      "\n",
      "15\n",
      "0.156*\"war\" + 0.146*\"survive\" + 0.063*\"picture\" + 0.062*\"Israel\" + 0.046*\"come_back\" + 0.028*\"go_back\" + 0.028*\"concentration_camp\" + 0.026*\"end_of\" + 0.024*\"hide\" + 0.021*\"Russia\"\n",
      "\n",
      "\n",
      "16\n",
      "0.142*\"leave\" + 0.071*\"send\" + 0.064*\"find\" + 0.045*\"Germany\" + 0.045*\"write\" + 0.044*\"find_out\" + 0.030*\"my_parent\" + 0.023*\"letter\" + 0.020*\"later_on\" + 0.019*\"United_States\"\n",
      "\n",
      "\n",
      "17\n",
      "0.068*\"hospital\" + 0.060*\"sick\" + 0.052*\"doctor\" + 0.042*\"girl\" + 0.036*\"home\" + 0.035*\"bring\" + 0.029*\"baby\" + 0.027*\"take_care\" + 0.025*\"over_there\" + 0.024*\"week\"\n",
      "\n",
      "\n",
      "18\n",
      "0.210*\"time\" + 0.082*\"year\" + 0.047*\"alive\" + 0.031*\"kid\" + 0.024*\"half\" + 0.023*\"show\" + 0.020*\"still_alive\" + 0.020*\"month\" + 0.020*\"till\" + 0.018*\"memory\"\n",
      "\n",
      "\n",
      "19\n",
      "0.056*\"thing\" + 0.055*\"of_course\" + 0.047*\"place\" + 0.045*\"bury\" + 0.044*\"people_who\" + 0.042*\"person\" + 0.038*\"try_to\" + 0.032*\"kind_of\" + 0.027*\"sort_of\" + 0.024*\"time\"\n",
      "\n",
      "\n",
      "20\n",
      "0.158*\"give\" + 0.094*\"food\" + 0.082*\"eat\" + 0.041*\"bring\" + 0.040*\"bread\" + 0.029*\"could_not\" + 0.023*\"start\" + 0.020*\"piece_of\" + 0.016*\"little_bit\" + 0.015*\"hungry\"\n",
      "\n",
      "\n",
      "21\n",
      "0.076*\"happen\" + 0.066*\"death\" + 0.045*\"lose\" + 0.033*\"end\" + 0.030*\"forget\" + 0.029*\"hope\" + 0.025*\"do_not\" + 0.022*\"at_least\" + 0.020*\"side\" + 0.018*\"order\"\n",
      "\n",
      "\n",
      "22\n",
      "0.187*\"work\" + 0.102*\"make\" + 0.028*\"money\" + 0.025*\"business\" + 0.024*\"job\" + 0.021*\"home\" + 0.020*\"start\" + 0.018*\"factory\" + 0.017*\"buy\" + 0.014*\"stay\"\n",
      "\n",
      "\n",
      "23\n",
      "0.589*\"do_not\" + 0.123*\"seconds\" + 0.057*\"what_happen\" + 0.022*\"care\" + 0.018*\"happen\" + 0.016*\"point\" + 0.009*\"PAUSES\" + 0.008*\"guess\" + 0.008*\"matter\" + 0.007*\"know_exactly\"\n",
      "\n",
      "\n",
      "24\n",
      "0.182*\"child\" + 0.053*\"wife\" + 0.053*\"marry\" + 0.044*\"husband\" + 0.044*\"son\" + 0.044*\"live\" + 0.034*\"daughter\" + 0.024*\"my_husband\" + 0.024*\"age\" + 0.023*\"young\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"kill\"][]{0,50}[lemma=\"numb\"])|([lemma=\"numb\"][]{0,50}[lemma=\"kill\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=50)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%223%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22Then%22%5D+%5B%22they%22%5D+%5B%22said%22%5D+%5B%5D+%5B%22you%22%5D+%5B%5D+%5B%22out%22%5D+%5B%22of%22%5D+%5B%22luck%22%5D+%5B%22because%22%5D+%5B%22we%22%5D+%5B%5D+%5B%22going%22%5D+%5B%22to%22%5D+%5B%22kill%22%5D+%5B%22you%22%5D+%5B%22all%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%224%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22We%22%5D+%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%224%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22were%22%5D+%5B%22numb%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"men . [ PAUSES FOR 3 SECONDS ] Then they said , you 're out of luck because we 're going to kill you all . [ PAUSES FOR 4 SECONDS ] We [ PAUSES FOR 4 SECONDS ] were numb . \", 'right': '', 'complete_match': \"men . [ PAUSES FOR 3 SECONDS ] Then they said , you 're out of luck because we 're going to kill you all . [ PAUSES FOR 4 SECONDS ] We [ PAUSES FOR 4 SECONDS ] were numb . \", 'testimony_id': 'usc_shoah_27347', 'shelfmark': ['USC 27347'], 'token_start': 10944, 'token_end': 10986}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"\"\n",
    "fragment_1['label']=\"\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22To%22%5D+%5B%22see%22%5D+%5B%22another%22%5D+%5B%5D+%5B%22Jews%22%5D+%5B%22come%22%5D+%5B%22and%22%5D+%5B%22be%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22another%22%5D+%5B%5D+%5B%22be%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22That%22%5D+%5B%22was%22%5D+%5B%22not%22%5D+%5B%5D%7B0%2C50%7D+%5B%22that%22%5D+%5B%22we%22%5D+%5B%22wanted%22%5D+%5B%22to%22%5D+%5B%22see%22%5D+%5B%5D+%5B%22You%22%5D+%5B%22know%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%5D+%5B%22Then%22%5D+%5B%22after%22%5D+%5B%22awhile%22%5D+%5B%22you%22%5D+%5B%22become%22%5D+%5B%22so%22%5D+%5B%22numb%22%5D+%5B%22that%22%5D+%5B%22you%22%5D+%5B%22just%22%5D+%5B%5D%7B0%2C50%7D+%5B%22you%22%5D+%5B%22do%22%5D+%5B%5D+%5B%22react%22%5D+%5B%5D+%5B%22you%22%5D+%5B%5D%7B0%2C50%7D+%5B%22you%22%5D+%5B%22know%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"To see another 10,000 Jews come and be killed , another 20,000 be killed ? That was not ... that we wanted to see . You know . We had to . Then after awhile you become so numb that you just ... you do n't react , you ... you know ? \", 'right': '', 'complete_match': \"To see another 10,000 Jews come and be killed , another 20,000 be killed ? That was not ... that we wanted to see . You know . We had to . Then after awhile you become so numb that you just ... you do n't react , you ... you know ? \", 'testimony_id': 'irn504680', 'shelfmark': ['USHMM RG-50.030*0184'], 'token_start': 9900, 'token_end': 9953}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"\"\n",
    "fragment_2['label']=\"\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22so%22%5D+%5B%22numb%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%5D+%5B%22Cathy%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22every%22%5D+%5B%2210%22%5D+%5B%22people%22%5D+%5B%22get%22%5D+%5B%22killed%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we were so numb , [ ? Cathy ? ] every 10 people get killed . ', 'right': '', 'complete_match': 'And we were so numb , [ ? Cathy ? ] every 10 people get killed . ', 'testimony_id': 'usc_shoah_25287', 'shelfmark': ['USC 25287'], 'token_start': 16040, 'token_end': 16057}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"\"\n",
    "fragment_3['label']=\"\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22from%22%5D+%5B%22people%22%5D+%5B%22being%22%5D+%5B%22wounded%22%5D+%5B%5D+%5B%22from%22%5D+%5B%22people%22%5D+%5B%22being%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22from%22%5D+%5B%22cries%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22finally%22%5D+%5B%5D+%5B%22everything%22%5D+%5B%22subsided%22%5D+%5B%5D+%5B%22But%22%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22alive%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22even%22%5D+%5B%22think%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22numb%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"from people being wounded , from people being killed , from cries . And finally , everything subsided . But I was alive . I did n't -- I did n't even think . I was numb . \", 'right': '', 'complete_match': \"from people being wounded , from people being killed , from cries . And finally , everything subsided . But I was alive . I did n't -- I did n't even think . I was numb . \", 'testimony_id': 'usc_shoah_2590', 'shelfmark': ['USC 2590'], 'token_start': 6362, 'token_end': 6400}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"\"\n",
    "fragment_4['label']= \"\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"\"\n",
    "fragment_5['label']= \"\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"shoot\",\"rape\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"shoot\"][]{0,50}[lemma=\"rape\"])|([lemma=\"rape\"][]{0,50}[lemma=\"shoot\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=50)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"shoot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%223%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22Then%22%5D+%5B%22they%22%5D+%5B%22said%22%5D+%5B%5D+%5B%22you%22%5D+%5B%5D+%5B%22out%22%5D+%5B%22of%22%5D+%5B%22luck%22%5D+%5B%22because%22%5D+%5B%22we%22%5D+%5B%5D+%5B%22going%22%5D+%5B%22to%22%5D+%5B%22kill%22%5D+%5B%22you%22%5D+%5B%22all%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%224%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22We%22%5D+%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%224%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22were%22%5D+%5B%22numb%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"men . [ PAUSES FOR 3 SECONDS ] Then they said , you 're out of luck because we 're going to kill you all . [ PAUSES FOR 4 SECONDS ] We [ PAUSES FOR 4 SECONDS ] were numb . \", 'right': '', 'complete_match': \"men . [ PAUSES FOR 3 SECONDS ] Then they said , you 're out of luck because we 're going to kill you all . [ PAUSES FOR 4 SECONDS ] We [ PAUSES FOR 4 SECONDS ] were numb . \", 'testimony_id': 'usc_shoah_27347', 'shelfmark': ['USC 27347'], 'token_start': 10944, 'token_end': 10986}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"They probably raped them, abused them, eventually shot them because nobody survived\"\n",
    "fragment_1['label']=\"They probably raped them, abused them, eventually shot them because nobody survived\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22To%22%5D+%5B%22see%22%5D+%5B%22another%22%5D+%5B%5D+%5B%22Jews%22%5D+%5B%22come%22%5D+%5B%22and%22%5D+%5B%22be%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22another%22%5D+%5B%5D+%5B%22be%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22That%22%5D+%5B%22was%22%5D+%5B%22not%22%5D+%5B%5D%7B0%2C50%7D+%5B%22that%22%5D+%5B%22we%22%5D+%5B%22wanted%22%5D+%5B%22to%22%5D+%5B%22see%22%5D+%5B%5D+%5B%22You%22%5D+%5B%22know%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%5D+%5B%22Then%22%5D+%5B%22after%22%5D+%5B%22awhile%22%5D+%5B%22you%22%5D+%5B%22become%22%5D+%5B%22so%22%5D+%5B%22numb%22%5D+%5B%22that%22%5D+%5B%22you%22%5D+%5B%22just%22%5D+%5B%5D%7B0%2C50%7D+%5B%22you%22%5D+%5B%22do%22%5D+%5B%5D+%5B%22react%22%5D+%5B%5D+%5B%22you%22%5D+%5B%5D%7B0%2C50%7D+%5B%22you%22%5D+%5B%22know%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"To see another 10,000 Jews come and be killed , another 20,000 be killed ? That was not ... that we wanted to see . You know . We had to . Then after awhile you become so numb that you just ... you do n't react , you ... you know ? \", 'right': '', 'complete_match': \"To see another 10,000 Jews come and be killed , another 20,000 be killed ? That was not ... that we wanted to see . You know . We had to . Then after awhile you become so numb that you just ... you do n't react , you ... you know ? \", 'testimony_id': 'irn504680', 'shelfmark': ['USHMM RG-50.030*0184'], 'token_start': 9900, 'token_end': 9953}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"They picked the nice looking girls, they raped them and then they shot them.\"\n",
    "fragment_2['label']=\"They picked the nice looking girls, they raped them and then they shot them.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22so%22%5D+%5B%22numb%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%5D+%5B%22Cathy%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22every%22%5D+%5B%2210%22%5D+%5B%22people%22%5D+%5B%22get%22%5D+%5B%22killed%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we were so numb , [ ? Cathy ? ] every 10 people get killed . ', 'right': '', 'complete_match': 'And we were so numb , [ ? Cathy ? ] every 10 people get killed . ', 'testimony_id': 'usc_shoah_25287', 'shelfmark': ['USC 25287'], 'token_start': 16040, 'token_end': 16057}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"was raped and shot in the middle of the street.\"\n",
    "fragment_3['label']=\"(..) name was Hannah Prusak, P-R-U-S-A-K-- was raped and shot in the middle of the street.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22from%22%5D+%5B%22people%22%5D+%5B%22being%22%5D+%5B%22wounded%22%5D+%5B%5D+%5B%22from%22%5D+%5B%22people%22%5D+%5B%22being%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22from%22%5D+%5B%22cries%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22finally%22%5D+%5B%5D+%5B%22everything%22%5D+%5B%22subsided%22%5D+%5B%5D+%5B%22But%22%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22alive%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22even%22%5D+%5B%22think%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22numb%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"from people being wounded , from people being killed , from cries . And finally , everything subsided . But I was alive . I did n't -- I did n't even think . I was numb . \", 'right': '', 'complete_match': \"from people being wounded , from people being killed , from cries . And finally , everything subsided . But I was alive . I did n't -- I did n't even think . I was numb . \", 'testimony_id': 'usc_shoah_2590', 'shelfmark': ['USC 2590'], 'token_start': 6362, 'token_end': 6400}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"I know she was raped. I know that they put a big stick through her genitals. I know that they tortured her before they shot her. \"\n",
    "fragment_4['label']= \"I know she was raped. (.. ) I know that they tortured her before they shot her.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"kill\"][]{0,50}[lemma=\"numb\"])|([lemma=\"numb\"][]{0,50}[lemma=\"kill\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=50)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%223%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22Then%22%5D+%5B%22they%22%5D+%5B%22said%22%5D+%5B%5D+%5B%22you%22%5D+%5B%5D+%5B%22out%22%5D+%5B%22of%22%5D+%5B%22luck%22%5D+%5B%22because%22%5D+%5B%22we%22%5D+%5B%5D+%5B%22going%22%5D+%5B%22to%22%5D+%5B%22kill%22%5D+%5B%22you%22%5D+%5B%22all%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%224%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22We%22%5D+%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%224%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22were%22%5D+%5B%22numb%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"men . [ PAUSES FOR 3 SECONDS ] Then they said , you 're out of luck because we 're going to kill you all . [ PAUSES FOR 4 SECONDS ] We [ PAUSES FOR 4 SECONDS ] were numb . \", 'right': '', 'complete_match': \"men . [ PAUSES FOR 3 SECONDS ] Then they said , you 're out of luck because we 're going to kill you all . [ PAUSES FOR 4 SECONDS ] We [ PAUSES FOR 4 SECONDS ] were numb . \", 'testimony_id': 'usc_shoah_27347', 'shelfmark': ['USC 27347'], 'token_start': 10944, 'token_end': 10986}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"\"\n",
    "fragment_1['label']=\"\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22To%22%5D+%5B%22see%22%5D+%5B%22another%22%5D+%5B%5D+%5B%22Jews%22%5D+%5B%22come%22%5D+%5B%22and%22%5D+%5B%22be%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22another%22%5D+%5B%5D+%5B%22be%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22That%22%5D+%5B%22was%22%5D+%5B%22not%22%5D+%5B%5D%7B0%2C50%7D+%5B%22that%22%5D+%5B%22we%22%5D+%5B%22wanted%22%5D+%5B%22to%22%5D+%5B%22see%22%5D+%5B%5D+%5B%22You%22%5D+%5B%22know%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%5D+%5B%22Then%22%5D+%5B%22after%22%5D+%5B%22awhile%22%5D+%5B%22you%22%5D+%5B%22become%22%5D+%5B%22so%22%5D+%5B%22numb%22%5D+%5B%22that%22%5D+%5B%22you%22%5D+%5B%22just%22%5D+%5B%5D%7B0%2C50%7D+%5B%22you%22%5D+%5B%22do%22%5D+%5B%5D+%5B%22react%22%5D+%5B%5D+%5B%22you%22%5D+%5B%5D%7B0%2C50%7D+%5B%22you%22%5D+%5B%22know%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"To see another 10,000 Jews come and be killed , another 20,000 be killed ? That was not ... that we wanted to see . You know . We had to . Then after awhile you become so numb that you just ... you do n't react , you ... you know ? \", 'right': '', 'complete_match': \"To see another 10,000 Jews come and be killed , another 20,000 be killed ? That was not ... that we wanted to see . You know . We had to . Then after awhile you become so numb that you just ... you do n't react , you ... you know ? \", 'testimony_id': 'irn504680', 'shelfmark': ['USHMM RG-50.030*0184'], 'token_start': 9900, 'token_end': 9953}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"\"\n",
    "fragment_2['label']=\"\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22so%22%5D+%5B%22numb%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%5D+%5B%22Cathy%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22every%22%5D+%5B%2210%22%5D+%5B%22people%22%5D+%5B%22get%22%5D+%5B%22killed%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we were so numb , [ ? Cathy ? ] every 10 people get killed . ', 'right': '', 'complete_match': 'And we were so numb , [ ? Cathy ? ] every 10 people get killed . ', 'testimony_id': 'usc_shoah_25287', 'shelfmark': ['USC 25287'], 'token_start': 16040, 'token_end': 16057}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"\"\n",
    "fragment_3['label']=\"\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22from%22%5D+%5B%22people%22%5D+%5B%22being%22%5D+%5B%22wounded%22%5D+%5B%5D+%5B%22from%22%5D+%5B%22people%22%5D+%5B%22being%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22from%22%5D+%5B%22cries%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22finally%22%5D+%5B%5D+%5B%22everything%22%5D+%5B%22subsided%22%5D+%5B%5D+%5B%22But%22%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22alive%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22even%22%5D+%5B%22think%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22numb%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"from people being wounded , from people being killed , from cries . And finally , everything subsided . But I was alive . I did n't -- I did n't even think . I was numb . \", 'right': '', 'complete_match': \"from people being wounded , from people being killed , from cries . And finally , everything subsided . But I was alive . I did n't -- I did n't even think . I was numb . \", 'testimony_id': 'usc_shoah_2590', 'shelfmark': ['USC 2590'], 'token_start': 6362, 'token_end': 6400}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"\"\n",
    "fragment_4['label']= \"\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"\"\n",
    "fragment_5['label']= \"\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
