{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"recognize\"\n",
    "#delete_main_node(main_node)\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"recognize\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lda model began\n",
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5Blemma%3D%22recognize%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=20\n",
      "training of gensim corpus began\n",
      "gensim corpus done\n"
     ]
    }
   ],
   "source": [
    "result = topic_concordancer.main(query,window=20,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.483*\"people\" + 0.147*\"of_course\" + 0.110*\"happen\" + 0.085*\"a_lot\" + 0.025*\"find_out\" + 0.022*\"survive\" + 0.016*\"beginning\" + 0.015*\"anymore\" + 0.012*\"kapo\" + 0.011*\"check\"\n",
      "\n",
      "\n",
      "1\n",
      "0.152*\"call\" + 0.109*\"place\" + 0.095*\"guy\" + 0.076*\"give\" + 0.054*\"ghetto\" + 0.048*\"Germany\" + 0.031*\"factory\" + 0.029*\"question\" + 0.026*\"lady\" + 0.026*\"special\"\n",
      "\n",
      "\n",
      "2\n",
      "0.194*\"walk\" + 0.138*\"street\" + 0.101*\"meet\" + 0.081*\"head\" + 0.070*\"stand\" + 0.036*\"long\" + 0.032*\"line\" + 0.029*\"night\" + 0.026*\"go_through\" + 0.023*\"step\"\n",
      "\n",
      "\n",
      "3\n",
      "0.207*\"do_not\" + 0.120*\"war\" + 0.079*\"life\" + 0.061*\"feel\" + 0.061*\"fact\" + 0.029*\"money\" + 0.026*\"most_of\" + 0.025*\"my_own\" + 0.025*\"save\" + 0.024*\"great\"\n",
      "\n",
      "\n",
      "4\n",
      "0.222*\"my_father\" + 0.204*\"my_mother\" + 0.122*\"woman\" + 0.070*\"voice\" + 0.037*\"scream\" + 0.035*\"neighbor\" + 0.032*\"suddenly\" + 0.028*\"sit\" + 0.026*\"business\" + 0.021*\"morning\"\n",
      "\n",
      "\n",
      "5\n",
      "0.244*\"man\" + 0.096*\"give\" + 0.089*\"school\" + 0.065*\"Israel\" + 0.032*\"at_least\" + 0.028*\"state\" + 0.026*\"store\" + 0.025*\"approach\" + 0.024*\"army\" + 0.024*\"United_States\"\n",
      "\n",
      "\n",
      "6\n",
      "0.158*\"live\" + 0.126*\"house\" + 0.124*\"friend\" + 0.081*\"family\" + 0.068*\"brother\" + 0.042*\"boy\" + 0.038*\"good\" + 0.029*\"Warsaw\" + 0.027*\"world\" + 0.023*\"eventually\"\n",
      "\n",
      "\n",
      "7\n",
      "0.327*\"could_not\" + 0.144*\"child\" + 0.111*\"face\" + 0.088*\"come_back\" + 0.082*\"year\" + 0.037*\"my_sister\" + 0.035*\"lose\" + 0.022*\"black\" + 0.019*\"body\" + 0.018*\"couple_of\"\n",
      "\n",
      "\n",
      "8\n",
      "0.105*\"wear\" + 0.079*\"mother\" + 0.075*\"girl\" + 0.073*\"kill\" + 0.045*\"my_sister\" + 0.038*\"daughter\" + 0.038*\"story\" + 0.037*\"shoot\" + 0.034*\"catch\" + 0.033*\"moment\"\n",
      "\n",
      "\n",
      "9\n",
      "0.125*\"put\" + 0.086*\"bring\" + 0.058*\"number\" + 0.050*\"town\" + 0.044*\"prisoner\" + 0.032*\"these_people\" + 0.032*\"over_there\" + 0.031*\"case\" + 0.029*\"escape\" + 0.029*\"order\"\n",
      "\n",
      "\n",
      "10\n",
      "0.282*\"time\" + 0.102*\"talk\" + 0.077*\"stay\" + 0.042*\"must_have\" + 0.038*\"wife\" + 0.033*\"close\" + 0.030*\"one_day\" + 0.029*\"learn\" + 0.025*\"arm\" + 0.022*\"pick_up\"\n",
      "\n",
      "\n",
      "11\n",
      "0.397*\"do_not\" + 0.085*\"train\" + 0.081*\"speak\" + 0.048*\"accent\" + 0.039*\"language\" + 0.035*\"russian\" + 0.030*\"German\" + 0.030*\"Polish\" + 0.029*\"police\" + 0.027*\"cousin\"\n",
      "\n",
      "\n",
      "12\n",
      "0.139*\"picture\" + 0.108*\"find\" + 0.090*\"try_to\" + 0.086*\"send\" + 0.066*\"show\" + 0.043*\"die\" + 0.035*\"survivor\" + 0.031*\"talk_about\" + 0.028*\"danger\" + 0.027*\"my_parent\"\n",
      "\n",
      "\n",
      "13\n",
      "0.334*\"do_not\" + 0.102*\"leave\" + 0.080*\"change\" + 0.046*\"sister\" + 0.041*\"kid\" + 0.039*\"village\" + 0.030*\"good\" + 0.029*\"my_sister\" + 0.026*\"many_year\" + 0.025*\"building\"\n",
      "\n",
      "\n",
      "14\n",
      "0.234*\"Jews\" + 0.105*\"afraid\" + 0.063*\"group\" + 0.046*\"realize\" + 0.032*\"country\" + 0.032*\"able_to\" + 0.027*\"reason\" + 0.027*\"immediately_recognize\" + 0.023*\"suppose\" + 0.021*\"arrest\"\n",
      "\n",
      "\n",
      "15\n",
      "0.201*\"work\" + 0.183*\"make\" + 0.086*\"my_brother\" + 0.085*\"day\" + 0.046*\"today\" + 0.031*\"teacher\" + 0.025*\"Mr.\" + 0.025*\"part_of\" + 0.024*\"government\" + 0.022*\"fear\"\n",
      "\n",
      "\n",
      "16\n",
      "0.096*\"immediately\" + 0.078*\"look_like\" + 0.074*\"father\" + 0.073*\"big\" + 0.061*\"door\" + 0.057*\"eye\" + 0.056*\"open\" + 0.034*\"visit\" + 0.032*\"young\" + 0.032*\"parent\"\n",
      "\n",
      "\n",
      "17\n",
      "0.148*\"back\" + 0.104*\"hair\" + 0.083*\"clothes\" + 0.070*\"dress\" + 0.047*\"recognize_each\" + 0.045*\"shave\" + 0.033*\"clothing\" + 0.031*\"shoe\" + 0.030*\"completely\" + 0.028*\"front\"\n",
      "\n",
      "\n",
      "18\n",
      "0.259*\"jew\" + 0.144*\"thing\" + 0.112*\"Germans\" + 0.069*\"point\" + 0.041*\"Poles\" + 0.027*\"right_away\" + 0.023*\"officer\" + 0.021*\"God\" + 0.021*\"job\" + 0.021*\"trouble\"\n",
      "\n",
      "\n",
      "19\n",
      "0.140*\"camp\" + 0.064*\"uniform\" + 0.055*\"doctor\" + 0.052*\"Auschwitz\" + 0.046*\"ss\" + 0.045*\"window\" + 0.036*\"people_who\" + 0.036*\"soldier\" + 0.036*\"concentration_camp\" + 0.030*\"Russians\"\n",
      "\n",
      "\n",
      "20\n",
      "0.331*\"jewish\" + 0.140*\"people\" + 0.089*\"polish\" + 0.074*\"person\" + 0.066*\"Poland\" + 0.047*\"hide\" + 0.046*\"beard\" + 0.042*\"understand\" + 0.022*\"easy\" + 0.014*\"all_kind\"\n",
      "\n",
      "\n",
      "21\n",
      "0.211*\"do_not\" + 0.103*\"seconds\" + 0.076*\"city\" + 0.072*\"hear\" + 0.063*\"go_back\" + 0.056*\"cry\" + 0.032*\"alive\" + 0.032*\"son\" + 0.031*\"car\" + 0.028*\"come_home\"\n",
      "\n",
      "\n",
      "22\n",
      "0.167*\"german\" + 0.114*\"right_away\" + 0.053*\"stop\" + 0.051*\"write\" + 0.044*\"food\" + 0.043*\"pass\" + 0.038*\"arrive\" + 0.036*\"bread\" + 0.035*\"what_happen\" + 0.033*\"dead\"\n",
      "\n",
      "\n",
      "23\n",
      "0.644*\"do_not\" + 0.212*\"remember\" + 0.016*\"stand\" + 0.014*\"tank\" + 0.013*\"return\" + 0.012*\"four_year\" + 0.012*\"relative\" + 0.010*\"evening\" + 0.010*\"happy\" + 0.009*\"away_from\"\n",
      "\n",
      "\n",
      "24\n",
      "0.172*\"start\" + 0.159*\"look_at\" + 0.076*\"run\" + 0.075*\"home\" + 0.059*\"each_other\" + 0.056*\"town\" + 0.053*\"recognize_each\" + 0.034*\"word\" + 0.025*\"sort_of\" + 0.024*\"beat\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes . Of course , of course . And I helped a lot of people while being there . Somebody recognized me and so on . They came . . . Q : Did you give people jobs when they came ? A \n",
      "\n",
      "\n",
      ": Oh , yeah . Sure , a lot of people , a lot of people from Warsaw . They recognized me , too . And I met a lot of [ INAUDIBLE ] , which I used to belong to \n",
      "\n",
      "\n",
      "when they walked in , the people were pragmatists ( probably meaning that they tried to rationalize events ) and recognized the fact that this had happened . So of course , you had a lot of people come out and \n",
      "\n",
      "\n",
      ". A : Because we had -- we had -- you know , in choosing where to live , you recognize that people are people are people , and that people , being individualistic , have a right in their own \n",
      "\n",
      "\n",
      "not do it anymore -- they persuaded him , maybe you should n't go anymore , because you are being recognized . He approached the -- he took a batch of people , number of people . And he thought that \n",
      "\n",
      "\n",
      "or -- HS : Oh , a lot of Jews . A lot , as I say . Some people recognize me , which I never knew them , that I am Towja Fiszer 's daughter . And they told me \n",
      "\n",
      "\n",
      "was happening to me was happening for the last two years , but nobody knew about this and nobody really recognized this for what it was . Apparently , when it happened like this , later on it was partly explained \n",
      "\n",
      "\n",
      "years old . I had things that other people did n't have . Of course , in retrospect , I recognize now that I was of the fortunate ones , because there were a lot of poor people in Austria , \n",
      "\n",
      "\n",
      "came , we came -- we came to [ ? check this ? ] from Krakow , I did n't recognize it . Was a funny thing happened . I found out later , I asked what happened . I was \n",
      "\n",
      "\n",
      "I waited and pulled my pistol and shoved them to aside and collared them , found out . They had recognized him as a kapo from one of the concentration camps . And uh apparently they considered him a vicious kapo \n",
      "\n",
      "\n",
      "People you knew ? A : People I knew ? Even if people I knew I probably would not have recognized them . People looked very different during the war . They became extremely skinny , they ages , they were \n",
      "\n",
      "\n",
      "I looked to read this , I felt good about that . Uh , at least they finally , they recognized that there was such a terrible thing happened . There is a lot of people did n't want to believe \n",
      "\n",
      "\n",
      "I do n't know if I happened to be lucky to that I meet such people , or maybe they recognized something in me that we respond to each other . So I think it was happening to me . Put \n",
      "\n",
      "\n",
      "you see ? SUBJECT : I -- I saw -- saw -- saw people on the -- some people they recognize me or I -- so only -- I went to the -- to the city hall and I find out \n",
      "\n",
      "\n",
      "were near the window were calling the names of the people whom they knew , you know , that we recognize some people , so there was a lot of calling , come here , you know , your father , \n",
      "\n",
      "\n",
      "story . A lot of people did n't believe it , what 's going on . But that time we recognized that we knew that something like this is happening . INT : Did you know anything about the extermination programs \n",
      "\n",
      "\n",
      "people living in there and they were looking out the window which faced the street and of course when they recognized us , they got scared . They said , \" What ? You return , you survived ? We were \n",
      "\n",
      "\n",
      "were all kinds of people on the boat . And , in fact , there were two people that we recognized from Stutthof ... uh ... that we were both ... they were scared of us and we were scared of \n",
      "\n",
      "\n",
      "I came and people saw me , I went to my sister and she saw me , she did n't recognize me . And this was the worst thing . This was a killer . People could n't survive working a \n",
      "\n",
      "\n",
      "there and seeing the dead people . It was a distant cousin , and a lot of people could n't recognize really . Yeah . INT : So you went and looked at the bodies ? TH : Yes . INT \n",
      "\n",
      "\n",
      "telling them and they found the clothes , they found their glasses , clothes , food , _________ . They recognized that the people were killed . Q : So they found out what happened ? A : They found out \n",
      "\n",
      "\n",
      "sisters and I did not . And of course we spoke the language and a lot of people did n’t recognize us . So we ’d pretend we were gentiles . We led a double life . Q : Had you \n",
      "\n",
      "\n",
      "for what was happening , or what ultimately happened . It ’s not unusual for a civilian population not to recognize the danger points , not to be prepared . I think it would be too much to expect . Q \n",
      "\n",
      "\n",
      ". Dividing people ( Economics ... ) Right , dividing people according to special reasons . And now they are recognizing themselves as a Russian nation . They are trying to condemn Jews , and of course this process of recognition \n",
      "\n",
      "\n",
      "know , they were really Polish people who said no , we are Polish people , and we are not recognizing you as a power . So they send them to the prison . Of course , a lot of them \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"recognize\",\"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"recognize\"][]{0,3}[lemma=\"other\"])|([lemma=\"other\"][]{0,3}[lemma=\"recognize\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=3)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"could not recognize each other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Nobody%22%5D+%5B%22recognized%22%5D+%5B%22each%22%5D+%5B%22other%22%5D+%5B%5D+%5B%22You%22%5D+%5B%22do%22%5D+%5B%5D+%5B%22recognize%22%5D+%5B%22your%22%5D+%5B%22mother%22%5D+%5B%5D+%5B%22Your%22%5D+%5B%22mother%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22recognize%22%5D+%5B%22you%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"Nobody recognized each other . You do n't recognize your mother . Your mother did n't recognize you , \", 'right': '', 'complete_match': \"Nobody recognized each other . You do n't recognize your mother . Your mother did n't recognize you , \", 'testimony_id': 'irn504592', 'shelfmark': ['USHMM RG-50.030*0098'], 'token_start': 3466, 'token_end': 3485}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"Nobody recognized each other. You don't recognize your mother. Your mother didn't recognize you,\"\n",
    "fragment_1['label']=\"Nobody recognized each other. You don't recognize your mother. Your mother didn't recognize you (..).\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22we%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22recognize%22%5D+%5B%22each%22%5D+%5B%22other%22%5D+%5B%22because%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22without%22%5D+%5B%5D%7B0%2C3%7D+%5B%22sisters%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22recognize%22%5D+%5B%22each%22%5D+%5B%22other%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"we did n't recognize each other because we were without -- sisters did n't recognize each other . \", 'right': '', 'complete_match': \"we did n't recognize each other because we were without -- sisters did n't recognize each other . \", 'testimony_id': 'HVT-134', 'shelfmark': ['Fortunoff HVT-134'], 'token_start': 11904, 'token_end': 11922}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"we didn't recognize each other because we were without-- sisters didn't recognize each other.\"\n",
    "fragment_2['label']=\"(..)we didn't recognize each other because we were without-- sisters didn't recognize each other.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22did%22%5D+%5B%22not%22%5D+%5B%22recognize%22%5D+%5B%22each%22%5D+%5B%22other%22%5D+%5B%5D+%5B%22You%22%5D+%5B%22know%22%5D+%5B%5D+%5B%22we%22%5D+%5B%5D+%5B%22just%22%5D+%5B%22standing%22%5D+%5B%22there%22%5D+%5B%22completely%22%5D+%5B%22naked%22%5D+%5B%22with%22%5D+%5B%22bald%22%5D+%5B%22head%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"We did not recognize each other . You know , we 're just standing there completely naked with bald head . \", 'right': '', 'complete_match': \"We did not recognize each other . You know , we 're just standing there completely naked with bald head . \", 'testimony_id': 'usc_shoah_10162', 'shelfmark': ['USC 10162'], 'token_start': 11700, 'token_end': 11721}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"We did not recognize each other. You know, we're just standing there completely naked with bald head.\"\n",
    "fragment_3['label']=\"We did not recognize each other. You know, we're just standing there completely naked with bald head.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22said%22%5D+%5B%22this%22%5D+%5B%22is%22%5D+%5B%22an%22%5D+%5B%22insane%22%5D+%5B%22asylum%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22we%22%5D+%5B%22hardly%22%5D+%5B%22recognized%22%5D+%5B%22each%22%5D+%5B%22other%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we said this is an insane asylum . And we hardly recognized each other . ', 'right': '', 'complete_match': 'And we said this is an insane asylum . And we hardly recognized each other . ', 'testimony_id': 'usc_shoah_1530', 'shelfmark': ['USC 1530'], 'token_start': 9806, 'token_end': 9822}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"And we said this is an insane asylum. And we hardly recognized each other.\"\n",
    "fragment_4['label']= \"And we said this is an insane asylum. And we hardly recognized each other.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22when%22%5D+%5B%22we%22%5D+%5B%22looked%22%5D+%5B%22at%22%5D+%5B%22each%22%5D+%5B%22other%22%5D+%5B%5D+%5B%22we%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22recognize%22%5D+%5B%22each%22%5D+%5B%22other%22%5D+%5B%5D+%5B%22even%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22were%22%5D+%5B%22looking%22%5D+%5B%22so%22%5D+%5B%22strange%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"And when we looked at each other , we could n't recognize each other , even . We were looking so strange . \", 'right': '', 'complete_match': \"And when we looked at each other , we could n't recognize each other , even . We were looking so strange . \", 'testimony_id': 'usc_shoah_26983', 'shelfmark': ['USC 26983'], 'token_start': 19395, 'token_end': 19418}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"And when we looked at each other, we couldn't recognize each other, even. We were looking so strange.\"\n",
    "fragment_5['label']= \"And when we looked at each other, we couldn't recognize each other, even. We were looking so strange.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"recognize\",\"fear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"recognize\"][]{0,10}[lemma=\"fear\"])|([lemma=\"fear\"][]{0,10}[lemma=\"recognize\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"fear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22So%22%5D+%5B%22every%22%5D+%5B%22moment%22%5D+%5B%5D+%5B%22every%22%5D+%5B%22step%22%5D+%5B%22that%22%5D+%5B%22you%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22had%22%5D+%5B%22a%22%5D+%5B%22certain%22%5D+%5B%22fear%22%5D+%5B%22in%22%5D+%5B%22yourself%22%5D+%5B%5D+%5B%22There%22%5D+%5B%22was%22%5D+%5B%22a%22%5D+%5B%22fear%22%5D+%5B%22of%22%5D+%5B%22being%22%5D+%5B%22recognized%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'So every moment , every step that you did , you had a certain fear in yourself . There was a fear of being recognized . ', 'right': '', 'complete_match': 'So every moment , every step that you did , you had a certain fear in yourself . There was a fear of being recognized . ', 'testimony_id': 'irn504823', 'shelfmark': ['USHMM RG-50.030*0329'], 'token_start': 3458, 'token_end': 3484}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"So every moment, every step that you did, you had a certain fear in yourself. There was a fear of being recognized.\"\n",
    "fragment_1['label']=\"So every moment, every step that you did, you had a certain fear in yourself. There was a fear of being recognized.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22the%22%5D+%5B%22fear%22%5D+%5B%22of%22%5D+%5B%22being%22%5D+%5B%22recognized%22%5D+%5B%5D+%5B%22always%22%5D+%5B%22alert%22%5D+%5B%5D+%5B%22always%22%5D+%5B%22listening%22%5D+%5B%22to%22%5D+%5B%22people%22%5D+%5B%22who%22%5D+%5B%22is%22%5D+%5B%22coming%22%5D+%5B%22and%22%5D+%5B%22who%22%5D+%5B%22is%22%5D+%5B%22going%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'the fear of being recognized , always alert , always listening to people who is coming and who is going , ', 'right': '', 'complete_match': 'the fear of being recognized , always alert , always listening to people who is coming and who is going , ', 'testimony_id': 'usc_shoah_19939', 'shelfmark': ['USC 19939'], 'token_start': 19614, 'token_end': 19635}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"the fear of being recognized, always alert, always listening to people who is coming and who is going,\"\n",
    "fragment_2['label']=\"(..)the fear of being recognized, always alert, always listening to people who is coming and who is going(..)\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22was%22%5D+%5B%22always%22%5D+%5B%22in%22%5D+%5B%22fear%22%5D+%5B%22that%22%5D+%5B%22maybe%22%5D+%5B%22somebody%22%5D+%5B%22will%22%5D+%5B%22recognize%22%5D+%5B%22me%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I was always in fear that maybe somebody will recognize me . ', 'right': '', 'complete_match': 'I was always in fear that maybe somebody will recognize me . ', 'testimony_id': 'usc_shoah_5328', 'shelfmark': ['USC 5328'], 'token_start': 12055, 'token_end': 12067}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"I was always in fear that maybe somebody will recognize me.\"\n",
    "fragment_3['label']=\"I was always in fear that maybe somebody will recognize me.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22there%22%5D+%5B%22was%22%5D+%5B%22this%22%5D+%5B%22fear%22%5D+%5B%22that%22%5D+%5B%5D%7B0%2C3%7D+%5B%22that%22%5D+%5B%22I%22%5D+%5B%22might%22%5D+%5B%22be%22%5D+%5B%22recognized%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'there was this fear that -- that I might be recognized ', 'right': '', 'complete_match': 'there was this fear that -- that I might be recognized ', 'testimony_id': 'usc_shoah_5248', 'shelfmark': ['USC 5248'], 'token_start': 13718, 'token_end': 13729}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"there was this fear that-- that I might be recognized \"\n",
    "fragment_4['label']= \"(..) there was this fear that-- that I might be recognized (..).\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22But%22%5D+%5B%22I%22%5D+%5B%22lived%22%5D+%5B%22with%22%5D+%5B%22that%22%5D+%5B%22fear%22%5D+%5B%22all%22%5D+%5B%22the%22%5D+%5B%22time%22%5D+%5B%5D+%5B%22that%22%5D+%5B%22this%22%5D+%5B%22boy%22%5D+%5B%22would%22%5D+%5B%22recognize%22%5D+%5B%22me%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'But I lived with that fear all the time , that this boy would recognize me . ', 'right': '', 'complete_match': 'But I lived with that fear all the time , that this boy would recognize me . ', 'testimony_id': 'usc_shoah_7094', 'shelfmark': ['USC 7094'], 'token_start': 16614, 'token_end': 16631}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"But I lived with that fear all the time, that this boy would recognize me.\"\n",
    "fragment_5['label']= \"But I lived with that fear all the time, that this boy would recognize me.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"recognize\",\"father\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"recognize\"][]{0,10}[lemma=\"father\"])|([lemma=\"father\"][]{0,10}[lemma=\"recognize\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"father\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22cut%22%5D+%5B%22off%22%5D+%5B%22the%22%5D+%5B%22beard%22%5D+%5B%5D+%5B%22cut%22%5D+%5B%22off%22%5D+%5B%22the%22%5D+%5B%22peyos%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22recognize%22%5D+%5B%22my%22%5D+%5B%22father%22%5D+%5B%5D+%5B%22His%22%5D+%5B%22face%22%5D+%5B%22was%22%5D+%5B%22so%22%5D+%5B%22pale%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"And cut off the beard , cut off the peyos , I could n't recognize my father . His face was so pale . \", 'right': '', 'complete_match': \"And cut off the beard , cut off the peyos , I could n't recognize my father . His face was so pale . \", 'testimony_id': 'usc_shoah_6076', 'shelfmark': ['USC 6076'], 'token_start': 15523, 'token_end': 15547}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And cut off the beard, cut off the peyos, I couldn't recognize my father. His face was so pale.\"\n",
    "fragment_1['label']=\"And cut off the beard, cut off the peyos, I couldn't recognize my father. His face was so pale.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22so%22%5D+%5B%22he%22%5D+%5B%22came%22%5D+%5B%22to%22%5D+%5B%22me%22%5D+%5B%22and%22%5D+%5B%22he%22%5D+%5B%22grabbed%22%5D+%5B%22me%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%22barely%22%5D+%5B%22recognize%22%5D+%5B%22my%22%5D+%5B%22father%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'so he came to me and he grabbed me , and I could barely recognize my father . ', 'right': '', 'complete_match': 'so he came to me and he grabbed me , and I could barely recognize my father . ', 'testimony_id': 'irn42014', 'shelfmark': ['USHMM RG-50.030*0584'], 'token_start': 10092, 'token_end': 10110}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"so he came to me and he grabbed me, and I could barely recognize my father.\"\n",
    "fragment_2['label']=\"(..) so he came to me and he grabbed me, and I could barely recognize my father.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22when%22%5D+%5B%22we%22%5D+%5B%22looked%22%5D+%5B%22on%22%5D+%5B%22my%22%5D+%5B%22father%22%5D+%5B%22we%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22recognize%22%5D+%5B%22him%22%5D+%5B%5D+%5B%22He%22%5D+%5B%22looked%22%5D+%5B%22like%22%5D+%5B%22a%22%5D+%5B%22strange%22%5D+%5B%22person%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"And when we looked on my father we did n't recognize him . He looked like a strange person \", 'right': '', 'complete_match': \"And when we looked on my father we did n't recognize him . He looked like a strange person \", 'testimony_id': 'irn504693', 'shelfmark': ['USHMM RG-50.030*0199'], 'token_start': 14717, 'token_end': 14736}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"And when we looked on my father we didn't recognize him. He looked like a strange person\"\n",
    "fragment_3['label']=\"And when we looked on my father we didn't recognize him. He looked like a strange person (..).\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22could%22%5D+%5B%22see%22%5D+%5B%22my%22%5D+%5B%22father%22%5D+%5B%22and%22%5D+%5B%22I%22%5D+%5B%22hardly%22%5D+%5B%22recognized%22%5D+%5B%22my%22%5D+%5B%22father%22%5D+%5B%22because%22%5D+%5B%22he%22%5D+%5B%22looked%22%5D+%5B%22like%22%5D+%5B%5D+%5B%22before%22%5D+%5B%22Auschwitz%22%5D+%5B%22like%22%5D+%5B%22on%22%5D+%5B%22this%22%5D+%5B%22photo%22%5D+%5B%5D+%5B%22he%22%5D+%5B%22looked%22%5D+%5B%22so%22%5D+%5B%22young%22%5D+%5B%5D+%5B%22not%22%5D+%5B%22white%22%5D+%5B%22hair%22%5D+%5B%22or%22%5D+%5B%22nothing%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I could see my father and I hardly recognized my father because he looked like , before Auschwitz like on this photo , he looked so young , not white hair or nothing . ', 'right': '', 'complete_match': 'I could see my father and I hardly recognized my father because he looked like , before Auschwitz like on this photo , he looked so young , not white hair or nothing . ', 'testimony_id': 'irn509199', 'shelfmark': ['USHMM RG-50.233*0117'], 'token_start': 15527, 'token_end': 15561}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"I could see my father and I hardly recognized my father because he looked like, before Auschwitz like on this photo, he looked so young, not white hair or nothing.\"\n",
    "fragment_4['label']= \"I could see my father and I hardly recognized my father because (..) he looked so young, not white hair or nothing.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mid node exists cannot be added\n"
     ]
    }
   ],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
