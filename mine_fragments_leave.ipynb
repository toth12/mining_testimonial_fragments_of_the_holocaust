{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"leave\"\n",
    "delete_main_node(main_node)\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"leave\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lda model began\n",
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5Blemma%3D%22leave%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=20\n",
      "training of gensim corpus began\n",
      "gensim corpus done\n"
     ]
    }
   ],
   "source": [
    "result = topic_concordancer.main(query,window=20,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.399*\"do_not\" + 0.075*\"feel\" + 0.053*\"good\" + 0.034*\"cry\" + 0.025*\"question\" + 0.017*\"free\" + 0.017*\"happy\" + 0.016*\"little_bit\" + 0.016*\"nice\" + 0.013*\"bad\"\n",
      "\n",
      "\n",
      "1\n",
      "0.156*\"live\" + 0.154*\"house\" + 0.113*\"place\" + 0.060*\"move\" + 0.052*\"apartment\" + 0.020*\"my_grandmother\" + 0.014*\"place_where\" + 0.014*\"neighbor\" + 0.013*\"follow\" + 0.013*\"my_grandfather\"\n",
      "\n",
      "\n",
      "2\n",
      "0.121*\"back\" + 0.070*\"bring\" + 0.055*\"picture\" + 0.055*\"hear\" + 0.055*\"come_back\" + 0.047*\"send\" + 0.031*\"left\" + 0.021*\"my_wife\" + 0.021*\"next_day\" + 0.020*\"carry\"\n",
      "\n",
      "\n",
      "3\n",
      "0.162*\"could_not\" + 0.039*\"talk_about\" + 0.039*\"close\" + 0.032*\"tape\" + 0.031*\"anymore\" + 0.031*\"stop\" + 0.031*\"change\" + 0.030*\"stand\" + 0.029*\"let_'s\" + 0.028*\"part_of\"\n",
      "\n",
      "\n",
      "4\n",
      "0.086*\"give\" + 0.072*\"thing\" + 0.065*\"walk\" + 0.065*\"money\" + 0.052*\"start\" + 0.033*\"business\" + 0.021*\"buy\" + 0.021*\"area\" + 0.021*\"passport\" + 0.021*\"sell\"\n",
      "\n",
      "\n",
      "5\n",
      "0.186*\"family\" + 0.173*\"child\" + 0.090*\"friend\" + 0.067*\"life\" + 0.024*\"message\" + 0.023*\"kind_of\" + 0.022*\"experience\" + 0.021*\"save\" + 0.021*\"feeling\" + 0.020*\"hope\"\n",
      "\n",
      "\n",
      "6\n",
      "0.112*\"Poland\" + 0.089*\"decide\" + 0.086*\"Israel\" + 0.039*\"wife\" + 0.039*\"meet\" + 0.036*\"England\" + 0.030*\"Palestine\" + 0.030*\"marry\" + 0.027*\"bear\" + 0.025*\"son\"\n",
      "\n",
      "\n",
      "7\n",
      "0.084*\"find\" + 0.081*\"put\" + 0.040*\"leave_behind\" + 0.036*\"fact\" + 0.023*\"alive\" + 0.018*\"shoe\" + 0.018*\"sick\" + 0.017*\"horse\" + 0.015*\"look_at\" + 0.015*\"line\"\n",
      "\n",
      "\n",
      "8\n",
      "0.142*\"camp\" + 0.034*\"order\" + 0.026*\"march\" + 0.023*\"guard\" + 0.023*\"concentration_camp\" + 0.021*\"soldier\" + 0.021*\"number\" + 0.018*\"small\" + 0.017*\"prisoner\" + 0.016*\"word\"\n",
      "\n",
      "\n",
      "9\n",
      "0.119*\"train\" + 0.073*\"give\" + 0.047*\"food\" + 0.039*\"room\" + 0.031*\"eat\" + 0.029*\"clothes\" + 0.027*\"door\" + 0.027*\"open\" + 0.025*\"car\" + 0.025*\"bread\"\n",
      "\n",
      "\n",
      "10\n",
      "0.091*\"war\" + 0.084*\"school\" + 0.078*\"start\" + 0.074*\"year\" + 0.058*\"Germany\" + 0.037*\"end_of\" + 0.027*\"end\" + 0.022*\"beginning\" + 0.022*\"finish\" + 0.019*\"age\"\n",
      "\n",
      "\n",
      "11\n",
      "0.311*\"time\" + 0.031*\"week\" + 0.027*\"army\" + 0.027*\"month\" + 0.026*\"wait\" + 0.024*\"speak\" + 0.023*\"over_there\" + 0.023*\"till\" + 0.019*\"couple_of\" + 0.017*\"pick_up\"\n",
      "\n",
      "\n",
      "12\n",
      "0.066*\"kill\" + 0.055*\"night\" + 0.052*\"run\" + 0.047*\"group\" + 0.039*\"girl\" + 0.038*\"hide\" + 0.035*\"come_back\" + 0.031*\"shoot\" + 0.027*\"village\" + 0.026*\"sit\"\n",
      "\n",
      "\n",
      "13\n",
      "0.119*\"Jews\" + 0.081*\"german\" + 0.077*\"man\" + 0.061*\"country\" + 0.057*\"woman\" + 0.054*\"jewish\" + 0.026*\"jew\" + 0.021*\"most_of\" + 0.018*\"take_away\" + 0.018*\"Nazis\"\n",
      "\n",
      "\n",
      "14\n",
      "0.070*\"try_to\" + 0.054*\"Russia\" + 0.053*\"able_to\" + 0.047*\"survive\" + 0.044*\"transport\" + 0.040*\"people_who\" + 0.033*\"kid\" + 0.032*\"France\" + 0.028*\"Europe\" + 0.028*\"escape\"\n",
      "\n",
      "\n",
      "15\n",
      "0.357*\"people\" + 0.129*\"ghetto\" + 0.074*\"a_lot\" + 0.045*\"jewish\" + 0.032*\"write\" + 0.021*\"polish\" + 0.020*\"dead\" + 0.019*\"how_many\" + 0.018*\"letter\" + 0.015*\"russian\"\n",
      "\n",
      "\n",
      "16\n",
      "0.212*\"do_not\" + 0.111*\"seconds\" + 0.047*\"my_parent\" + 0.046*\"talk\" + 0.040*\"my_husband\" + 0.031*\"husband\" + 0.028*\"understand\" + 0.021*\"join\" + 0.017*\"guess\" + 0.017*\"turn\"\n",
      "\n",
      "\n",
      "17\n",
      "0.103*\"Germany\" + 0.052*\"United_States\" + 0.041*\"America\" + 0.041*\"arrive\" + 0.039*\"ship\" + 0.034*\"Vienna\" + 0.029*\"boat\" + 0.029*\"papers\" + 0.025*\"New_York\" + 0.024*\"Holland\"\n",
      "\n",
      "\n",
      "18\n",
      "0.142*\"day\" + 0.042*\"big\" + 0.034*\"call\" + 0.033*\"suppose\" + 0.030*\"ss\" + 0.028*\"half\" + 0.026*\"guy\" + 0.026*\"Paris\" + 0.023*\"sort_of\" + 0.022*\"person\"\n",
      "\n",
      "\n",
      "19\n",
      "0.165*\"stay\" + 0.094*\"go_back\" + 0.037*\"of_course\" + 0.035*\"find_out\" + 0.033*\"Berlin\" + 0.032*\"story\" + 0.025*\"reason\" + 0.023*\"later_on\" + 0.022*\"baby\" + 0.020*\"realize\"\n",
      "\n",
      "\n",
      "20\n",
      "0.150*\"make\" + 0.071*\"parent\" + 0.050*\"Auschwitz\" + 0.043*\"what_happen\" + 0.039*\"point\" + 0.034*\"Warsaw\" + 0.031*\"of_course\" + 0.023*\"my_parent\" + 0.018*\"moment\" + 0.017*\"today\"\n",
      "\n",
      "\n",
      "21\n",
      "0.188*\"my_mother\" + 0.175*\"my_father\" + 0.085*\"mother\" + 0.070*\"my_brother\" + 0.067*\"brother\" + 0.066*\"father\" + 0.066*\"die\" + 0.065*\"my_sister\" + 0.060*\"sister\" + 0.008*\"grandmother\"\n",
      "\n",
      "\n",
      "22\n",
      "0.171*\"work\" + 0.069*\"thing\" + 0.042*\"job\" + 0.030*\"hospital\" + 0.023*\"factory\" + 0.021*\"Hitler\" + 0.021*\"doctor\" + 0.020*\"right_away\" + 0.020*\"begin\" + 0.016*\"hard\"\n",
      "\n",
      "\n",
      "23\n",
      "0.183*\"remember\" + 0.149*\"do_not\" + 0.089*\"happen\" + 0.049*\"morning\" + 0.023*\"early\" + 0.023*\"visit\" + 0.021*\"come_home\" + 0.018*\"evening\" + 0.016*\"must_have\" + 0.014*\"at_home\"\n",
      "\n",
      "\n",
      "24\n",
      "0.138*\"home\" + 0.103*\"Germans\" + 0.078*\"town\" + 0.047*\"city\" + 0.045*\"Russians\" + 0.040*\"call\" + 0.034*\"afraid\" + 0.026*\"run_away\" + 0.025*\"jf\" + 0.023*\"one_day\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"leave\",\"family\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"leave\"][]{0,10}[lemma=\"family\"])|([lemma=\"family\"][]{0,10}[lemma=\"leave\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"family\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22I%22%5D+%5B%22left%22%5D+%5B%22really%22%5D+%5B%5D%7B0%2C3%7D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22very%22%5D+%5B%22close%22%5D+%5B%22to%22%5D+%5B%22my%22%5D+%5B%22family%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And I left really -- I was very close to my family ', 'right': '', 'complete_match': 'And I left really -- I was very close to my family ', 'testimony_id': 'HVT-29', 'shelfmark': ['Fortunoff HVT-29'], 'token_start': 7224, 'token_end': 7236}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And I left really-- I was very close to my family\"\n",
    "fragment_1['label']=\"And I left really-- I was very close to my family (..).\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22have%22%5D+%5B%22left%22%5D+%5B%22their%22%5D+%5B%5D%7B0%2C3%7D+%5B%22their%22%5D+%5B%22families%22%5D+%5B%22to%22%5D+%5B%5D%7B0%2C3%7D+%5B%22to%22%5D+%5B%22save%22%5D+%5B%22themselves%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22joined%22%5D+%5B%22the%22%5D+%5B%22resistance%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They have left their – their families to – to save themselves , and they joined the resistance . ', 'right': '', 'complete_match': 'They have left their – their families to – to save themselves , and they joined the resistance . ', 'testimony_id': 'irn42018', 'shelfmark': ['USHMM RG-50.030*0586'], 'token_start': 23289, 'token_end': 23308}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"They have left their – their families to – to save themselves, and they joined the resistance.\"\n",
    "fragment_2['label']=\"They have left their – their families to – to save themselves, and they joined the resistance.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22the%22%5D+%5B%22constant%22%5D+%5B%22worry%22%5D+%5B%22about%22%5D+%5B%22the%22%5D+%5B%22family%22%5D+%5B%22left%22%5D+%5B%22behind%22%5D+%5B%22was%22%5D+%5B%22always%22%5D+%5B%22there%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'the constant worry about the family left behind was always there . ', 'right': '', 'complete_match': 'the constant worry about the family left behind was always there . ', 'testimony_id': 'irn504627', 'shelfmark': ['USHMM RG-50.030*0133'], 'token_start': 4955, 'token_end': 4967}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"the constant worry about the family left behind was always there.\"\n",
    "fragment_3['label']=\"(..)the constant worry about the family left behind was always there.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22they%22%5D+%5B%22were%22%5D+%5B%22crying%22%5D+%5B%22like%22%5D+%5B%22little%22%5D+%5B%22babies%22%5D+%5B%5D+%5B%22that%22%5D+%5B%22they%22%5D+%5B%22left%22%5D+%5B%22their%22%5D+%5B%22family%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'they were crying like little babies , that they left their family . ', 'right': '', 'complete_match': 'they were crying like little babies , that they left their family . ', 'testimony_id': 'irn504663', 'shelfmark': ['USHMM RG-50.030*0174'], 'token_start': 6003, 'token_end': 6016}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"they were crying like little babies, that they left their family.\"\n",
    "fragment_4['label']= \"(..)they were crying like little babies, that they left their family.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22a%22%5D+%5B%22lot%22%5D+%5B%22f%22%5D+%5B%22Jews%22%5D+%5B%22could%22%5D+%5B%22be%22%5D+%5B%22saved%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22could%22%5D+%5B%22save%22%5D+%5B%22themselves%22%5D+%5B%22to%22%5D+%5B%22run%22%5D+%5B%22away%22%5D+%5B%5D+%5B%22or%22%5D+%5B%22do%22%5D+%5B%22something%22%5D+%5B%5D+%5B%22but%22%5D+%5B%22they%22%5D+%5B%22would%22%5D+%5B%5D+%5B%22do%22%5D+%5B%22it%22%5D+%5B%22because%22%5D+%5B%22they%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22want%22%5D+%5B%22to%22%5D+%5B%22leave%22%5D+%5B%22their%22%5D+%5B%22families%22%5D+%5B%5D+%5B%22their%22%5D+%5B%22fathers%22%5D+%5B%5D+%5B%22their%22%5D+%5B%22mothers%22%5D+%5B%5D+%5B%22their%22%5D+%5B%22sisters%22%5D+%5B%22and%22%5D+%5B%22brothers%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'a lot f Jews could be saved , they could save themselves to run away , or do something , but they would n’t do it because they did n’t want to leave their families , their fathers , their mothers , their sisters and brothers . ', 'right': '', 'complete_match': 'a lot f Jews could be saved , they could save themselves to run away , or do something , but they would n’t do it because they did n’t want to leave their families , their fathers , their mothers , their sisters and brothers . ', 'testimony_id': 'irn509201', 'shelfmark': ['USHMM RG-50.233*0119'], 'token_start': 23259, 'token_end': 23306}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"a lot f Jews could be saved, they could save themselves to run away, or do something, but they wouldn’t do it because they didn’t want to leave their families, their fathers, their mothers, their sisters and brothers.\"\n",
    "fragment_5['label']= \"(..)to run away, or do something, but they wouldn’t do it because they didn’t want to leave their families (..).\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"leave\",\"cry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"leave\"][]{0,10}[lemma=\"cry\"])|([lemma=\"cry\"][]{0,10}[lemma=\"leave\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"cry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22remember%22%5D+%5B%22crying%22%5D+%5B%22after%22%5D+%5B%22them%22%5D+%5B%22leaving%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22crying%22%5D+%5B%22and%22%5D+%5B%5D%7B0%2C3%7D+%5B%22and%22%5D+%5B%22I%22%5D+%5B%22ran%22%5D+%5B%22after%22%5D+%5B%22them%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I remember crying after them leaving , and they were crying and – and I ran after them . ', 'right': '', 'complete_match': 'I remember crying after them leaving , and they were crying and – and I ran after them . ', 'testimony_id': 'irn41496', 'shelfmark': ['USHMM RG-50.030*0547'], 'token_start': 2698, 'token_end': 2717}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"I remember crying after them leaving, and they were crying and – and I ran after them.\"\n",
    "fragment_1['label']=\"I remember crying after them leaving, and they were crying and – and I ran after them.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22the%22%5D+%5B%22only%22%5D+%5B%22time%22%5D+%5B%22I%22%5D+%5B%22have%22%5D+%5B%22ever%22%5D+%5B%22seen%22%5D+%5B%22my%22%5D+%5B%22father%22%5D+%5B%22cry%22%5D+%5B%22except%22%5D+%5B%22for%22%5D+%5B%22when%22%5D+%5B%22I%22%5D+%5B%22left%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'the only time I have ever seen my father cry except for when I left ', 'right': '', 'complete_match': 'the only time I have ever seen my father cry except for when I left ', 'testimony_id': 'irn507583', 'shelfmark': ['USHMM RG-50.106*0086'], 'token_start': 5397, 'token_end': 5412}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"the only time I have ever seen my father cry except for when I left \"\n",
    "fragment_2['label']=\"(..)the only time I have ever seen my father cry except for when I left (..).\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22started%22%5D+%5B%22crying%22%5D+%5B%22tremendously%22%5D+%5B%5D+%5B%22somebody%22%5D+%5B%22from%22%5D+%5B%22the%22%5D+%5B%22family%22%5D+%5B%5D+%5B%22that%22%5D+%5B%22she%22%5D+%5B%5D+%5B%22leaving%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22nobody%22%5D+%5B%22will%22%5D+%5B%22see%22%5D+%5B%22her%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"started crying tremendously , somebody from the family , that she 's leaving , and nobody will see her \", 'right': '', 'complete_match': \"started crying tremendously , somebody from the family , that she 's leaving , and nobody will see her \", 'testimony_id': 'usc_shoah_11144', 'shelfmark': ['USC 11144'], 'token_start': 2934, 'token_end': 2953}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \" started crying tremendously, somebody from the family, that she's leaving, and nobody will see her\"\n",
    "fragment_3['label']=\"(..)started crying tremendously, somebody from the family, that she's leaving, and nobody will see her (..)\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22Mother%22%5D+%5B%22and%22%5D+%5B%22Father%22%5D+%5B%22bravely%22%5D+%5B%22smiled%22%5D+%5B%22at%22%5D+%5B%22us%22%5D+%5B%22until%22%5D+%5B%22the%22%5D+%5B%22time%22%5D+%5B%22came%22%5D+%5B%22for%22%5D+%5B%22the%22%5D+%5B%22train%22%5D+%5B%22to%22%5D+%5B%22leave%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22I%22%5D+%5B%22cried%22%5D+%5B%22impulsively%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And Mother and Father bravely smiled at us until the time came for the train to leave . And I cried impulsively , ', 'right': '', 'complete_match': 'And Mother and Father bravely smiled at us until the time came for the train to leave . And I cried impulsively , ', 'testimony_id': 'usc_shoah_21626', 'shelfmark': ['USC 21626'], 'token_start': 6881, 'token_end': 6904}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"And Mother and Father bravely smiled at us until the time came for the train to leave. And I cried impulsively,\"\n",
    "fragment_4['label']= \"And Mother and Father bravely smiled at us until the time came for the train to leave. And I cried impulsively (..).\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22leaving%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22then%22%5D+%5B%22Laura%22%5D+%5B%22turned%22%5D+%5B%22around%22%5D+%5B%22and%22%5D+%5B%5D+%5B%22crying%22%5D+%5B%5D+%5B%22ran%22%5D+%5B%22again%22%5D+%5B%22into%22%5D+%5B%22his%22%5D+%5B%22arms%22%5D+%5B%5D+%5B%22saying%22%5D+%5B%5D%7B0%2C3%7D+%5B%22kissing%22%5D+%5B%22him%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we were leaving , and then Laura turned around and , crying , ran again into his arms , saying -- kissing him . ', 'right': '', 'complete_match': 'And we were leaving , and then Laura turned around and , crying , ran again into his arms , saying -- kissing him . ', 'testimony_id': 'usc_shoah_7455', 'shelfmark': ['USC 7455'], 'token_start': 20875, 'token_end': 20900}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"And we were leaving, and then Laura turned around and, crying, ran again into his arms, saying-- kissing him.\"\n",
    "fragment_5['label']= \"And we were leaving, and then Laura turned around and, crying, ran again into his arms, saying-- kissing him.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"leave\",\"kill\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"leave\"][]{0,15}[lemma=\"kill\"])|([lemma=\"kill\"][]{0,15}[lemma=\"leave\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=15)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"kill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22All%22%5D+%5B%22the%22%5D+%5B%22Jews%22%5D+%5B%22got%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22The%22%5D+%5B%22Jews%22%5D+%5B%22in%22%5D+%5B%22Norden%22%5D+%5B%5D+%5B%22in%22%5D+%5B%22Holland%22%5D+%5B%5D+%5B%22had%22%5D+%5B%22almost%22%5D+%5B%22nobody%22%5D+%5B%22left%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'All the Jews got killed . The Jews in Norden , in Holland , had almost nobody left . ', 'right': '', 'complete_match': 'All the Jews got killed . The Jews in Norden , in Holland , had almost nobody left . ', 'testimony_id': 'HVT-42', 'shelfmark': ['Fortunoff HVT-42'], 'token_start': 10563, 'token_end': 10582}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"All the Jews got killed. The Jews in Norden, in Holland, had almost nobody left.\"\n",
    "fragment_1['label']=\"All the Jews got killed. The Jews in Norden, in Holland, had almost nobody left.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22The%22%5D+%5B%22one%22%5D+%5B%22who%22%5D+%5B%22had%22%5D+%5B%22in%22%5D+%5B%22Poland%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22all%22%5D+%5B%22been%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22There%22%5D+%5B%22was%22%5D+%5B%22just%22%5D+%5B%22nobody%22%5D+%5B%22left%22%5D+%5B%22but%22%5D+%5B%22me%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'The one who had in Poland , they all been killed . There was just nobody left but me . ', 'right': '', 'complete_match': 'The one who had in Poland , they all been killed . There was just nobody left but me . ', 'testimony_id': 'irn504565', 'shelfmark': ['USHMM RG-50.030*0069'], 'token_start': 448, 'token_end': 468}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"The one who had in Poland, they all been killed. There was just nobody left but me.\"\n",
    "fragment_2['label']=\"(..)they all been killed. There was just nobody left but me.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22So%22%5D+%5B%22from%22%5D+%5B%22all%22%5D+%5B%22our%22%5D+%5B%22group%22%5D+%5B%5D+%5B%22there%22%5D+%5B%22were%22%5D+%5B%22four%22%5D+%5B%22left%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And they were killed . So from all our group , there were four left . ', 'right': '', 'complete_match': 'And they were killed . So from all our group , there were four left . ', 'testimony_id': 'irn504693', 'shelfmark': ['USHMM RG-50.030*0199'], 'token_start': 38370, 'token_end': 38386}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"And they were killed. So from all our group, there were four left.\"\n",
    "fragment_3['label']=\"And they were killed. So from all our group, there were four left.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22there%22%5D+%5B%22was%22%5D+%5B%22nobody%22%5D+%5B%22left%22%5D+%5B%5D+%5B%22Everybody%22%5D+%5B%22was%22%5D+%5B%22killed%22%5D+%5B%22that%22%5D+%5B%22night%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And there was nobody left . Everybody was killed that night . ', 'right': '', 'complete_match': 'And there was nobody left . Everybody was killed that night . ', 'testimony_id': 'usc_shoah_2316', 'shelfmark': ['USC 2316'], 'token_start': 15159, 'token_end': 15171}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"And there was nobody left. Everybody was killed that night.\"\n",
    "fragment_4['label']= \"And there was nobody left. Everybody was killed that night.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22It%22%5D+%5B%22was%22%5D+%5B%22horrible%22%5D+%5B%5D+%5B%22Because%22%5D+%5B%22your%22%5D+%5B%22whole%22%5D+%5B%22family%22%5D+%5B%22was%22%5D+%5B%22killed%22%5D+%5B%22and%22%5D+%5B%22you%22%5D+%5B%22have%22%5D+%5B%22nobody%22%5D+%5B%22left%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'It was horrible . Because your whole family was killed and you have nobody left ', 'right': '', 'complete_match': 'It was horrible . Because your whole family was killed and you have nobody left ', 'testimony_id': 'irn506671', 'shelfmark': ['USHMM RG-50.549.02*0014'], 'token_start': 22428, 'token_end': 22443}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \" It was horrible. Because your whole family was killed and you have nobody left\"\n",
    "fragment_5['label']= \"It was horrible. Because your whole family was killed and you have nobody left (..).\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
