{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"drive\"\n",
    "delete_main_node(main_node)\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[word=\"driven\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lda model began\n",
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5Bword%3D%22driven%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=10\n",
      "training of gensim corpus began\n",
      "gensim corpus done\n"
     ]
    }
   ],
   "source": [
    "result = topic_concordancer.main(query,window=10,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.500*\"street\" + 0.250*\"people\" + 0.250*\"house\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"say\" + 0.000*\"work\" + 0.000*\"like\" + 0.000*\"know\"\n",
      "\n",
      "\n",
      "1\n",
      "0.714*\"german\" + 0.143*\"work\" + 0.071*\"seconds\" + 0.071*\"back\" + 0.000*\"way\" + 0.000*\"a_lot\" + 0.000*\"like\" + 0.000*\"drive_through\" + 0.000*\"know\" + 0.000*\"camp\"\n",
      "\n",
      "\n",
      "2\n",
      "0.889*\"train\" + 0.111*\"Jews\" + 0.000*\"work\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"house\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"say\"\n",
      "\n",
      "\n",
      "3\n",
      "0.609*\"horse\" + 0.348*\"Germans\" + 0.043*\"people\" + 0.000*\"work\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"say\" + 0.000*\"drive_through\" + 0.000*\"see\" + 0.000*\"train\"\n",
      "\n",
      "\n",
      "4\n",
      "0.435*\"school\" + 0.304*\"seconds\" + 0.261*\"work\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"see\" + 0.000*\"say\" + 0.000*\"like\" + 0.000*\"know\"\n",
      "\n",
      "\n",
      "5\n",
      "0.684*\"barracks\" + 0.211*\"ghetto\" + 0.105*\"time\" + 0.000*\"say\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"drive_through\" + 0.000*\"work\" + 0.000*\"like\" + 0.000*\"know\"\n",
      "\n",
      "\n",
      "6\n",
      "1.000*\"horse\" + 0.000*\"say\" + 0.000*\"see\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"house\" + 0.000*\"train\" + 0.000*\"like\" + 0.000*\"work\"\n",
      "\n",
      "\n",
      "7\n",
      "0.742*\"Jews\" + 0.194*\"ghetto\" + 0.032*\"work\" + 0.032*\"train\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"drive_through\" + 0.000*\"say\" + 0.000*\"know\" + 0.000*\"see\"\n",
      "\n",
      "\n",
      "8\n",
      "0.789*\"wagon\" + 0.211*\"call\" + 0.000*\"a_lot\" + 0.000*\"work\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"house\" + 0.000*\"say\" + 0.000*\"like\" + 0.000*\"know\"\n",
      "\n",
      "\n",
      "9\n",
      "0.400*\"a_lot\" + 0.400*\"time\" + 0.150*\"put\" + 0.050*\"school\" + 0.000*\"see\" + 0.000*\"camp\" + 0.000*\"drive_through\" + 0.000*\"say\" + 0.000*\"know\" + 0.000*\"work\"\n",
      "\n",
      "\n",
      "10\n",
      "0.538*\"day\" + 0.462*\"car\" + 0.000*\"say\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"house\" + 0.000*\"like\" + 0.000*\"see\" + 0.000*\"Germans\"\n",
      "\n",
      "\n",
      "11\n",
      "0.815*\"camp\" + 0.111*\"town\" + 0.037*\"seconds\" + 0.037*\"train\" + 0.000*\"a_lot\" + 0.000*\"way\" + 0.000*\"work\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"say\"\n",
      "\n",
      "\n",
      "12\n",
      "0.650*\"remember\" + 0.150*\"people\" + 0.100*\"a_lot\" + 0.100*\"back\" + 0.000*\"camp\" + 0.000*\"know\" + 0.000*\"drive_through\" + 0.000*\"way\" + 0.000*\"work\" + 0.000*\"see\"\n",
      "\n",
      "\n",
      "13\n",
      "0.897*\"people\" + 0.069*\"time\" + 0.034*\"drive_through\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"house\" + 0.000*\"like\" + 0.000*\"see\" + 0.000*\"Germans\"\n",
      "\n",
      "\n",
      "14\n",
      "0.556*\"drive_through\" + 0.222*\"school\" + 0.222*\"house\" + 0.000*\"a_lot\" + 0.000*\"say\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"like\" + 0.000*\"work\" + 0.000*\"know\"\n",
      "\n",
      "\n",
      "15\n",
      "0.667*\"car\" + 0.333*\"truck\" + 0.000*\"say\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"house\" + 0.000*\"work\" + 0.000*\"like\" + 0.000*\"know\"\n",
      "\n",
      "\n",
      "16\n",
      "0.519*\"home\" + 0.407*\"Germany\" + 0.074*\"people\" + 0.000*\"work\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"say\" + 0.000*\"drive_through\" + 0.000*\"like\" + 0.000*\"know\"\n",
      "\n",
      "\n",
      "17\n",
      "1.000*\"truck\" + 0.000*\"say\" + 0.000*\"see\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"house\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"work\"\n",
      "\n",
      "\n",
      "18\n",
      "1.000*\"people\" + 0.000*\"work\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"house\" + 0.000*\"say\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"Germans\"\n",
      "\n",
      "\n",
      "19\n",
      "0.842*\"back\" + 0.105*\"town\" + 0.053*\"put\" + 0.000*\"a_lot\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"work\" + 0.000*\"know\" + 0.000*\"see\" + 0.000*\"say\"\n",
      "\n",
      "\n",
      "20\n",
      "1.000*\"place\" + 0.000*\"a_lot\" + 0.000*\"work\" + 0.000*\"way\" + 0.000*\"camp\" + 0.000*\"house\" + 0.000*\"say\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"Germans\"\n",
      "\n",
      "\n",
      "21\n",
      "0.958*\"do_not\" + 0.042*\"Jews\" + 0.000*\"say\" + 0.000*\"see\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"drive_through\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"work\"\n",
      "\n",
      "\n",
      "22\n",
      "0.889*\"put\" + 0.056*\"work\" + 0.056*\"call\" + 0.000*\"see\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"drive_through\" + 0.000*\"say\" + 0.000*\"know\" + 0.000*\"school\"\n",
      "\n",
      "\n",
      "23\n",
      "0.409*\"town\" + 0.364*\"call\" + 0.227*\"work\" + 0.000*\"a_lot\" + 0.000*\"camp\" + 0.000*\"drive_through\" + 0.000*\"say\" + 0.000*\"like\" + 0.000*\"know\" + 0.000*\"school\"\n",
      "\n",
      "\n",
      "24\n",
      "0.520*\"ghetto\" + 0.400*\"soldier\" + 0.080*\"Germans\" + 0.000*\"say\" + 0.000*\"see\" + 0.000*\"camp\" + 0.000*\"a_lot\" + 0.000*\"drive_through\" + 0.000*\"like\" + 0.000*\"know\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put them everybody on the street , and they were driven to the ghetto . For example , into our house \n",
      "\n",
      "\n",
      "all -- we were taken onto the lorries and were driven out into the street . As I was passing , \n",
      "\n",
      "\n",
      "-- had to be dug by the people who were driven down the street , through -- to the -- to \n",
      "\n",
      "\n",
      "on the street we were reminded of how we were driven as cattle in the gutter under the Nazis . When \n",
      "\n",
      "\n",
      "by the German Nazis , was to guard the people driven out on the street so that they could not move \n",
      "\n",
      "\n",
      "women were taken to work or somewhere , we were driven in an unknown direction , but we knew what street \n",
      "\n",
      "\n",
      "And [ SIGHS ] ' A herd of humans are driven through deserted streets . A spark of hope in our \n",
      "\n",
      "\n",
      "were killed right in the street . And everybody was driven out . INT : And who was with you in \n",
      "\n",
      "\n",
      "the main street in the middle of the street , driven like cattle , with the valises , with the baggages \n",
      "\n",
      "\n",
      "center of the street where the cars and horses were driven , and they started to force us to march to \n",
      "\n",
      "\n",
      "and as I walked with him , Jews were being driven in the streets to the trains , and I bought \n",
      "\n",
      "\n",
      "-- you know , drive -- every time we were driven out of our homes into the streets . And the \n",
      "\n",
      "\n",
      "a dairy product factory . And the -- the horse driven carts came into the house . And we had a \n",
      "\n",
      "\n",
      "Rudnitska , a lot of people were staying already , driven out from the house , and we were staying in \n",
      "\n",
      "\n",
      "house in Hyde Park , and had gone there , driven there , and were just simply turned away . At \n",
      "\n",
      "\n",
      "they -- [ PAUSES FOR 4 SECONDS ] we were driven out of the house . Raus ! Raus ! Nothing \n",
      "\n",
      "\n",
      "from one type of synagogue near the house , were driven with their shawls , or , as we call it \n",
      "\n",
      "\n",
      ". So we came into these houses where they had driven those poor devils out . The wardrobes -- they were \n",
      "\n",
      "\n",
      ". It was called [ NON-ENGLISH ] And we got driven out from these houses -- actually barns -- and all \n",
      "\n",
      "\n",
      "where cases of typhus broke out . And people were driven from those houses to public baths to , to , \n",
      "\n",
      "\n",
      "where they had a plank for the people to be driven up into the boxcars , here was like in a \n",
      "\n",
      "\n",
      "Hungary , from Czechoslovakia , from Romania . People were driven to the -- to the Reich . And so we \n",
      "\n",
      "\n",
      "to get up , because more and more people were driven in . And because otherwise would -- we would be \n",
      "\n",
      "\n",
      "allowed to enter the promised land because the people have driven him to impatience . I mean , it 's difficult \n",
      "\n",
      "\n",
      "people in Dresden react to this , to your being driven to the train station ? HD : Well , we \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"town\",\"drive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"town\"][]{0,10}[lemma=\"drive\"])|([lemma=\"drive\"][]{0,10}[lemma=\"town\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"town\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22we%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22to%22%5D+%5B%22this%22%5D+%5B%22town%22%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22Hungarian%22%5D+%5B%22gendarme%22%5D+%5B%22using%22%5D+%5B%22two%22%5D+%5B%22truncheons%22%5D+%5B%22on%22%5D+%5B%22old%22%5D+%5B%22people%22%5D+%5B%22who%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22walk%22%5D+%5B%22fast%22%5D+%5B%22enough%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"we were driven to this town with the Hungarian gendarme using two truncheons on old people who could n't walk fast enough . \", 'right': '', 'complete_match': \"we were driven to this town with the Hungarian gendarme using two truncheons on old people who could n't walk fast enough . \", 'testimony_id': 'usc_shoah_4361', 'shelfmark': ['USC Shoah Foundation 4361'], 'token_start': 10702, 'token_end': 10725}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"we were driven to this town with the Hungarian gendarme using two truncheons on old people who couldn't walk fast enough.\"\n",
    "fragment_1['label']=\"(..) we were driven to this town with the Hungarian gendarme using two truncheons on old people who couldn't walk fast enough.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22the%22%5D+%5B%22Jews%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22from%22%5D+%5B%22all%22%5D+%5B%22the%22%5D+%5B%22surrounding%22%5D+%5B%22little%22%5D+%5B%22towns%22%5D+%5B%22and%22%5D+%5B%22villages%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'the Jews were driven from all the surrounding little towns and villages ', 'right': '', 'complete_match': 'the Jews were driven from all the surrounding little towns and villages ', 'testimony_id': 'irn508694', 'shelfmark': ['USHMM RG-50.462*0070'], 'token_start': 23436, 'token_end': 23448}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \" the Jews were driven from all the surrounding little towns and villages\"\n",
    "fragment_2['label']=\"(..) the Jews were driven from all the surrounding little towns and villages (..)\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22first%22%5D+%5B%22sense%22%5D+%5B%22of%22%5D+%5B%22brutality%22%5D+%5B%22was%22%5D+%5B%22when%22%5D+%5B%22the%22%5D+%5B%22Russian%22%5D+%5B%22prisoners%22%5D+%5B%22of%22%5D+%5B%22war%22%5D+%5B%22came%22%5D+%5B%22through%22%5D+%5B%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22through%22%5D+%5B%22the%22%5D+%5B%22town%22%5D+%5B%22and%22%5D+%5B%22the%22%5D+%5B%22treatment%22%5D+%5B%22of%22%5D+%5B%22them%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'first sense of brutality was when the Russian prisoners of war came through , were driven through the town and the treatment of them . ', 'right': '', 'complete_match': 'first sense of brutality was when the Russian prisoners of war came through , were driven through the town and the treatment of them . ', 'testimony_id': 'irn504577', 'shelfmark': ['USHMM RG-50.030*0082'], 'token_start': 9657, 'token_end': 9682}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"first sense of brutality was when the Russian prisoners of war came through, were driven through the town and the treatment of them.\"\n",
    "fragment_3['label']=\"(..)first sense of brutality was when the Russian prisoners of war came through, were driven through the town and the treatment of them.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22but%22%5D+%5B%22it%22%5D+%5B%5D+%5B%22enough%22%5D+%5B%22if%22%5D+%5B%22you%22%5D+%5B%22are%22%5D+%5B%22driven%22%5D+%5B%22across%22%5D+%5B%22one%22%5D+%5B%22town%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"but it 's enough if you are driven across one town . \", 'right': '', 'complete_match': \"but it 's enough if you are driven across one town . \", 'testimony_id': 'usc_shoah_19550', 'shelfmark': ['USC Shoah Foundation 19550'], 'token_start': 27102, 'token_end': 27114}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"but it's enough if you are driven across one town.\"\n",
    "fragment_4['label']= \"(..) but it's enough if you are driven across one town.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"train\",\"drive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"train\"][]{0,10}[lemma=\"drive\"])|([lemma=\"drive\"][]{0,10}[lemma=\"train\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"to the train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22that%22%5D+%5B%22morning%22%5D+%5B%5D+%5B%22at%22%5D+%5B%225%22%5D+%5B%22o%27clock%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22SS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22the%22%5D+%5B%22Gestapo%22%5D+%5B%5D%7B0%2C3%7D+%5B%22came%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22drove%22%5D+%5B%22them%22%5D+%5B%22all%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22trains%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"And that morning , at 5 o'clock , the SS -- the Gestapo -- came , and they drove them all to the trains . \", 'right': '', 'complete_match': \"And that morning , at 5 o'clock , the SS -- the Gestapo -- came , and they drove them all to the trains . \", 'testimony_id': 'usc_shoah_1641', 'shelfmark': ['USC Shoah Foundation 1641'], 'token_start': 2593, 'token_end': 2618}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And that morning, at 5 o'clock, the SS-- the Gestapo-- came, and they drove them all to the trains.\"\n",
    "fragment_1['label']=\"And that morning, at 5 o'clock, the SS-- the Gestapo-- came, and they drove them all to the trains.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22gathered%22%5D+%5B%22the%22%5D+%5B%22Jews%22%5D+%5B%22and%22%5D+%5B%22drove%22%5D+%5B%22the%22%5D+%5B%22Jews%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22death%22%5D+%5B%22camps%22%5D+%5B%22and%22%5D+%5B%22drove%22%5D+%5B%22the%22%5D+%5B%22trains%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'gathered the Jews and drove the Jews to the death camps and drove the trains ', 'right': '', 'complete_match': 'gathered the Jews and drove the Jews to the death camps and drove the trains ', 'testimony_id': 'irn506747', 'shelfmark': ['USHMM RG-50.549.02*0020'], 'token_start': 3937, 'token_end': 3952}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"gathered the Jews and drove the Jews to the death camps and drove the trains \"\n",
    "fragment_2['label']=\"(..) gathered the Jews and drove the Jews to the death camps and drove the trains (..)\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22told%22%5D+%5B%22us%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22drove%22%5D+%5B%22us%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22station%22%5D+%5B%22when%22%5D+%5B%22the%22%5D+%5B%22train%22%5D+%5B%22was%22%5D+%5B%22set%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They told us , and they drove us to the station when the train was set . ', 'right': '', 'complete_match': 'They told us , and they drove us to the station when the train was set . ', 'testimony_id': 'irn517852', 'shelfmark': ['USHMM RG-50.030*0500'], 'token_start': 44336, 'token_end': 44353}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"They told us, and they drove us to the station when the train was set.\"\n",
    "fragment_3['label']=\"They told us, and they drove us to the station when the train was set.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22at%22%5D+%5B%22least%22%5D+%5B%2250%22%5D+%5B%22Jewish%22%5D+%5B%22people%22%5D+%5B%5D%7B0%2C3%7D+%5B%22I%22%5D+%5B%22knew%22%5D+%5B%22quite%22%5D+%5B%22a%22%5D+%5B%22few%22%5D+%5B%22of%22%5D+%5B%22them%22%5D+%5B%5D%7B0%2C3%7D+%5B%22who%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22railroad%22%5D+%5B%22station%22%5D+%5B%22from%22%5D+%5B%22where%22%5D+%5B%22they%22%5D+%5B%22took%22%5D+%5B%22the%22%5D+%5B%22train%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'at least 50 Jewish people -- I knew quite a few of them -- who were driven to the railroad station from where they took the train ', 'right': '', 'complete_match': 'at least 50 Jewish people -- I knew quite a few of them -- who were driven to the railroad station from where they took the train ', 'testimony_id': 'usc_shoah_14168', 'shelfmark': ['USC Shoah Foundation 14168'], 'token_start': 8284, 'token_end': 8311}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \" at least 50 Jewish people-- I knew quite a few of them-- who were driven to the railroad station from where they took the train\"\n",
    "fragment_4['label']= \"(..) at least 50 Jewish people (..) who were driven to the railroad station from where they took the train (..)\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22people%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22trains%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22the%22%5D+%5B%22train%22%5D+%5B%22left%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'people were driven into the trains , and the train left , ', 'right': '', 'complete_match': 'people were driven into the trains , and the train left , ', 'testimony_id': 'usc_shoah_27507', 'shelfmark': ['USC Shoah Foundation 27507'], 'token_start': 28375, 'token_end': 28387}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"people were driven into the trains, and the train left, \"\n",
    "fragment_5['label']= \"(..) people were driven into the trains, and the train left (..)\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"street\",\"drive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"street\"][]{0,10}[lemma=\"drive\"])|([lemma=\"drive\"][]{0,10}[lemma=\"street\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"through streets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Jews%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22through%22%5D+%5B%22the%22%5D+%5B%22main%22%5D+%5B%22street%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22Praggerstrasser%22%5D+%5B%22in%22%5D+%5B%22Dresden%22%5D+%5B%5D+%5B%22on%22%5D+%5B%22their%22%5D+%5B%22way%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22station%22%5D+%5B%22to%22%5D+%5B%22be%22%5D+%5B%22taken%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22concentration%22%5D+%5B%22camps%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Jews were driven through the main street , the Praggerstrasser in Dresden , on their way to the station to be taken to the concentration camps ', 'right': '', 'complete_match': 'Jews were driven through the main street , the Praggerstrasser in Dresden , on their way to the station to be taken to the concentration camps ', 'testimony_id': 'irn506688', 'shelfmark': ['USHMM RG-50.030*0455'], 'token_start': 7258, 'token_end': 7284}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"Jews were driven through the main street, the Praggerstrasser in Dresden, on their way to the station to be taken to the concentration camps\"\n",
    "fragment_1['label']=\"(..)Jews were driven through the main street, the Praggerstrasser in Dresden, on their way to the station to be taken to the concentration camps (..)\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22A%22%5D+%5B%22herd%22%5D+%5B%22of%22%5D+%5B%22humans%22%5D+%5B%22are%22%5D+%5B%22driven%22%5D+%5B%22through%22%5D+%5B%22deserted%22%5D+%5B%22streets%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'A herd of humans are driven through deserted streets . ', 'right': '', 'complete_match': 'A herd of humans are driven through deserted streets . ', 'testimony_id': 'usc_shoah_4345', 'shelfmark': ['USC Shoah Foundation 4345'], 'token_start': 26936, 'token_end': 26946}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"A herd of humans are driven through deserted streets.\"\n",
    "fragment_2['label']=\"A herd of humans are driven through deserted streets.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22put%22%5D+%5B%22us%22%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22center%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22street%22%5D+%5B%22where%22%5D+%5B%22the%22%5D+%5B%22cars%22%5D+%5B%22and%22%5D+%5B%22horses%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22started%22%5D+%5B%22to%22%5D+%5B%22force%22%5D+%5B%22us%22%5D+%5B%22to%22%5D+%5B%22march%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22ghetto%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They put us in the center of the street where the cars and horses were driven , and they started to force us to march to the ghetto . ', 'right': '', 'complete_match': 'They put us in the center of the street where the cars and horses were driven , and they started to force us to march to the ghetto . ', 'testimony_id': 'usc_shoah_4361', 'shelfmark': ['USC Shoah Foundation 4361'], 'token_start': 10402, 'token_end': 10431}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"They put us in the center of the street where the cars and horses were driven, and they started to force us to march to the ghetto.\"\n",
    "fragment_3['label']=\"They put us in the center of the street (..) and they started to force us to march to the ghetto.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22people%22%5D+%5B%22who%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22down%22%5D+%5B%22the%22%5D+%5B%22street%22%5D+%5B%5D+%5B%22through%22%5D+%5B%5D%7B0%2C3%7D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%5D%7B0%2C3%7D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22edge%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22forest%22%5D+%5B%22and%22%5D+%5B%22brutally%22%5D+%5B%22murdered%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'people who were driven down the street , through -- to the -- to the edge of the forest and brutally murdered ', 'right': '', 'complete_match': 'people who were driven down the street , through -- to the -- to the edge of the forest and brutally murdered ', 'testimony_id': 'usc_shoah_8916', 'shelfmark': ['USC Shoah Foundation 8916'], 'token_start': 6531, 'token_end': 6553}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"people who were driven down the street, through-- to the-- to the edge of the forest and brutally murdered\"\n",
    "fragment_4['label']= \"The people who were driven down the street, through to the edge of the forest and brutally murdered.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"drive\",\"cattle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"drive\"][]{0,5}[lemma=\"cattle\"])|([lemma=\"cattle\"][]{0,5}[lemma=\"drive\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=5)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"like cattle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Like%22%5D+%5B%22cattle%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22gutter%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Like cattle we were driven into the gutter . ', 'right': '', 'complete_match': 'Like cattle we were driven into the gutter . ', 'testimony_id': 'irn504712', 'shelfmark': ['USHMM RG-50.030*0220'], 'token_start': 1451, 'token_end': 1460}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"Like cattle we were driven into the gutter.\"\n",
    "fragment_1['label']=\"Like cattle we were driven into the gutter.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22the%22%5D+%5B%22aim%22%5D+%5B%22was%22%5D+%5B%22to%22%5D+%5B%22drive%22%5D+%5B%22us%22%5D+%5B%22like%22%5D+%5B%22cattle%22%5D+%5B%22into%22%5D+%5B%22Czernowitz%22%5D+%5B%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22ghetto%22%5D+%5B%5D+%5B%22in%22%5D+%5B%22Czernowitz%22%5D+%5B%22ghetto%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'the aim was to drive us like cattle into Czernowitz , into the ghetto , in Czernowitz ghetto . ', 'right': '', 'complete_match': 'the aim was to drive us like cattle into Czernowitz , into the ghetto , in Czernowitz ghetto . ', 'testimony_id': 'irn508694', 'shelfmark': ['USHMM RG-50.462*0070'], 'token_start': 16280, 'token_end': 16299}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"the aim was to drive us like cattle into Czernowitz, into the ghetto, in Czernowitz ghetto.\"\n",
    "fragment_2['label']=\"(..)the aim was to drive us like cattle into Czernowitz, into the ghetto, in Czernowitz ghetto.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22should%22%5D+%5B%22be%22%5D+%5B%22all%22%5D+%5B%22driven%22%5D+%5B%22away%22%5D+%5B%22like%22%5D+%5B%22cattle%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22know%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They should be all driven away like cattle , you know . ', 'right': '', 'complete_match': 'They should be all driven away like cattle , you know . ', 'testimony_id': 'usc_shoah_1641', 'shelfmark': ['USC Shoah Foundation 1641'], 'token_start': 17266, 'token_end': 17278}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"They should be all driven away like cattle, you know.\"\n",
    "fragment_3['label']=\"They should be all driven away like cattle, you know.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22driven%22%5D+%5B%22like%22%5D+%5B%22cattle%22%5D+%5B%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22valises%22%5D+%5B%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22baggages%22%5D+%5B%5D+%5B%22with%22%5D+%5B%22my%22%5D+%5B%22little%22%5D+%5B%22nieces%22%5D+%5B%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22little%22%5D+%5B%22infant%22%5D+%5B%22who%22%5D+%5B%22was%22%5D+%5B%22five%22%5D+%5B%5D+%5B%22six%22%5D+%5B%22weeks%22%5D+%5B%22old%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'driven like cattle , with the valises , with the baggages , with my little nieces , with the little infant who was five , six weeks old . ', 'right': '', 'complete_match': 'driven like cattle , with the valises , with the baggages , with my little nieces , with the little infant who was five , six weeks old . ', 'testimony_id': 'usc_shoah_4361', 'shelfmark': ['USC Shoah Foundation 4361'], 'token_start': 10539, 'token_end': 10568}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"driven like cattle, with the valises, with the baggages, with my little nieces, with the little infant who was five, six weeks old.\"\n",
    "fragment_4['label']= \"(..)driven like cattle, with the valises, with the baggages, with my little nieces, with the little infant who was five, six weeks old.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22they%22%5D+%5B%5D+%5B%22driven%22%5D+%5B%5D+%5B%22literally%22%5D+%5B%22driven%22%5D+%5B%22like%22%5D+%5B%22cattle%22%5D+%5B%5D+%5B%22from%22%5D+%5B%22the%22%5D+%5B%22Riga%22%5D+%5B%22ghetto%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"they 're driven , literally driven like cattle , from the Riga ghetto \", 'right': '', 'complete_match': \"they 're driven , literally driven like cattle , from the Riga ghetto \", 'testimony_id': 'usc_shoah_9538', 'shelfmark': ['USC Shoah Foundation 9538'], 'token_start': 7263, 'token_end': 7276}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"they're driven, literally driven like cattle, from the Riga ghetto \"\n",
    "fragment_5['label']= \"(..)they're driven, literally driven like cattle, from the Riga ghetto (..).\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"drive\",\"barracks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"drive\"][]{0,50}[lemma=\"barracks\"])|([lemma=\"barracks\"][]{0,50}[lemma=\"drive\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=50)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"barracks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22then%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22marched%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22camp%22%5D+%5B%22itself%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22into%22%5D+%5B%22barracks%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And then we were marched into the camp itself . We were driven into barracks . ', 'right': '', 'complete_match': 'And then we were marched into the camp itself . We were driven into barracks . ', 'testimony_id': 'usc_shoah_2916', 'shelfmark': ['USC Shoah Foundation 2916'], 'token_start': 13107, 'token_end': 13123}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And then we were marched into the camp itself. We were driven into barracks.\"\n",
    "fragment_1['label']=\"And then we were marched into the camp itself. We were driven into barracks.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22back%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22barracks%22%5D+%5B%22he%22%5D+%5B%22was%22%5D+%5B%22behind%22%5D+%5B%22me%22%5D+%5B%5D+%5B%22Because%22%5D+%5B%22when%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22in%22%5D+%5B%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22in%22%5D+%5B%5D+%5B%22literally%22%5D+%5B%22by%22%5D+%5B%5D%7B0%2C3%7D+%5B%22by%22%5D+%5B%22with%22%5D+%5B%22beatings%22%5D+%5B%5D+%5B%22gun%22%5D+%5B%22butts%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'we were driven back into the barracks he was behind me . Because when we were driven in , we were driven in , literally by -- by with beatings , gun butts . ', 'right': '', 'complete_match': 'we were driven back into the barracks he was behind me . Because when we were driven in , we were driven in , literally by -- by with beatings , gun butts . ', 'testimony_id': 'usc_shoah_7069', 'shelfmark': ['USC Shoah Foundation 7069'], 'token_start': 29179, 'token_end': 29213}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"We were driven back into the barracks he was behind me. Because when we were driven in, we were driven in, literally by-- by with beatings, gun butts.\"\n",
    "fragment_2['label']=\"We were driven back into the barracks (..) we were driven in, literally by-- by with beatings, gun butts.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22this%22%5D+%5B%22was%22%5D+%5B%22a%22%5D+%5B%22demoralizing%22%5D+%5B%22process%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22I%22%5D+%5B%22think%22%5D+%5B%22that%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22demoralized%22%5D+%5B%22by%22%5D+%5B%22the%22%5D+%5B%22second%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22mean%22%5D+%5B%22you%22%5D+%5B%22moved%22%5D+%5B%22with%22%5D+%5B%22it%22%5D+%5B%5D+%5B%22it%22%5D+%5B%22was%22%5D+%5B%22as%22%5D+%5B%22though%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22in%22%5D+%5B%22into%22%5D+%5B%22barracks%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'this was a demoralizing process , and I think that we were demoralized by the second , I mean you moved with it , it was as though we were driven in into barracks . ', 'right': '', 'complete_match': 'this was a demoralizing process , and I think that we were demoralized by the second , I mean you moved with it , it was as though we were driven in into barracks . ', 'testimony_id': 'irn505578', 'shelfmark': ['USHMM RG-50.042*0025'], 'token_start': 2578, 'token_end': 2613}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"this was a demoralizing process, and I think that we were demoralized by the second, I mean you moved with it, it was as though we were driven in into barracks.\"\n",
    "fragment_3['label']=\"(..) this was a demoralizing process (..) it was as though we were driven in into barracks.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22drove%22%5D+%5B%22us%22%5D+%5B%22naked%22%5D+%5B%22from%22%5D+%5B%22the%22%5D+%5B%22barracks%22%5D+%5B%22for%22%5D+%5B%22300%22%5D+%5B%22feet%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They drove us naked from the barracks for 300 feet . ', 'right': '', 'complete_match': 'They drove us naked from the barracks for 300 feet . ', 'testimony_id': 'irn507492', 'shelfmark': ['USHMM RG-50.031*0062'], 'token_start': 3212, 'token_end': 3223}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"They drove us naked from the barracks for 300 feet.\"\n",
    "fragment_4['label']= \"They drove us naked from the barracks for 300 feet.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22we%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22into%22%5D+%5B%22a%22%5D+%5B%22barrack%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22we%22%5D+%5B%22came%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22barrack%22%5D+%5B%5D+%5B%22was%22%5D+%5B%22chloride%22%5D+%5B%22up%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22knees%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'we were driven into a barrack . And we came into the barrack , was chloride up to the knees . ', 'right': '', 'complete_match': 'we were driven into a barrack . And we came into the barrack , was chloride up to the knees . ', 'testimony_id': 'usc_shoah_27390', 'shelfmark': ['USC Shoah Foundation 27390'], 'token_start': 26230, 'token_end': 26251}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"we were driven into a barrack. And we came into the barrack, was chloride up to the knees.\"\n",
    "fragment_5['label']= \"(..) we were driven into a barrack. And we came into the barrack, was chloride up to the knees.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"drive\",\"river\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"drive\"][]{0,10}[lemma=\"river\"])|([lemma=\"river\"][]{0,10}[lemma=\"drive\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"river\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22drove%22%5D+%5B%22them%22%5D+%5B%22down%22%5D+%5B%22into%22%5D+%5B%22a%22%5D+%5B%22river%22%5D+%5B%22bank%22%5D+%5B%22where%22%5D+%5B%22they%22%5D+%5B%22could%22%5D+%5B%22control%22%5D+%5B%22that%22%5D+%5B%22you%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22run%22%5D+%5B%22away%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"They drove them down into a river bank where they could control that you could n't run away . \", 'right': '', 'complete_match': \"They drove them down into a river bank where they could control that you could n't run away . \", 'testimony_id': 'irn504718', 'shelfmark': ['USHMM RG-50.030*0228'], 'token_start': 4351, 'token_end': 4370}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"They drove them down into a river bank where they could control that you couldn't run away.\"\n",
    "fragment_1['label']=\"They drove them down into a river bank where they could control that you couldn't run away.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22next%22%5D+%5B%22day%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%5D%7B0%2C3%7D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22chased%22%5D+%5B%22over%22%5D+%5B%22the%22%5D+%5B%22river%22%5D+%5B%22and%22%5D+%5B%22shot%22%5D+%5B%5D+%5B%22All%22%5D+%5B%22the%22%5D+%5B%22people%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And next day , they were driven -- they were chased over the river and shot . All the people . ', 'right': '', 'complete_match': 'And next day , they were driven -- they were chased over the river and shot . All the people . ', 'testimony_id': 'HVT-181', 'shelfmark': ['Fortunoff Archive HVT-181'], 'token_start': 8009, 'token_end': 8030}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"And next day, they were driven-- they were chased over the river and shot. All the people.\"\n",
    "fragment_2['label']=\"(..) they were driven-- they were chased over the river and shot. All the people.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22were%22%5D+%5B%22driven%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22river%22%5D+%5B%22and%22%5D+%5B%22shot%22%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22back%22%5D+%5B%22with%22%5D+%5B%22machine%22%5D+%5B%22guns%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They were driven into the river and shot in the back with machine guns . ', 'right': '', 'complete_match': 'They were driven into the river and shot in the back with machine guns . ', 'testimony_id': 'usc_shoah_945', 'shelfmark': ['USC Shoah Foundation 945'], 'token_start': 10051, 'token_end': 10066}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"They were driven into the river and shot in the back with machine guns.\"\n",
    "fragment_3['label']=\"They were driven into the river and shot in the back with machine guns.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"drive\",\"beat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"drive\"][]{0,10}[lemma=\"beat\"])|([lemma=\"beat\"][]{0,10}[lemma=\"drive\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"beat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22were%22%5D+%5B%22driving%22%5D+%5B%22us%22%5D+%5B%22like%22%5D+%5B%22you%22%5D+%5B%22would%22%5D+%5B%22drive%22%5D+%5B%22a%22%5D+%5B%22sheep%22%5D+%5B%5D+%5B%22animals%22%5D+%5B%5D+%5B%22Beating%22%5D+%5B%22on%22%5D+%5B%22us%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22way%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They were driving us like you would drive a sheep , animals . Beating on us on the way . ', 'right': '', 'complete_match': 'They were driving us like you would drive a sheep , animals . Beating on us on the way . ', 'testimony_id': 'irn504860', 'shelfmark': ['USHMM RG-50.030*0367'], 'token_start': 18268, 'token_end': 18288}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"They were driving us like you would drive a sheep, animals. Beating on us on the way.\"\n",
    "fragment_1['label']=\"They were driving us like you would drive a sheep, animals. Beating on us on the way.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22with%22%5D+%5B%22the%22%5D+%5B%22whip%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22just%22%5D+%5B%22beating%22%5D+%5B%22them%22%5D+%5B%22and%22%5D+%5B%5D%7B0%2C3%7D+%5B%22and%22%5D+%5B%22driving%22%5D+%5B%22them%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'with the whip , they were just beating them and -- and driving them . ', 'right': '', 'complete_match': 'with the whip , they were just beating them and -- and driving them . ', 'testimony_id': 'usc_shoah_182', 'shelfmark': ['USC Shoah Foundation 182'], 'token_start': 5952, 'token_end': 5967}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"with the whip, they were just beating them and-- and driving them.\"\n",
    "fragment_2['label']=\"(..) with the whip, they were just beating them and and driving them.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22they%22%5D+%5B%22were%22%5D+%5B%22being%22%5D+%5B%22driven%22%5D+%5B%22and%22%5D+%5B%22beaten%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22cars%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'they were being driven and beaten into the , into the cars ', 'right': '', 'complete_match': 'they were being driven and beaten into the , into the cars ', 'testimony_id': 'usc_shoah_19550', 'shelfmark': ['USC Shoah Foundation 19550'], 'token_start': 19335, 'token_end': 19347}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"they were being driven and beaten into the, into the cars\"\n",
    "fragment_3['label']=\"(..) they were being driven and beaten into the, into the cars (..)\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22drive%22%5D+%5B%22people%22%5D+%5B%22through%22%5D+%5B%22the%22%5D+%5B%22streets%22%5D+%5B%5D+%5B%22beat%22%5D+%5B%22them%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'drive people through the streets , beat them , ', 'right': '', 'complete_match': 'drive people through the streets , beat them , ', 'testimony_id': 'irn505558', 'shelfmark': ['USHMM RG-50.042*0004'], 'token_start': 384, 'token_end': 393}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"drive people through the streets, beat them,\"\n",
    "fragment_4['label']= \"(..)drive people through the streets, beat them(..)\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22they%22%5D+%5B%22were%22%5D+%5B%22beating%22%5D+%5B%22the%22%5D+%5B%22people%22%5D+%5B%22and%22%5D+%5B%22beating%22%5D+%5B%22them%22%5D+%5B%22with%22%5D+%5B%22sticks%22%5D+%5B%22and%22%5D+%5B%22with%22%5D+%5B%22whips%22%5D+%5B%22and%22%5D+%5B%22driving%22%5D+%5B%22with%22%5D+%5B%22screams%22%5D+%5B%22and%22%5D+%5B%22shouts%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'they were beating the people and beating them with sticks and with whips and driving with screams and shouts ', 'right': '', 'complete_match': 'they were beating the people and beating them with sticks and with whips and driving with screams and shouts ', 'testimony_id': 'usc_shoah_27507', 'shelfmark': ['USC Shoah Foundation 27507'], 'token_start': 26213, 'token_end': 26232}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"they were beating the people and beating them with sticks and with whips and driving with screams and shouts\"\n",
    "fragment_5['label']= \"(..)beating them with sticks and with whips and driving with screams and shouts(..).\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
