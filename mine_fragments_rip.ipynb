{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"rip\"\n",
    "delete_main_node(main_node)\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = '[lemma=\"rip\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result = topic_concordancer.main(query,window=25,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=0\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"rip\",\"child\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"rip\"][]{0,10}[lemma=\"child\"])|([lemma=\"child\"][]{0,10}[lemma=\"rip\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"child\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22one%22%5D+%5B%22German%22%5D+%5B%22taking%22%5D+%5B%22a%22%5D+%5B%22child%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22taking%22%5D+%5B%22him%22%5D+%5B%22by%22%5D+%5B%22his%22%5D+%5B%22legs%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22ripping%22%5D+%5B%22him%22%5D+%5B%22up%22%5D+%5B%22to%22%5D+%5B%22here%22%5D+%5B%22and%22%5D+%5B%22throwing%22%5D+%5B%22against%22%5D+%5B%22the%22%5D+%5B%22wall%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'one German taking a child , and taking him by his legs , and ripping him up to here and throwing against the wall . ', 'right': '', 'complete_match': 'one German taking a child , and taking him by his legs , and ripping him up to here and throwing against the wall . ', 'testimony_id': 'usc_shoah_9995', 'shelfmark': ['USC Shoah Foundation 9995'], 'token_start': 11159, 'token_end': 11184}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"one German taking a child, and taking him by his legs, and ripping him up to here and throwing against the wall.\"\n",
    "fragment_1['label']=\"(..) one German taking a child, and taking him by his legs, and ripping him up to here and throwing against the wall.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22soldiers%22%5D+%5B%22came%22%5D+%5B%22to%22%5D+%5B%22a%22%5D+%5B%22house%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22could%22%5D+%5B%22take%22%5D+%5B%22a%22%5D+%5B%22child%22%5D+%5B%5D+%5B%22Rip%22%5D+%5B%22off%22%5D+%5B%22in%22%5D+%5B%22two%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'soldiers came to a house , they could take a child . Rip off in two ', 'right': '', 'complete_match': 'soldiers came to a house , they could take a child . Rip off in two ', 'testimony_id': 'HVT-81', 'shelfmark': ['Fortunoff Archive HVT-81'], 'token_start': 8562, 'token_end': 8578}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"soldiers came to a house, they could take a child. Rip off in two\"\n",
    "fragment_2['label']=\"(..) soldiers came to a house, they could take a child. Rip off in two (..)\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22see%22%5D+%5B%22people%22%5D+%5B%22lying%22%5D+%5B%22dead%22%5D+%5B%22with%22%5D+%5B%22open%22%5D+%5B%22mouths%22%5D+%5B%5D+%5B%22Children%22%5D+%5B%5D+%5B%22blood%22%5D+%5B%5D+%5B%22flesh%22%5D+%5B%5D+%5B%22pieces%22%5D+%5B%22ripped%22%5D+%5B%22apart%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I see people lying dead with open mouths . Children , blood , flesh , pieces ripped apart . ', 'right': '', 'complete_match': 'I see people lying dead with open mouths . Children , blood , flesh , pieces ripped apart . ', 'testimony_id': 'irn504926', 'shelfmark': ['USHMM RG-50.549.01*0022'], 'token_start': 3557, 'token_end': 3576}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"I see people lying dead with open mouths. Children, blood, flesh, pieces ripped apart.\"\n",
    "fragment_3['label']=\" I see people lying dead with open mouths. Children, blood, flesh, pieces ripped apart.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22That%22%5D+%5B%22she%22%5D+%5B%22has%22%5D+%5B%22a%22%5D+%5B%22little%22%5D+%5B%22baby%22%5D+%5B%5D+%5B%22he%22%5D+%5B%22should%22%5D+%5B%22let%22%5D+%5B%22her%22%5D+%5B%22go%22%5D+%5B%22or%22%5D+%5B%22whatever%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22he%22%5D+%5B%22took%22%5D+%5B%22that%22%5D+%5B%22baby%22%5D+%5B%22and%22%5D+%5B%22ripped%22%5D+%5B%22it%22%5D+%5B%22apart%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'That she has a little baby , he should let her go or whatever . And he took that baby and ripped it apart . ', 'right': '', 'complete_match': 'That she has a little baby , he should let her go or whatever . And he took that baby and ripped it apart . ', 'testimony_id': 'HVT-81', 'shelfmark': ['Fortunoff Archive HVT-81'], 'token_start': 8644, 'token_end': 8669}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"That she has a little baby, he should let her go or whatever. And he took that baby and ripped it apart. \"\n",
    "fragment_4['label']= \"That she has a little baby, he should let her go or whatever. And he took that baby and ripped it apart. \"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22if%22%5D+%5B%22there%22%5D+%5B%22is%22%5D+%5B%22a%22%5D+%5B%22God%22%5D+%5B%5D+%5B%22how%22%5D+%5B%22could%22%5D+%5B%22he%22%5D+%5B%22allow%22%5D+%5B%22a%22%5D+%5B%22little%22%5D+%5B%22tiny%22%5D+%5B%5D%7B0%2C3%7D+%5B%22baby%22%5D+%5B%22being%22%5D+%5B%22ripped%22%5D+%5B%22apart%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'if there is a God , how could he allow a little tiny six-month-old baby being ripped apart ? ', 'right': '', 'complete_match': 'if there is a God , how could he allow a little tiny six-month-old baby being ripped apart ? ', 'testimony_id': 'usc_shoah_17867', 'shelfmark': ['USC Shoah Foundation 17867'], 'token_start': 14704, 'token_end': 14723}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"if there is a God, how could he allow a little tiny six-month-old baby being ripped apart?\"\n",
    "fragment_5['label']= \"(..) if there is a God, how could he allow a little tiny six-month-old baby being ripped apart?\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"clothes\",\"rip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"clothes\"][]{0,10}[lemma=\"rip\"])|([lemma=\"rip\"][]{0,10}[lemma=\"clothes\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"clothes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22took%22%5D+%5B%22the%22%5D+%5B%22clothes%22%5D+%5B%22off%22%5D+%5B%22of%22%5D+%5B%22you%22%5D+%5B%5D+%5B%22They%22%5D+%5B%22ripped%22%5D+%5B%22the%22%5D+%5B%22clothes%22%5D+%5B%22off%22%5D+%5B%22of%22%5D+%5B%22you%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They took the clothes off of you . They ripped the clothes off of you . ', 'right': '', 'complete_match': 'They took the clothes off of you . They ripped the clothes off of you . ', 'testimony_id': 'usc_shoah_7188', 'shelfmark': ['USC Shoah Foundation 7188'], 'token_start': 6684, 'token_end': 6700}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"They took the clothes off of you. They ripped the clothes off of you.\"\n",
    "fragment_1['label']=\"They took the clothes off of you. They ripped the clothes off of you.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22So%22%5D+%5B%22they%22%5D+%5B%22dragged%22%5D+%5B%22me%22%5D+%5B%22out%22%5D+%5B%22and%22%5D+%5B%22ripped%22%5D+%5B%22my%22%5D+%5B%22clothes%22%5D+%5B%22off%22%5D+%5B%22and%22%5D+%5B%22thought%22%5D+%5B%22that%22%5D+%5B%22that%22%5D+%5B%22was%22%5D+%5B%22very%22%5D+%5B%22funny%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'So they dragged me out and ripped my clothes off and thought that that was very funny . ', 'right': '', 'complete_match': 'So they dragged me out and ripped my clothes off and thought that that was very funny . ', 'testimony_id': 'usc_shoah_4284', 'shelfmark': ['USC Shoah Foundation 4284'], 'token_start': 6173, 'token_end': 6191}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"So they dragged me out and ripped my clothes off and thought that that was very funny.\"\n",
    "fragment_2['label']=\"So they dragged me out and ripped my clothes off and thought that that was very funny.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22My%22%5D+%5B%22mother%22%5D+%5B%22was%22%5D+%5B%22attacked%22%5D+%5B%5D+%5B%22too%22%5D+%5B%5D+%5B%22when%22%5D+%5B%22she%22%5D+%5B%22picked%22%5D+%5B%22me%22%5D+%5B%22up%22%5D+%5B%5D+%5B%22They%22%5D+%5B%22ripped%22%5D+%5B%22the%22%5D+%5B%22clothes%22%5D+%5B%22off%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22know%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'My mother was attacked , too , when she picked me up . They ripped the clothes off , you know ? ', 'right': '', 'complete_match': 'My mother was attacked , too , when she picked me up . They ripped the clothes off , you know ? ', 'testimony_id': 'usc_shoah_7188', 'shelfmark': ['USC Shoah Foundation 7188'], 'token_start': 3974, 'token_end': 3996}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"My mother was attacked, too, when she picked me up. They ripped the clothes off, you know?\"\n",
    "fragment_3['label']=\"My mother was attacked, too, when she picked me up. They ripped the clothes off, you know?\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22It%22%5D+%5B%22was%22%5D+%5B%22a%22%5D+%5B%5D%7B0%2C3%7D+%5B%22he%22%5D+%5B%22got%22%5D+%5B%22them%22%5D+%5B%22help%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22ripped%22%5D+%5B%22their%22%5D+%5B%22clothes%22%5D+%5B%22and%22%5D+%5B%22he%22%5D+%5B%22shot%22%5D+%5B%22them%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'It was a -- he got them help , they ripped their clothes and he shot them . ', 'right': '', 'complete_match': 'It was a -- he got them help , they ripped their clothes and he shot them . ', 'testimony_id': 'irn504860', 'shelfmark': ['USHMM RG-50.030*0367'], 'token_start': 14284, 'token_end': 14302}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"It was a -- he got them help, they ripped their clothes and he shot them.\"\n",
    "fragment_4['label']= \"It was a -- he got them help, they ripped their clothes and he shot them.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22The%22%5D+%5B%22dog%22%5D+%5B%22came%22%5D+%5B%22and%22%5D+%5B%22ripped%22%5D+%5B%22up%22%5D+%5B%22the%22%5D+%5B%22clothes%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'The dog came and ripped up the clothes . ', 'right': '', 'complete_match': 'The dog came and ripped up the clothes . ', 'testimony_id': 'HVT-70', 'shelfmark': ['Fortunoff Archive HVT-70'], 'token_start': 7269, 'token_end': 7278}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"The dog came and ripped up the clothes.\"\n",
    "fragment_5['label']= \"The dog came and ripped up the clothes.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"dog\",\"rip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"dog\"][]{0,10}[lemma=\"rip\"])|([lemma=\"rip\"][]{0,10}[lemma=\"dog\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22everybody%22%5D+%5B%22was%22%5D+%5B%22very%22%5D+%5B%22scared%22%5D+%5B%5D+%5B%22The%22%5D+%5B%22dog%22%5D+%5B%22came%22%5D+%5B%22and%22%5D+%5B%22ripped%22%5D+%5B%22up%22%5D+%5B%22the%22%5D+%5B%22clothes%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And everybody was very scared . The dog came and ripped up the clothes . ', 'right': '', 'complete_match': 'And everybody was very scared . The dog came and ripped up the clothes . ', 'testimony_id': 'HVT-70', 'shelfmark': ['Fortunoff Archive HVT-70'], 'token_start': 7263, 'token_end': 7278}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And everybody was very scared. The dog came and ripped up the clothes.\"\n",
    "fragment_1['label']=\"And everybody was very scared. The dog came and ripped up the clothes.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22do%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22do%22%5D+%5B%5D+%5B%22want%22%5D+%5B%22to%22%5D+%5B%22remember%22%5D+%5B%22the%22%5D+%5B%22face%22%5D+%5B%22of%22%5D+%5B%5D%7B0%2C3%7D+%5B%22people%22%5D+%5B%22who%22%5D+%5B%5D+%5B%22on%22%5D+%5B%22our%22%5D+%5B%22appelleplatz%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22man%22%5D+%5B%22who%22%5D+%5B%22sends%22%5D+%5B%22the%22%5D+%5B%22dog%22%5D+%5B%22to%22%5D+%5B%22rip%22%5D+%5B%22me%22%5D+%5B%22apart%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I do n’t -- do n’t want to remember the face of th-the people who , on our appelleplatz , the man who sends the dog to rip me apart . ', 'right': '', 'complete_match': 'I do n’t -- do n’t want to remember the face of th-the people who , on our appelleplatz , the man who sends the dog to rip me apart . ', 'testimony_id': 'irn506730', 'shelfmark': ['USHMM RG-50.549.02*0057'], 'token_start': 8787, 'token_end': 8818}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"I don’t -- don’t want to remember the face of th-the people who, on our appelleplatz, the man who sends the dog to rip me apart.\"\n",
    "fragment_2['label']=\"don’t want to remember the face of the people who, on our appelleplatz, the man who sends the dog to rip me apart.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22they%22%5D+%5B%22were%22%5D+%5B%22thrown%22%5D+%5B%22and%22%5D+%5B%22ripped%22%5D+%5B%22apart%22%5D+%5B%22by%22%5D+%5B%22the%22%5D+%5B%22dogs%22%5D+%5B%5D+%5B%22The%22%5D+%5B%22dogs%22%5D+%5B%22ripped%22%5D+%5B%22them%22%5D+%5B%22apart%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'they were thrown and ripped apart by the dogs . The dogs ripped them apart . ', 'right': '', 'complete_match': 'they were thrown and ripped apart by the dogs . The dogs ripped them apart . ', 'testimony_id': 'usc_shoah_1387', 'shelfmark': ['USC Shoah Foundation 1387'], 'token_start': 6147, 'token_end': 6163}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"they were thrown and ripped apart by the dogs. The dogs ripped them apart.\"\n",
    "fragment_3['label']=\"(..) they were thrown and ripped apart by the dogs. The dogs ripped them apart.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22One%22%5D+%5B%22word%22%5D+%5B%22that%22%5D+%5B%22Gestapo%22%5D+%5B%22said%22%5D+%5B%22to%22%5D+%5B%22that%22%5D+%5B%22dog%22%5D+%5B%5D%7B0%2C3%7D+%5B%22he%22%5D+%5B%22went%22%5D+%5B%22over%22%5D+%5B%22to%22%5D+%5B%22her%22%5D+%5B%5D+%5B%22he%22%5D+%5B%22ripped%22%5D+%5B%22her%22%5D+%5B%22stomach%22%5D+%5B%22open%22%5D+%5B%22and%22%5D+%5B%22ripped%22%5D+%5B%22out%22%5D+%5B%22the%22%5D+%5B%22baby%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'One word that Gestapo said to that dog -- he went over to her , he ripped her stomach open and ripped out the baby ', 'right': '', 'complete_match': 'One word that Gestapo said to that dog -- he went over to her , he ripped her stomach open and ripped out the baby ', 'testimony_id': 'usc_shoah_4345', 'shelfmark': ['USC Shoah Foundation 4345'], 'token_start': 9857, 'token_end': 9882}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"One word that Gestapo said to that dog-- he went over to her, he ripped her stomach open and ripped out the baby\"\n",
    "fragment_4['label']= \"(..) Gestapo said to that dog-- he went over to her, he ripped her stomach open and ripped out the baby (..).\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22they%22%5D+%5B%22sic%22%5D+%5B%22the%22%5D+%5B%22dogs%22%5D+%5B%22on%22%5D+%5B%22them%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22the%22%5D+%5B%22dogs%22%5D+%5B%22rip%22%5D+%5B%22them%22%5D+%5B%22apart%22%5D+%5B%5D+%5B%22limb%22%5D+%5B%22by%22%5D+%5B%22limb%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And they sic the dogs on them . And the dogs rip them apart , limb by limb . ', 'right': '', 'complete_match': 'And they sic the dogs on them . And the dogs rip them apart , limb by limb . ', 'testimony_id': 'usc_shoah_8002', 'shelfmark': ['USC Shoah Foundation 8002'], 'token_start': 13015, 'token_end': 13034}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"And they sic the dogs on them. And the dogs rip them apart, limb by limb.\"\n",
    "fragment_5['label']= \"And they sic the dogs on them. And the dogs rip them apart, limb by limb.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
