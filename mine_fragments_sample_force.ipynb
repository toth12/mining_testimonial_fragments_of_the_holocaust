{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"force\"\n",
    "#delete_main_node(main_node)\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"force\" &pos=\"V.*\"] [lemma!=\"labor\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lda model began\n",
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5Blemma%3D%22force%22+%26pos%3D%22V.%2A%22%5D+%5Blemma%21%3D%22labor%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=20\n",
      "training of gensim corpus began\n",
      "gensim corpus done\n"
     ]
    }
   ],
   "source": [
    "result = topic_concordancer.main(query,window=20,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.444*\"do_not\" + 0.224*\"family\" + 0.158*\"stay\" + 0.092*\"street\" + 0.026*\"go_back\" + 0.020*\"order\" + 0.015*\"understand\" + 0.010*\"start\" + 0.005*\"Poland\" + 0.005*\"class\"\n",
      "\n",
      "\n",
      "1\n",
      "0.508*\"Jews\" + 0.228*\"wear\" + 0.101*\"star\" + 0.069*\"group\" + 0.053*\"hang\" + 0.021*\"what_happen\" + 0.011*\"force_labor\" + 0.005*\"cry\" + 0.005*\"Hitler\" + 0.000*\"allow\"\n",
      "\n",
      "\n",
      "2\n",
      "0.470*\"give\" + 0.131*\"open\" + 0.089*\"food\" + 0.083*\"house\" + 0.048*\"start\" + 0.048*\"order\" + 0.036*\"law\" + 0.024*\"mother\" + 0.018*\"stand\" + 0.018*\"german\"\n",
      "\n",
      "\n",
      "3\n",
      "0.278*\"could_not\" + 0.128*\"join\" + 0.100*\"train\" + 0.094*\"job\" + 0.089*\"find\" + 0.072*\"push\" + 0.056*\"farm\" + 0.056*\"my_sister\" + 0.050*\"one_day\" + 0.022*\"a_lot\"\n",
      "\n",
      "\n",
      "4\n",
      "0.343*\"make\" + 0.172*\"walk\" + 0.157*\"feel\" + 0.071*\"force_labor\" + 0.071*\"good\" + 0.061*\"transport\" + 0.040*\"stand\" + 0.040*\"suppose\" + 0.010*\"people_who\" + 0.010*\"eat\"\n",
      "\n",
      "\n",
      "5\n",
      "0.438*\"time\" + 0.203*\"thing\" + 0.062*\"factory\" + 0.057*\"kind_of\" + 0.057*\"woman\" + 0.052*\"come_back\" + 0.031*\"story\" + 0.021*\"remember\" + 0.021*\"my_parent\" + 0.021*\"a_lot\"\n",
      "\n",
      "\n",
      "6\n",
      "0.254*\"force_laborer\" + 0.234*\"Germany\" + 0.164*\"send\" + 0.090*\"people_who\" + 0.070*\"polish\" + 0.055*\"march\" + 0.050*\"back\" + 0.035*\"sort_of\" + 0.020*\"my_sister\" + 0.015*\"religion\"\n",
      "\n",
      "\n",
      "7\n",
      "0.200*\"live\" + 0.130*\"business\" + 0.130*\"sell\" + 0.098*\"area\" + 0.093*\"Nazis\" + 0.079*\"man\" + 0.079*\"beat\" + 0.047*\"wife\" + 0.037*\"Hitler\" + 0.023*\"beginning\"\n",
      "\n",
      "\n",
      "8\n",
      "0.323*\"march\" + 0.149*\"point\" + 0.097*\"happen\" + 0.072*\"city\" + 0.067*\"guy\" + 0.067*\"Russia\" + 0.062*\"these_people\" + 0.046*\"cry\" + 0.036*\"prisoner\" + 0.031*\"fact\"\n",
      "\n",
      "\n",
      "9\n",
      "0.335*\"move\" + 0.228*\"live\" + 0.146*\"apartment\" + 0.087*\"parent\" + 0.083*\"throw\" + 0.068*\"night\" + 0.034*\"job\" + 0.015*\"hang\" + 0.005*\"hand\" + 0.000*\"sure\"\n",
      "\n",
      "\n",
      "10\n",
      "0.277*\"home\" + 0.145*\"day\" + 0.139*\"eat\" + 0.116*\"life\" + 0.087*\"house\" + 0.064*\"my_brother\" + 0.052*\"room\" + 0.040*\"write\" + 0.029*\"army\" + 0.017*\"stand\"\n",
      "\n",
      "\n",
      "11\n",
      "0.183*\"try_to\" + 0.127*\"people\" + 0.122*\"write\" + 0.112*\"seconds\" + 0.086*\"truck\" + 0.061*\"hear\" + 0.051*\"go_back\" + 0.046*\"dig\" + 0.041*\"polish\" + 0.041*\"story\"\n",
      "\n",
      "\n",
      "12\n",
      "0.315*\"Germans\" + 0.196*\"back\" + 0.071*\"Russians\" + 0.071*\"prisoner\" + 0.065*\"woman\" + 0.065*\"guard\" + 0.060*\"decide\" + 0.054*\"french\" + 0.043*\"force_laborer\" + 0.022*\"war\"\n",
      "\n",
      "\n",
      "13\n",
      "0.754*\"leave\" + 0.106*\"Poland\" + 0.050*\"place\" + 0.050*\"work\" + 0.017*\"french\" + 0.011*\"street\" + 0.006*\"join\" + 0.006*\"cry\" + 0.000*\"three\" + 0.000*\"big\"\n",
      "\n",
      "\n",
      "14\n",
      "0.301*\"my_father\" + 0.181*\"war\" + 0.124*\"house\" + 0.088*\"father\" + 0.078*\"sign\" + 0.073*\"change\" + 0.057*\"catch\" + 0.041*\"understand\" + 0.036*\"begin\" + 0.010*\"clean\"\n",
      "\n",
      "\n",
      "15\n",
      "0.867*\"do_not\" + 0.044*\"time\" + 0.034*\"run\" + 0.025*\"practically\" + 0.015*\"move_into\" + 0.010*\"understand\" + 0.005*\"one_day\" + 0.000*\"together\" + 0.000*\"law\" + 0.000*\"allow\"\n",
      "\n",
      "\n",
      "16\n",
      "0.423*\"ghetto\" + 0.159*\"people\" + 0.101*\"labor\" + 0.074*\"move_into\" + 0.058*\"clean\" + 0.053*\"resign\" + 0.048*\"age\" + 0.026*\"work\" + 0.016*\"put\" + 0.011*\"time\"\n",
      "\n",
      "\n",
      "17\n",
      "0.814*\"work\" + 0.052*\"bring\" + 0.043*\"religion\" + 0.038*\"sort_of\" + 0.019*\"room\" + 0.014*\"street\" + 0.014*\"beginning\" + 0.005*\"write\" + 0.000*\"allow\" + 0.000*\"one_day\"\n",
      "\n",
      "\n",
      "18\n",
      "0.281*\"german\" + 0.216*\"put\" + 0.132*\"kill\" + 0.090*\"position\" + 0.066*\"show\" + 0.060*\"sit\" + 0.048*\"country\" + 0.042*\"practically\" + 0.024*\"Germany\" + 0.018*\"open\"\n",
      "\n",
      "\n",
      "19\n",
      "0.232*\"remember\" + 0.227*\"people\" + 0.141*\"place\" + 0.086*\"big\" + 0.081*\"bring\" + 0.070*\"man\" + 0.059*\"watch\" + 0.049*\"army\" + 0.027*\"what_happen\" + 0.011*\"my_parent\"\n",
      "\n",
      "\n",
      "20\n",
      "0.275*\"school\" + 0.166*\"jewish\" + 0.114*\"friend\" + 0.078*\"jew\" + 0.052*\"number\" + 0.052*\"girl\" + 0.052*\"change\" + 0.052*\"mother\" + 0.047*\"fact\" + 0.041*\"kid\"\n",
      "\n",
      "\n",
      "21\n",
      "0.235*\"child\" + 0.209*\"of_course\" + 0.118*\"talk\" + 0.102*\"ss\" + 0.091*\"later_on\" + 0.059*\"side\" + 0.053*\"gun\" + 0.043*\"law\" + 0.037*\"school\" + 0.016*\"country\"\n",
      "\n",
      "\n",
      "22\n",
      "0.243*\"jewish\" + 0.097*\"close\" + 0.092*\"call\" + 0.081*\"government\" + 0.081*\"start\" + 0.081*\"russian\" + 0.070*\"store\" + 0.059*\"volunteer\" + 0.059*\"money\" + 0.032*\"person\"\n",
      "\n",
      "\n",
      "23\n",
      "0.293*\"camp\" + 0.220*\"my_mother\" + 0.131*\"learn\" + 0.079*\"speak\" + 0.073*\"hand\" + 0.068*\"part_of\" + 0.063*\"language\" + 0.026*\"my_parent\" + 0.016*\"Germans\" + 0.010*\"my_brother\"\n",
      "\n",
      "\n",
      "24\n",
      "0.422*\"people\" + 0.139*\"town\" + 0.102*\"watch\" + 0.096*\"try_to\" + 0.075*\"situation\" + 0.048*\"class\" + 0.037*\"clean\" + 0.027*\"force_labor\" + 0.021*\"make\" + 0.016*\"begin\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"force\"] []{0,16} [lemma=\"watch\"] []{0,15} [lemma=\"hang.*\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"have\"] [\"to\"] [lemma=\"watch\"] []{0,15} [lemma=\"hang.*\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"watch hanging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22there%22%5D+%5B%22was%22%5D+%5B%22a%22%5D+%5B%22central%22%5D+%5B%22place%22%5D+%5B%22where%22%5D+%5B%22we%22%5D+%5B%22all%22%5D+%5B%22were%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22gather%22%5D+%5B%22and%22%5D+%5B%22to%22%5D+%5B%22watch%22%5D+%5B%22the%22%5D+%5B%22hangings%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'there was a central place where we all were forced to gather and to watch the hangings . ', 'right': '', 'complete_match': 'there was a central place where we all were forced to gather and to watch the hangings . ', 'testimony_id': 'irn504408', 'shelfmark': ['USHMM RG-50.030*0203'], 'token_start': 10874, 'token_end': 10892}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"there was a central place where we all were forced to gather and to watch the hangings.\"\n",
    "fragment_1['label']=\"(..) there was a central place where we all were forced to gather and to watch the hangings.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22know%22%5D+%5B%5D+%5B%22most%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22people%22%5D+%5B%22that%22%5D+%5B%22tried%22%5D+%5B%22to%22%5D+%5B%22run%22%5D+%5B%22away%22%5D+%5B%22were%22%5D+%5B%22hanged%22%5D+%5B%22the%22%5D+%5B%22next%22%5D+%5B%22day%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22did%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22and%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22watch%22%5D+%5B%22the%22%5D+%5B%22hanging%22%5D+%5B%5D+%5B%22of%22%5D+%5B%22course%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And , you know , most of the people that tried to run away were hanged the next day . We did n’t -- and we were forced to watch the hanging , of course . ', 'right': '', 'complete_match': 'And , you know , most of the people that tried to run away were hanged the next day . We did n’t -- and we were forced to watch the hanging , of course . ', 'testimony_id': 'irn507287', 'shelfmark': ['USHMM RG-50.030*0397'], 'token_start': 21267, 'token_end': 21303}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"And, you know, most of the people that tried to run away were hanged the next day. We didn’t -- and we were forced to watch the hanging, of course.\"\n",
    "fragment_2['label']=\"(..)people that tried to run away were hanged the next day. (..)we were forced to watch the hanging(..).\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22my%22%5D+%5B%22brother%22%5D+%5B%22and%22%5D+%5B%22two%22%5D+%5B%22of%22%5D+%5B%22his%22%5D+%5B%22friends%22%5D+%5B%22were%22%5D+%5B%22hanged%22%5D+%5B%22in%22%5D+%5B%22front%22%5D+%5B%22of%22%5D+%5B%5D+%5B%22prisoners%22%5D+%5B%22who%22%5D+%5B%22were%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22stand%22%5D+%5B%22there%22%5D+%5B%22and%22%5D+%5B%22to%22%5D+%5B%22watch%22%5D+%5B%22the%22%5D+%5B%22show%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'my brother and two of his friends were hanged in front of 15,000 prisoners who were forced to stand there and to watch the show ', 'right': '', 'complete_match': 'my brother and two of his friends were hanged in front of 15,000 prisoners who were forced to stand there and to watch the show ', 'testimony_id': 'usc_shoah_17135', 'shelfmark': ['USC 17135'], 'token_start': 47576, 'token_end': 47601}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"my brother and two of his friends were hanged in front of 15,000 prisoners who were forced to stand there and to watch the show\"\n",
    "fragment_3['label']=\"(..)my brother and two of his friends were hanged in front of 15,000 prisoners who were forced to stand there and to watch the show(..).\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22he%22%5D+%5B%22and%22%5D+%5B%22his%22%5D+%5B%22two%22%5D+%5B%22sons%22%5D+%5B%22were%22%5D+%5B%22hung%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'he and his two sons were hung ', 'right': '', 'complete_match': 'he and his two sons were hung ', 'testimony_id': 'irn504725', 'shelfmark': ['USHMM RG-50.030*0236'], 'token_start': 7219, 'token_end': 7226}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"he and his two sons were hung\"\n",
    "fragment_4['label']= \"(..)he and his two sons were hung [hanged] and I remember that we were all forced to go and watch it.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22all%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22watch%22%5D+%5B%22by%22%5D+%5B%22the%22%5D+%5B%22appell%22%5D+%5B%22how%22%5D+%5B%22the%22%5D+%5B%22man%22%5D+%5B%22was%22%5D+%5B%22hanged%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'We all had to watch by the appell how the man was hanged . ', 'right': '', 'complete_match': 'We all had to watch by the appell how the man was hanged . ', 'testimony_id': 'irn504693', 'shelfmark': ['USHMM RG-50.030*0199'], 'token_start': 7361, 'token_end': 7375}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"We all had to watch by the appell how the man was hanged.\"\n",
    "fragment_5['label']= \"We all had to watch by the appell how the man was hanged.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"force\",\"move\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"force\"][]{0,10}[lemma=\"move\"])|([lemma=\"move\"][]{0,10}[lemma=\"force\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"leave home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22uh%22%5D+%5B%22as%22%5D+%5B%22I%22%5D+%5B%22told%22%5D+%5B%22you%22%5D+%5B%5D+%5B%22my%22%5D+%5B%22my%22%5D+%5B%22garden%22%5D+%5B%22hold%22%5D+%5B%22held%22%5D+%5B%22most%22%5D+%5B%22of%22%5D+%5B%22my%22%5D+%5B%22wonderful%22%5D+%5B%22childhood%22%5D+%5B%22memories%22%5D+%5B%22and%22%5D+%5B%22uh%22%5D+%5B%22fairly%22%5D+%5B%22soon%22%5D+%5B%5D%7B0%2C50%7D+%5B%22I%22%5D+%5B%22would%22%5D+%5B%22say%22%5D+%5B%22probably%22%5D+%5B%22in%22%5D+%5B%22early%22%5D+%5B%221940%22%5D+%5B%5D+%5B%22uh%22%5D+%5B%22there%22%5D+%5B%22was%22%5D+%5B%22a%22%5D+%5B%22sign%22%5D+%5B%22on%22%5D+%5B%22it%22%5D+%5B%22that%22%5D+%5B%22no%22%5D+%5B%22Jews%22%5D+%5B%22or%22%5D+%5B%22dogs%22%5D+%5B%22were%22%5D+%5B%22permitted%22%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22garden%22%5D+%5B%5D+%5B%22so%22%5D+%5B%22we%22%5D+%5B%22naturally%22%5D+%5B%22did%22%5D+%5B%22not%22%5D+%5B%22go%22%5D+%5B%22but%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22morning%22%5D+%5B%22on%22%5D+%5B%22which%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22leave%22%5D+%5B%22our%22%5D+%5B%22home%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22jumped%22%5D+%5B%22over%22%5D+%5B%22the%22%5D+%5B%22fence%22%5D+%5B%22and%22%5D+%5B%22went%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22garden%22%5D+%5B%22and%22%5D+%5B%22I%22%5D+%5B%22ran%22%5D+%5B%22around%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And uh as I told you , my my garden hold held most of my wonderful childhood memories and uh fairly soon ... I would say probably in early 1940 , uh there was a sign on it that no Jews or dogs were permitted in the garden , so we naturally did not go but on the morning on which we were forced to leave our home , I jumped over the fence and went to the garden and I ran around . ', 'right': '', 'complete_match': 'And uh as I told you , my my garden hold held most of my wonderful childhood memories and uh fairly soon ... I would say probably in early 1940 , uh there was a sign on it that no Jews or dogs were permitted in the garden , so we naturally did not go but on the morning on which we were forced to leave our home , I jumped over the fence and went to the garden and I ran around . ', 'testimony_id': 'irn504599', 'shelfmark': ['USHMM RG-50.030*0105'], 'token_start': 4866, 'token_end': 4950}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And uh as I told you, my my garden hold held most of my wonderful childhood memories and uh fairly soon...I would say probably in early 1940, uh there was a sign on it that no Jews or dogs were permitted in the garden, so we naturally did not go but on the morning on which we were forced to leave our home, I jumped over the fence and went to the garden and I ran around.\"\n",
    "fragment_1['label']=\"(..)my garden held most of my wonderful childhood memories but on the morning on which we were forced to leave our home, I (..) went to the garden(..)\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22that%22%5D+%5B%22specific%22%5D+%5B%22moment%22%5D+%5B%22in%22%5D+%5B%22which%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22leave%22%5D+%5B%22our%22%5D+%5B%22home%22%5D+%5B%22became%22%5D+%5B%22embedded%22%5D+%5B%22in%22%5D+%5B%22my%22%5D+%5B%22art%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'that specific moment in which we were forced to leave our home became embedded in my art . ', 'right': '', 'complete_match': 'that specific moment in which we were forced to leave our home became embedded in my art . ', 'testimony_id': 'irn538186', 'shelfmark': ['USHMM RG-50.926*0001'], 'token_start': 1246, 'token_end': 1264}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"that specific moment in which we were forced to leave our home became embedded in my art.\"\n",
    "fragment_2['label']=\"(..)that specific moment in which we were forced to leave our home became embedded in my art.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22they%22%5D+%5B%22told%22%5D+%5B%22the%22%5D+%5B%22Jews%22%5D+%5B%22have%22%5D+%5B%22to%22%5D+%5B%22leave%22%5D+%5B%22their%22%5D+%5B%22towns%22%5D+%5B%5D%7B0%2C50%7D+%5B%22their%22%5D+%5B%22towns%22%5D+%5B%22and%22%5D+%5B%22homes%22%5D+%5B%22and%22%5D+%5B%22belongings%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'they told the Jews have to leave their towns ... their towns and homes and belongings . ', 'right': '', 'complete_match': 'they told the Jews have to leave their towns ... their towns and homes and belongings . ', 'testimony_id': 'irn504553', 'shelfmark': ['USHMM RG-50.030*0055'], 'token_start': 2283, 'token_end': 2300}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"they told the Jews have to leave their towns...their towns and homes and belongings.\"\n",
    "fragment_3['label']=\"they told the Jews have to leave their towns...their towns and homes and belongings.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22this%22%5D+%5B%22was%22%5D+%5B%22the%22%5D+%5B%22worst%22%5D+%5B%22thing%22%5D+%5B%22that%22%5D+%5B%22would%22%5D+%5B%22happen%22%5D+%5B%22til%22%5D+%5B%22now%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22leave%22%5D+%5B%22our%22%5D+%5B%22home%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And this was the worst thing that would happen til now . We had to leave our home . ', 'right': '', 'complete_match': 'And this was the worst thing that would happen til now . We had to leave our home . ', 'testimony_id': 'irn504693', 'shelfmark': ['USHMM RG-50.030*0199'], 'token_start': 17828, 'token_end': 17847}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"And this was the worst thing that would happen til now. We had to leave our home.\"\n",
    "fragment_4['label']= \"And this was the worst thing that would happen til now. We had to leave our home.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22all%22%5D+%5B%22the%22%5D+%5B%22Jewish%22%5D+%5B%22people%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22move%22%5D+%5B%22out%22%5D+%5B%22of%22%5D+%5B%22their%22%5D+%5B%22homes%22%5D+%5B%22and%22%5D+%5B%22go%22%5D+%5B%22to%22%5D+%5B%22live%22%5D+%5B%22in%22%5D+%5B%22that%22%5D+%5B%22area%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'all the Jewish people had to move out of their homes and go to live in that area . ', 'right': '', 'complete_match': 'all the Jewish people had to move out of their homes and go to live in that area . ', 'testimony_id': 'irn504617', 'shelfmark': ['USHMM RG-50.030*0123'], 'token_start': 542, 'token_end': 561}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"all the Jewish people had to move out of their homes and go to live in that area.\"\n",
    "fragment_5['label']= \"(..) all the Jewish people had to move out of their homes and go to live in that area.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"force\",\"walk\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"force\"][]{0,10}[lemma=\"walk\"])|([lemma=\"walk\"][]{0,10}[lemma=\"force\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '([lemma=\"force\"][lemma=\".*self\"] []{0,10}[lemma=\"walk\"])|([lemma=\"walk\"][]{0,10}[lemma=\"force\"])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"myself walking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%5D%7B0%2C3%7D+%5B%22I%22%5D+%5B%22forced%22%5D+%5B%5D%7B0%2C3%7D+%5B%22I%22%5D+%5B%22forced%22%5D+%5B%5D%7B0%2C3%7D+%5B%22and%22%5D+%5B%22then%22%5D+%5B%22I%22%5D+%5B%22forced%22%5D+%5B%22myself%22%5D+%5B%22walking%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22forced%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I -- I forced -- I forced -- and then I forced myself walking . I forced . ', 'right': '', 'complete_match': 'I -- I forced -- I forced -- and then I forced myself walking . I forced . ', 'testimony_id': 'usc_shoah_2830', 'shelfmark': ['USC 2830'], 'token_start': 19635, 'token_end': 19653}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \" I-- I forced-- I forced-- and then I forced myself walking. I forced.\"\n",
    "fragment_1['label']=\" I-- I forced-- I forced-- and then I forced myself walking. I forced.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22So%22%5D+%5B%22I%22%5D+%5B%22forced%22%5D+%5B%22myself%22%5D+%5B%22to%22%5D+%5B%22walk%22%5D+%5B%22straight%22%5D+%5B%22and%22%5D+%5B%22to%22%5D+%5B%5D%7B0%2C3%7D+%5B%22unbelievable%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'So I forced myself to walk straight and to -- unbelievable . ', 'right': '', 'complete_match': 'So I forced myself to walk straight and to -- unbelievable . ', 'testimony_id': 'usc_shoah_3658', 'shelfmark': ['USC 3658'], 'token_start': 14400, 'token_end': 14412}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"So I forced myself to walk straight and to-- unbelievable.\"\n",
    "fragment_2['label']=\"So I forced myself to walk straight and to-- unbelievable.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22it%22%5D+%5B%22was%22%5D+%5B%22not%22%5D+%5B%22even%22%5D+%5B%22any%22%5D+%5B%22physical%22%5D+%5B%22to%22%5D+%5B%22be%22%5D+%5B%22outside%22%5D+%5B%5D+%5B%22force%22%5D+%5B%22yourself%22%5D+%5B%22to%22%5D+%5B%22walk%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And it was not even any physical to be outside , force yourself to walk . ', 'right': '', 'complete_match': 'And it was not even any physical to be outside , force yourself to walk . ', 'testimony_id': 'usc_shoah_8892', 'shelfmark': ['USC 8892'], 'token_start': 21798, 'token_end': 21814}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"And it was not even any physical to be outside, force yourself to walk.\"\n",
    "fragment_3['label']=\"And it was not even any physical to be outside, force yourself to walk.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Who%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22walk%22%5D+%5B%22got%22%5D+%5B%22killed%22%5D+%5B%5D+%5B%22So%22%5D+%5B%22you%22%5D+%5B%22make%22%5D+%5B%22yourself%22%5D+%5B%22walk%22%5D+%5B%5D%7B0%2C50%7D+%5B%22does%22%5D+%5B%5D+%5B%22matter%22%5D+%5B%22how%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Who could n’t walk got killed . So you make yourself walk ... does n’t matter how . ', 'right': '', 'complete_match': 'Who could n’t walk got killed . So you make yourself walk ... does n’t matter how . ', 'testimony_id': 'irn510728', 'shelfmark': ['USHMM RG-50.154*0008'], 'token_start': 10388, 'token_end': 10406}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"Who couldn’t walk got killed. So you make yourself walk...doesn’t matter how.\"\n",
    "fragment_4['label']= \"Who couldn’t walk got killed. So you make yourself walk...doesn’t matter how.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"force\",\"labor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"force\"][]{0,10}[lemma=\"labor\"])|([lemma=\"labor\"][]{0,10}[lemma=\"force\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '([lemma=\"force\"][\"to\"]{0,10}[lemma=\"labor\"])|([lemma=\"labor\"][]{0,10}[lemma=\"force\"])'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Any%22%5D+%5B%22kind%22%5D+%5B%22of%22%5D+%5B%22work%22%5D+%5B%22assigned%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22adults%22%5D+%5B%22was%22%5D+%5B%22of%22%5D+%5B%22course%22%5D+%5B%22forced%22%5D+%5B%5D+%5B%22The%22%5D+%5B%22children%22%5D+%5B%22were%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22work%22%5D+%5B%22as%22%5D+%5B%22of%22%5D+%5B%22age%22%5D+%5B%2210%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Any kind of work assigned to the adults was of course forced . The children were forced to work as of age 10 . ', 'right': '', 'complete_match': 'Any kind of work assigned to the adults was of course forced . The children were forced to work as of age 10 . ', 'testimony_id': 'usc_shoah_1216', 'shelfmark': ['USC 1216'], 'token_start': 10520, 'token_end': 10544}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"Any kind of work assigned to the adults was of course forced. The children were forced to work as of age 10.\"\n",
    "fragment_1['label']=\"Any kind of work assigned to the adults was of course forced. The children were forced to work as of age 10.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Everybody%22%5D+%5B%22was%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22work%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22everyone%22%5D+%5B%22was%22%5D+%5B%22assigned%22%5D+%5B%22in%22%5D+%5B%22places%22%5D+%5B%22according%22%5D+%5B%22to%22%5D+%5B%22their%22%5D+%5B%22trades%22%5D+%5B%5D+%5B%22according%22%5D+%5B%22to%22%5D+%5B%22their%22%5D+%5B%22education%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Everybody was forced to work . And everyone was assigned in places according to their trades , according to their education . ', 'right': '', 'complete_match': 'Everybody was forced to work . And everyone was assigned in places according to their trades , according to their education . ', 'testimony_id': 'usc_shoah_2578', 'shelfmark': ['USC 2578'], 'token_start': 1847, 'token_end': 1869}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"Everybody was forced to work. And everyone was assigned in places according to their trades, according to their education.\"\n",
    "fragment_2['label']=\"Everybody was forced to work. And everyone was assigned in places according to their trades, according to their education.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22were%22%5D+%5B%22slaves%22%5D+%5B%5D+%5B%22My%22%5D+%5B%22sisters%22%5D+%5B%22were%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22work%22%5D+%5B%5D+%5B%22They%22%5D+%5B%22were%22%5D+%5B%22not%22%5D+%5B%5D+%5B%22not%22%5D+%5B%5D+%5B%22not%22%5D+%5B%22paid%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They were slaves . My sisters were forced to work . They were not , not , not paid . ', 'right': '', 'complete_match': 'They were slaves . My sisters were forced to work . They were not , not , not paid . ', 'testimony_id': 'usc_shoah_8725', 'shelfmark': ['USC 8725'], 'token_start': 30704, 'token_end': 30724}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"They were slaves. My sisters were forced to work. They were not, not, not paid.\"\n",
    "fragment_3['label']=\"They were slaves. My sisters were forced to work. They were not, not, not paid.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22People%22%5D+%5B%22were%22%5D+%5B%22being%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22work%22%5D+%5B%22in%22%5D+%5B%22substandard%22%5D+%5B%22conditions%22%5D+%5B%5D+%5B%22with%22%5D+%5B%22less%22%5D+%5B%22than%22%5D+%5B%22normal%22%5D+%5B%22food%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'People were being forced to work in substandard conditions , with less than normal food . ', 'right': '', 'complete_match': 'People were being forced to work in substandard conditions , with less than normal food . ', 'testimony_id': 'usc_shoah_1799', 'shelfmark': ['USC 1799'], 'token_start': 13300, 'token_end': 13316}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"People were being forced to work in substandard conditions, with less than normal food.\"\n",
    "fragment_4['label']= \"People were being forced to work in substandard conditions, with less than normal food.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22was%22%5D+%5B%22there%22%5D+%5B%22for%22%5D+%5B%22three%22%5D+%5B%22years%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22forced%22%5D+%5B%22to%22%5D+%5B%22work%22%5D+%5B%2210%22%5D+%5B%22and%22%5D+%5B%2212%22%5D+%5B%22hours%22%5D+%5B%22a%22%5D+%5B%22day%22%5D+%5B%22in%22%5D+%5B%22this%22%5D+%5B%22plane%22%5D+%5B%22factory%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I was there for three years . I was forced to work 10 and 12 hours a day in this plane factory . ', 'right': '', 'complete_match': 'I was there for three years . I was forced to work 10 and 12 hours a day in this plane factory . ', 'testimony_id': 'HVT-180', 'shelfmark': ['Fortunoff HVT-180'], 'token_start': 3254, 'token_end': 3277}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"I was there for three years. I was forced to work 10 and 12 hours a day in this plane factory.\"\n",
    "fragment_5['label']= \"I was there for three years. I was forced to work 10 and 12 hours a day in this plane factory.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
