{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"laugh\"\n",
    "#delete_main_node(\"numbness\")\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"laugh\"]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lda model began\n",
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5Blemma%3D%22laugh%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=25\n",
      "training of gensim corpus began\n",
      "gensim corpus done\n"
     ]
    }
   ],
   "source": [
    "result = topic_concordancer.main(query,window=25,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.120*\"put\" + 0.082*\"stay\" + 0.067*\"bring\" + 0.061*\"find\" + 0.031*\"suppose\" + 0.030*\"arrive\" + 0.030*\"half\" + 0.028*\"move\" + 0.027*\"jew\" + 0.022*\"wait\"\n",
      "\n",
      "\n",
      "1\n",
      "0.317*\"do_not\" + 0.059*\"come_back\" + 0.051*\"what_happen\" + 0.034*\"God\" + 0.033*\"afraid\" + 0.031*\"Germans\" + 0.026*\"army\" + 0.024*\"world\" + 0.024*\"anymore\" + 0.023*\"sit\"\n",
      "\n",
      "\n",
      "2\n",
      "0.107*\"german\" + 0.079*\"talk\" + 0.076*\"could_not\" + 0.063*\"speak\" + 0.051*\"learn\" + 0.049*\"understand\" + 0.046*\"question\" + 0.038*\"polish\" + 0.031*\"English\" + 0.024*\"language\"\n",
      "\n",
      "\n",
      "3\n",
      "0.114*\"life\" + 0.101*\"a_lot\" + 0.055*\"kill\" + 0.054*\"story\" + 0.047*\"little_bit\" + 0.038*\"young\" + 0.034*\"book\" + 0.033*\"sing\" + 0.029*\"week\" + 0.029*\"change\"\n",
      "\n",
      "\n",
      "4\n",
      "0.102*\"family\" + 0.072*\"girl\" + 0.072*\"mother\" + 0.067*\"marry\" + 0.049*\"today\" + 0.038*\"wife\" + 0.037*\"wonderful\" + 0.036*\"father\" + 0.032*\"my_wife\" + 0.025*\"person\"\n",
      "\n",
      "\n",
      "5\n",
      "0.269*\"people\" + 0.158*\"good\" + 0.053*\"send\" + 0.046*\"nice\" + 0.037*\"Germany\" + 0.033*\"go_through\" + 0.020*\"people_who\" + 0.020*\"WH\" + 0.019*\"older\" + 0.019*\"number\"\n",
      "\n",
      "\n",
      "6\n",
      "0.128*\"live\" + 0.120*\"school\" + 0.073*\"kid\" + 0.069*\"house\" + 0.045*\"forget\" + 0.031*\"great\" + 0.030*\"teacher\" + 0.030*\"my_parent\" + 0.019*\"free\" + 0.019*\"class\"\n",
      "\n",
      "\n",
      "7\n",
      "0.099*\"day\" + 0.091*\"eat\" + 0.070*\"food\" + 0.049*\"could_not\" + 0.027*\"United_States\" + 0.025*\"terrible\" + 0.023*\"order\" + 0.020*\"cook\" + 0.017*\"right_away\" + 0.017*\"dog\"\n",
      "\n",
      "\n",
      "8\n",
      "0.090*\"man\" + 0.083*\"meet\" + 0.053*\"love\" + 0.048*\"get_marry\" + 0.041*\"my_husband\" + 0.039*\"Israel\" + 0.032*\"my_brother\" + 0.031*\"beautiful\" + 0.024*\"doctor\" + 0.023*\"fall\"\n",
      "\n",
      "\n",
      "9\n",
      "0.356*\"laugh\" + 0.203*\"start\" + 0.048*\"funny\" + 0.038*\"look_at\" + 0.032*\"die\" + 0.025*\"hit\" + 0.023*\"realize\" + 0.022*\"pick\" + 0.020*\"crazy\" + 0.019*\"look_like\"\n",
      "\n",
      "\n",
      "10\n",
      "0.359*\"laugh\" + 0.071*\"cry\" + 0.055*\"stand\" + 0.048*\"play\" + 0.045*\"joke\" + 0.034*\"face\" + 0.031*\"ss\" + 0.030*\"watch\" + 0.023*\"fun\" + 0.021*\"beat\"\n",
      "\n",
      "\n",
      "11\n",
      "0.258*\"remember\" + 0.072*\"big\" + 0.055*\"room\" + 0.041*\"sleep\" + 0.032*\"night\" + 0.028*\"bed\" + 0.026*\"morning\" + 0.023*\"ml_:\" + 0.023*\"hide\" + 0.021*\"soup\"\n",
      "\n",
      "\n",
      "12\n",
      "0.117*\"do_not\" + 0.114*\"leave\" + 0.086*\"feel\" + 0.069*\"woman\" + 0.061*\"hear\" + 0.039*\"must_have\" + 0.034*\"ghetto\" + 0.028*\"back\" + 0.028*\"eh\" + 0.021*\"special\"\n",
      "\n",
      "\n",
      "13\n",
      "0.203*\"thing\" + 0.084*\"happen\" + 0.075*\"kind_of\" + 0.058*\"sort_of\" + 0.054*\"talk_about\" + 0.042*\"read\" + 0.039*\"buy\" + 0.034*\"brother\" + 0.032*\"able_to\" + 0.031*\"wear\"\n",
      "\n",
      "\n",
      "14\n",
      "0.515*\"do_not\" + 0.049*\"show\" + 0.040*\"write\" + 0.039*\"point\" + 0.028*\"sb\" + 0.026*\"or_something\" + 0.018*\"guess\" + 0.017*\"part_of\" + 0.016*\"too_much\" + 0.014*\"care\"\n",
      "\n",
      "\n",
      "15\n",
      "0.126*\"jewish\" + 0.095*\"war\" + 0.087*\"friend\" + 0.058*\"guy\" + 0.033*\"experience\" + 0.032*\"find_out\" + 0.027*\"one_day\" + 0.025*\"over_there\" + 0.021*\"moment\" + 0.020*\"movie\"\n",
      "\n",
      "\n",
      "16\n",
      "0.236*\"make\" + 0.071*\"laugh\" + 0.066*\"try_to\" + 0.048*\"stop\" + 0.043*\"head\" + 0.033*\"each_other\" + 0.030*\"dress\" + 0.028*\"water\" + 0.026*\"throw\" + 0.026*\"look_at\"\n",
      "\n",
      "\n",
      "17\n",
      "0.129*\"call\" + 0.068*\"thank_you\" + 0.051*\"happy\" + 0.038*\"very_much\" + 0.037*\"lose\" + 0.035*\"word\" + 0.027*\"ms\" + 0.025*\"group\" + 0.022*\"spell\" + 0.018*\"bit\"\n",
      "\n",
      "\n",
      "18\n",
      "0.170*\"seconds\" + 0.085*\"back\" + 0.055*\"train\" + 0.054*\"run\" + 0.039*\"tape\" + 0.031*\"shoot\" + 0.029*\"car\" + 0.026*\"end_of\" + 0.025*\"catch\" + 0.023*\"Change_of\"\n",
      "\n",
      "\n",
      "19\n",
      "0.076*\"walk\" + 0.067*\"camp\" + 0.062*\"Jews\" + 0.057*\"Liz\" + 0.056*\"Fritzi\" + 0.037*\"street\" + 0.033*\"New_York\" + 0.032*\"town\" + 0.032*\"bad\" + 0.022*\"boat\"\n",
      "\n",
      "\n",
      "20\n",
      "0.085*\"picture\" + 0.074*\"year\" + 0.056*\"bear\" + 0.049*\"boy\" + 0.037*\"Poland\" + 0.035*\"business\" + 0.030*\"city\" + 0.028*\"end\" + 0.025*\"son\" + 0.025*\"jf\"\n",
      "\n",
      "\n",
      "21\n",
      "0.319*\"time\" + 0.113*\"of_course\" + 0.051*\"survive\" + 0.044*\"fact\" + 0.035*\"finally\" + 0.027*\"long\" + 0.024*\"wedding\" + 0.017*\"sh\" + 0.017*\"steal\" + 0.016*\"soldier\"\n",
      "\n",
      "\n",
      "22\n",
      "0.139*\"child\" + 0.127*\"my_mother\" + 0.106*\"my_father\" + 0.070*\"home\" + 0.051*\"crew_:\" + 0.035*\"my_sister\" + 0.033*\"sister\" + 0.032*\"parent\" + 0.025*\"let_'s\" + 0.022*\"baby\"\n",
      "\n",
      "\n",
      "23\n",
      "0.201*\"give\" + 0.053*\"money\" + 0.035*\"bread\" + 0.028*\"small\" + 0.026*\"pay\" + 0.025*\"clothes\" + 0.024*\"sell\" + 0.021*\"piece_of\" + 0.021*\"hand\" + 0.020*\"hospital\"\n",
      "\n",
      "\n",
      "24\n",
      "0.182*\"work\" + 0.063*\"place\" + 0.057*\"go_back\" + 0.049*\"job\" + 0.036*\"America\" + 0.032*\"open\" + 0.027*\"very_good\" + 0.024*\"door\" + 0.022*\"close\" + 0.020*\"travel\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games did you play ? ZG : What game did we play ? INT : Mm-hm . ZG : We played gin rummy . [ LAUGHS ] And what play did we play ? We went gymnastic . You know . I was n't too popular for that , but my \n",
      "\n",
      "\n",
      "their rifles and they put their rifles over their shoulder and they march down the main street , and I remember that my mother was laughing . And my father he still left to go to that parade and march in the parade , but he could n't have a rifle \n",
      "\n",
      "\n",
      "and standing . And I stood by , always watched it , and always watched the Hungarian officers and soldiers around smirking , ridiculing , laughing , smiling , screaming , ' it will not help . It will not help . It 's a waste . It 's not going \n",
      "\n",
      "\n",
      "they were falling . Standing with rifles , shooting them , and falling down . And the -- the -- my co-passengers , they were laughing and enjoying -- enjoying the view . And I , in order not to be suspect , I had to laugh along . It was \n",
      "\n",
      "\n",
      "they did . You could see it on their faces , they enjoyed it . As more they would beat you as more they were laughing . As more they hosed you out , as more they were laughing , as more they beat you with the back of a rifle \n",
      "\n",
      "\n",
      "if they saw on the street people like that , the SS men or the SA , they grab them , and they were the laugh of the whole public , and they cut the beards and cut the locks . It was such a horror , and they beat these \n",
      "\n",
      "\n",
      "were laughing , as more they beat you with the back of a rifle into the small of your back , the more they were laughing . As more they saw you writhing in pain , as more they laughed . Nobody told them to do that . They enjoyed it \n",
      "\n",
      "\n",
      "Huh ? Fun ? Yeah , you know how I had fun ? To aggravate the Germans . That was our fun . We would laugh so hard where we felt like crying . And they could not understand and then we , we would tell jokes , and we would \n",
      "\n",
      "\n",
      "the beard and payess and all this type of thing , they were just standing and cutting the beards , cutting the , payess , laughing at you , beating you up a little bit , and this type of thing . Q : So there was daily humiliation but you \n",
      "\n",
      "\n",
      "-- I think the words were , you cr -- when you cry , you cry a-alone , when you laugh , the whole world laughs with you . Okay ? And that ’s the truth . Nobody wants to hear you crying , nobody wants to see you crying . \n",
      "\n",
      "\n",
      "she had a common political interest , I think . And with her sense of humor as much as anything else , I think she laughed at my jokes . [ LAUGHS ] And I laughed at hers . INT : And are you still laughing at each other 's jokes \n",
      "\n",
      "\n",
      "Fun . A : Yeah . You know how I had fun ? To aggravate the Germans . That was our fun . We would laugh so hard where we felt like crying , and they could not understand . And then we -- we would tell jokes , and we \n",
      "\n",
      "\n",
      "then we -- we would tell jokes , and we would laugh . Really , we did n't feel like laughing , but we did laugh . And that just about killed them . My God , you should be crying there , what are you laughing ? You know . \n",
      "\n",
      "\n",
      "crying . It 's a funny thing that then , more than ever , at that time we were crying for happiness than smiling and laughing for happiness . That every second that we felt [ SOBBING ] that this is it . This is freedom . The -- the tears \n",
      "\n",
      "\n",
      ": Physical needs . The women 's and men 's , everybody together doing it on the open field . And the Germans watching and laughing and the Slovakian people in their uniform watching and laughing with the Germans . I love the expression . I -- I -- that expression \n",
      "\n",
      "\n",
      "That was his reward for being a kapo . And the kapo was supposed to watch us and scream at us . And they were laughing . They had fun . They said , well , one Jew screams at another . The kapo said , I have no choice . \n",
      "\n",
      "\n",
      "it . As more they would beat you as more they were laughing . As more they hosed you out , as more they were laughing , as more they beat you with the back of a rifle into the small of your back , the more they were laughing . \n",
      "\n",
      "\n",
      "not SS . They pulled out the Jews with the shuls and literally ripped their beards . They were shaving the beards and they were laughing , making fun . And they ripped the beards . And it made a terrible impact on me . But again , we were hoping \n",
      "\n",
      "\n",
      "remember [ indecipherable ] face . I just – I remember one face , I have this face in front of me , it was laughing and had those dimples [ indecipherable ] . But – Q : You do n’t know whose it was . A : No . Q \n",
      "\n",
      "\n",
      "to [ ? Beat them ? ] just to run around . And they 're fun -- the SS used to sit on top and laugh . Oh , do it again . Do it again . Just laughing . INT : Were you ever -- EF : I said , \n",
      "\n",
      "\n",
      ", he had a terrific sense of humor , he really did . And he enjoyed himself tremendously , h-his own jokes . He could laugh harder at his own jokes than anybody I knew . And I think I mentioned in my book that he had a collection of jokes \n",
      "\n",
      "\n",
      ". INT : Were you good ? SS : No , I was too chubby . I could n't run after the ball . [ LAUGHING ] But I still played though . I had fun , and I played . INT : Do you remember your bar mitzvah ? SS \n",
      "\n",
      "\n",
      "well standing there and watching people going in and out of stores and being badgered by SA people , and SA troopers . They would laugh and some would n’t go and some would go and there was no unpleasantness -- I did n’t see any of it . It was \n",
      "\n",
      "\n",
      "or so away from this clubhouse . When we came there , we saw a bunch of SS Nazis standing , joking , smoking , laughing , but they were there already . There was a bunch of shovels , and we were all -- especially the men and the boys \n",
      "\n",
      "\n",
      ". We did n’t have fun in the same manner as young people . Teen-agers have fun normally , but we had a lot of laughs and enjoyment , making fun and jokes of all this sort of thing . I : Yes . R : They were not ... I \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=10\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"laugh\",\"beat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"laugh\"][]{0,25}[lemma=\"beat\"])|([lemma=\"beat\"][]{0,25}[lemma=\"laugh\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=25)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"beat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22There%22%5D+%5B%22were%22%5D+%5B%22some%22%5D+%5B%22brown%22%5D+%5B%22shirts%22%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22beating%22%5D+%5B%22up%22%5D+%5B%22an%22%5D+%5B%22old%22%5D+%5B%22Jewish%22%5D+%5B%22man%22%5D+%5B%22with%22%5D+%5B%22a%22%5D+%5B%22long%22%5D+%5B%22beard%22%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22beating%22%5D+%5B%22him%22%5D+%5B%22up%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22there%22%5D+%5B%22was%22%5D+%5B%22people%22%5D+%5B%22standing%22%5D+%5B%22around%22%5D+%5B%22laughing%22%5D+%5B%22and%22%5D+%5B%22applauding%22%5D+%5B%22and%22%5D+%5B%5D+%5B%22as%22%5D+%5B%22I%22%5D+%5B%22say%22%5D+%5B%5D+%5B%22it%22%5D+%5B%22was%22%5D+%5B%22sort%22%5D+%5B%22of%22%5D+%5B%22like%22%5D+%5B%22a%22%5D+%5B%22Roman%22%5D+%5B%22circus%22%5D+%5B%22sort%22%5D+%5B%22of%22%5D+%5B%22atmosphere%22%5D+%5B%22that%22%5D+%5B%22night%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'There were some brown shirts and they were beating up an old Jewish man with a long beard and they were beating him up , and there was people standing around laughing and applauding and , as I say , it was sort of like a Roman circus sort of atmosphere that night . ', 'right': '', 'complete_match': 'There were some brown shirts and they were beating up an old Jewish man with a long beard and they were beating him up , and there was people standing around laughing and applauding and , as I say , it was sort of like a Roman circus sort of atmosphere that night . ', 'testimony_id': 'irn508626', 'shelfmark': ['USHMM RG-50.462*0005'], 'token_start': 24278, 'token_end': 24332}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"There were some brown shirts and they were beating up an old Jewish man with a long beard and they were beating him up, and there was people standing around laughing and applauding and, as I say, it was sort of like a Roman circus sort of atmosphere that night.\"\n",
    "fragment_1['label']=\"(..) they were beating up an old Jewish man with a long beard and they were beating him up, and there was people standing around laughing and applauding (..)\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22beating%22%5D+%5B%22us%22%5D+%5B%22and%22%5D+%5B%22laughing%22%5D+%5B%22about%22%5D+%5B%22us%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And they were beating us and laughing about us . ', 'right': '', 'complete_match': 'And they were beating us and laughing about us . ', 'testimony_id': 'usc_shoah_1537', 'shelfmark': ['USC 1537'], 'token_start': 4480, 'token_end': 4490}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"And they were beating us and laughing about us.\"\n",
    "fragment_2['label']=\" And they were beating us and laughing about us.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22where%22%5D+%5B%22would%22%5D+%5B%22they%22%5D+%5B%22beat%22%5D+%5B%22them%22%5D+%5B%22mostly%22%5D+%5B%5D+%5B%22On%22%5D+%5B%22their%22%5D+%5B%22sex%22%5D+%5B%22organs%22%5D+%5B%5D+%5B%22where%22%5D+%5B%22it%22%5D+%5B%22hurts%22%5D+%5B%22the%22%5D+%5B%22most%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22they%22%5D+%5B%22would%22%5D+%5B%22stand%22%5D+%5B%22there%22%5D+%5B%22and%22%5D+%5B%22laugh%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And where would they beat them mostly ? On their sex organs , where it hurts the most . And they would stand there and laugh . ', 'right': '', 'complete_match': 'And where would they beat them mostly ? On their sex organs , where it hurts the most . And they would stand there and laugh . ', 'testimony_id': 'irn505558', 'shelfmark': ['USHMM RG-50.042*0004'], 'token_start': 11655, 'token_end': 11682}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"And where would they beat them mostly? On their sex organs, where it hurts the most. And they would stand there and laugh.\"\n",
    "fragment_3['label']=\"And where would they beat them mostly? On their sex organs, where it hurts the most. And they would stand there and laugh.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22as%22%5D+%5B%22more%22%5D+%5B%22they%22%5D+%5B%22beat%22%5D+%5B%22you%22%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22back%22%5D+%5B%22of%22%5D+%5B%22a%22%5D+%5B%22rifle%22%5D+%5B%22into%22%5D+%5B%22the%22%5D+%5B%22small%22%5D+%5B%22of%22%5D+%5B%22your%22%5D+%5B%22back%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22more%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22laughing%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'as more they beat you with the back of a rifle into the small of your back , the more they were laughing ', 'right': '', 'complete_match': 'as more they beat you with the back of a rifle into the small of your back , the more they were laughing ', 'testimony_id': 'irn505558', 'shelfmark': ['USHMM RG-50.042*0004'], 'token_start': 15494, 'token_end': 15517}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"as more they beat you with the back of a rifle into the small of your back, the more they were laughing\"\n",
    "fragment_4['label']= \"(..) as more they beat you with the back of a rifle into the small of your back, the more they were laughing\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"not\",\"laugh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"not\"][]{0,1}[lemma=\"laugh\"])|([lemma=\"laugh\"][]{0,1}[lemma=\"not\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=1)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"cannot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22So%22%5D+%5B%22he%22%5D+%5B%22killed%22%5D+%5B%22my%22%5D+%5B%22father%22%5D+%5B%22right%22%5D+%5B%22in%22%5D+%5B%22front%22%5D+%5B%22of%22%5D+%5B%22me%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22I%22%5D+%5B%22froze%22%5D+%5B%5D+%5B%22I%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22cry%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22laugh%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22do%22%5D+%5B%22anything%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"So he killed my father right in front of me . And I froze . I , I could n't cry . I could n't laugh . I could n't do anything . \", 'right': '', 'complete_match': \"So he killed my father right in front of me . And I froze . I , I could n't cry . I could n't laugh . I could n't do anything . \", 'testimony_id': 'usc_shoah_8002', 'shelfmark': ['USC 8002'], 'token_start': 25990, 'token_end': 26023}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"So he killed my father right in front of me. And I froze. I, I couldn't cry. I couldn't laugh. I couldn't do anything.\"\n",
    "fragment_1['label']=\"So he killed my father right in front of me. And I froze. I, I couldn't cry. I couldn't laugh. I couldn't do anything.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Like%22%5D+%5B%22I%22%5D+%5B%22say%22%5D+%5B%5D+%5B%22now%22%5D+%5B%22I%22%5D+%5B%22can%22%5D+%5B%22laugh%22%5D+%5B%5D+%5B%22because%22%5D+%5B%22at%22%5D+%5B%22that%22%5D+%5B%22time%22%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22laugh%22%5D+%5B%5D+%5B%22Because%22%5D+%5B%22it%22%5D+%5B%22hurt%22%5D+%5B%22us%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Like I say , now I can laugh , because at that time I could n’t laugh . Because it hurt us . ', 'right': '', 'complete_match': 'Like I say , now I can laugh , because at that time I could n’t laugh . Because it hurt us . ', 'testimony_id': 'irn510703', 'shelfmark': ['USHMM RG-50.156*0049'], 'token_start': 21122, 'token_end': 21145}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"Like I say, now I can laugh, because at that time I couldn’t laugh. Because it hurt us.\"\n",
    "fragment_2['label']=\"Like I say, now I can laugh, because at that time I couldn’t laugh. Because it hurt us.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22can%22%5D+%5B%22not%22%5D+%5B%22laugh%22%5D+%5B%22as%22%5D+%5B%22wholeheartedly%22%5D+%5B%22as%22%5D+%5B%22anyone%22%5D+%5B%22else%22%5D+%5B%22can%22%5D+%5B%22laugh%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I can not laugh as wholeheartedly as anyone else can laugh ', 'right': '', 'complete_match': 'I can not laugh as wholeheartedly as anyone else can laugh ', 'testimony_id': 'HVT-43', 'shelfmark': ['Fortunoff HVT-43'], 'token_start': 28952, 'token_end': 28963}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"I cannot laugh as wholeheartedly as anyone else can laugh\"\n",
    "fragment_3['label']=\"I cannot laugh as wholeheartedly as anyone else can laugh (..)\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22A%22%5D+%5B%22cheerful%22%5D+%5B%22movie%22%5D+%5B%22might%22%5D+%5B%22bring%22%5D+%5B%22a%22%5D+%5B%22little%22%5D+%5B%22grin%22%5D+%5B%22on%22%5D+%5B%22my%22%5D+%5B%22face%22%5D+%5B%5D+%5B%22but%22%5D+%5B%22I%22%5D+%5B%22have%22%5D+%5B%5D+%5B%22heard%22%5D+%5B%22myself%22%5D+%5B%22heartily%22%5D+%5B%22laughing%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"A cheerful movie might bring a little grin on my face , but I have n't heard myself heartily laughing . \", 'right': '', 'complete_match': \"A cheerful movie might bring a little grin on my face , but I have n't heard myself heartily laughing . \", 'testimony_id': 'HVT-44', 'shelfmark': ['Fortunoff HVT-44'], 'token_start': 11079, 'token_end': 11100}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"A cheerful movie might bring a little grin on my face, but I haven't heard myself heartily laughing.\"\n",
    "fragment_4['label']= \" A cheerful movie might bring a little grin on my face, but I haven't heard myself heartily laughing.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22there%22%5D+%5B%22were%22%5D+%5B%22these%22%5D+%5B%22moments%22%5D+%5B%22that%22%5D+%5B%22there%22%5D+%5B%22is%22%5D+%5B%22some%22%5D+%5B%22humor%22%5D+%5B%22to%22%5D+%5B%22it%22%5D+%5B%22in%22%5D+%5B%22a%22%5D+%5B%22way%22%5D+%5B%5D+%5B%22but%22%5D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22laugh%22%5D+%5B%22at%22%5D+%5B%22the%22%5D+%5B%22time%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"there were these moments that there is some humor to it in a way , but I did n't laugh at the time . \", 'right': '', 'complete_match': \"there were these moments that there is some humor to it in a way , but I did n't laugh at the time . \", 'testimony_id': 'irn504849', 'shelfmark': ['USHMM RG-50.030*0356'], 'token_start': 5226, 'token_end': 5250}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"there were these moments that there is some humor to it in a way, but I didn't laugh at the time.\"\n",
    "fragment_5['label']= \"(..) there were these moments that there is some humor to it in a way, but I didn't laugh at the time.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
