{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"sleep\"\n",
    "delete_main_node(main_node)\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"sleep\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result = topic_concordancer.main(query,window=25,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=1\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Cold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"cold\",\"sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"cold\"][]{0,25}[lemma=\"sleep\"])|([lemma=\"sleep\"][]{0,25}[lemma=\"cold\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=25)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"cold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22went%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%5D+%5B%22But%22%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22winter%22%5D+%5B%22time%22%5D+%5B%5D+%5B%22it%22%5D+%5B%22was%22%5D+%5B%22very%22%5D+%5B%22cold%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we went to sleep . But in the winter time , it was very cold . ', 'right': '', 'complete_match': 'And we went to sleep . But in the winter time , it was very cold . ', 'testimony_id': 'HVT-70', 'shelfmark': ['Fortunoff Archive HVT-70'], 'token_start': 4564, 'token_end': 4581}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And we went to sleep. But in the winter time, it was very cold. \"\n",
    "fragment_1['label']=\"And we went to sleep. But in the winter time, it was very cold. \"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22It%22%5D+%5B%22was%22%5D+%5B%22dark%22%5D+%5B%5D+%5B%22cold%22%5D+%5B%5D+%5B%22wet%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22sleep%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"It was dark , cold , wet . I could n't sleep . \", 'right': '', 'complete_match': \"It was dark , cold , wet . I could n't sleep . \", 'testimony_id': 'irn504467', 'shelfmark': ['USHMM RG-50.030*0001'], 'token_start': 12007, 'token_end': 12020}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"It was dark, cold, wet. I couldn't sleep.\"\n",
    "fragment_2['label']=\"It was dark, cold, wet. I couldn't sleep.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22were%22%5D+%5B%22sleeping%22%5D+%5B%22outside%22%5D+%5B%22the%22%5D+%5B%22houses%22%5D+%5B%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22steps%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'We were sleeping outside the houses , on the steps ', 'right': '', 'complete_match': 'We were sleeping outside the houses , on the steps ', 'testimony_id': 'HVT-145', 'shelfmark': ['Fortunoff Archive HVT-145'], 'token_start': 6736, 'token_end': 6746}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"We were sleeping outside the houses, on the steps \"\n",
    "fragment_3['label']=\"We were sleeping outside the houses, on the steps outside-- in the cold, in the fields.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22fact%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22never%22%5D+%5B%22so%22%5D+%5B%22cold%22%5D+%5B%22in%22%5D+%5B%22my%22%5D+%5B%22life%22%5D+%5B%22as%22%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22in%22%5D+%5B%22May%22%5D+%5B%22of%22%5D+%5B%221942%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22slept%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22upper%22%5D+%5B%22bunk%22%5D+%5B%22and%22%5D+%5B%22the%22%5D+%5B%22wind%22%5D+%5B%22came%22%5D+%5B%22in%22%5D+%5B%22from%22%5D+%5B%22the%22%5D+%5B%22open%22%5D+%5B%22rafters%22%5D+%5B%22under%22%5D+%5B%22the%22%5D+%5B%22roof%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22we%22%5D+%5B%22had%22%5D+%5B%22no%22%5D+%5B%22bedding%22%5D+%5B%22or%22%5D+%5B%22blankets%22%5D+%5B%22and%22%5D+%5B%22slept%22%5D+%5B%22in%22%5D+%5B%22our%22%5D+%5B%22day%22%5D+%5B%22clothing%22%5D+%5B%5D+%5B%22which%22%5D+%5B%22consisted%22%5D+%5B%22of%22%5D+%5B%22a%22%5D+%5B%22shirt%22%5D+%5B%22and%22%5D+%5B%22a%22%5D+%5B%22cotton%22%5D+%5B%22blouse%22%5D+%5B%22of%22%5D+%5B%22sorts%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I fact , I was never so cold in my life as I was in May of 1942 . I slept on the upper bunk and the wind came in from the open rafters under the roof , and we had no bedding or blankets and slept in our day clothing , which consisted of a shirt and a cotton blouse of sorts . ', 'right': '', 'complete_match': 'I fact , I was never so cold in my life as I was in May of 1942 . I slept on the upper bunk and the wind came in from the open rafters under the roof , and we had no bedding or blankets and slept in our day clothing , which consisted of a shirt and a cotton blouse of sorts . ', 'testimony_id': 'irn509119', 'shelfmark': ['USHMM RG-50.233*0037'], 'token_start': 8348, 'token_end': 8412}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"I fact, I was never so cold in my life as I was in May of 1942. I slept on the upper bunk and the wind came in from the open rafters under the roof, and we had no bedding or blankets and slept in our day clothing, which consisted of a shirt and a cotton blouse of sorts.\"\n",
    "fragment_4['label']= \"I was never so cold in my life (..)I slept on the upper bunk and the wind came in from the open rafters(..)\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Of%22%5D+%5B%22course%22%5D+%5B%5D+%5B%22we%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22sleep%22%5D+%5B%22for%22%5D+%5B%22it%22%5D+%5B%22was%22%5D+%5B%22too%22%5D+%5B%22cold%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Of course , we could n’t sleep for it was too cold . ', 'right': '', 'complete_match': 'Of course , we could n’t sleep for it was too cold . ', 'testimony_id': 'irn509119', 'shelfmark': ['USHMM RG-50.233*0037'], 'token_start': 9608, 'token_end': 9621}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"Of course, we couldn’t sleep for it was too cold.\"\n",
    "fragment_5['label']= \"Of course, we couldn’t sleep for it was too cold.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"dead\",\"sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"dead\"][]{0,25}[lemma=\"sleep\"])|([lemma=\"sleep\"][]{0,25}[lemma=\"dead\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=25)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"with a dead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22I%22%5D+%5B%22slept%22%5D+%5B%22with%22%5D+%5B%22a%22%5D+%5B%22dead%22%5D+%5B%22woman%22%5D+%5B%22because%22%5D+%5B%22she%22%5D+%5B%22was%22%5D+%5B%22very%22%5D+%5B%22nice%22%5D+%5B%22and%22%5D+%5B%22cool%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And I slept with a dead woman because she was very nice and cool . ', 'right': '', 'complete_match': 'And I slept with a dead woman because she was very nice and cool . ', 'testimony_id': 'HVT-77', 'shelfmark': ['Fortunoff Archive HVT-77'], 'token_start': 4258, 'token_end': 4273}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And I slept with a dead woman because she was very nice and cool.\"\n",
    "fragment_1['label']=\"And I slept with a dead woman because she was very nice and cool.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22just%22%5D+%5B%22slept%22%5D+%5B%22there%22%5D+%5B%22the%22%5D+%5B%22rest%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22night%22%5D+%5B%5D+%5B%22but%22%5D+%5B%22he%22%5D+%5B%22was%22%5D+%5B%22dead%22%5D+%5B%22just%22%5D+%5B%22from%22%5D+%5B%22cold%22%5D+%5B%22and%22%5D+%5B%22not%22%5D+%5B%22eating%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I just slept there the rest of the night , but he was dead just from cold and not eating . ', 'right': '', 'complete_match': 'I just slept there the rest of the night , but he was dead just from cold and not eating . ', 'testimony_id': 'irn504694', 'shelfmark': ['USHMM RG-50.030*0200'], 'token_start': 13396, 'token_end': 13417}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"I just slept there the rest of the night, but he was dead just from cold and not eating.\"\n",
    "fragment_2['label']=\"I just slept there the rest of the night, but he was dead just from cold and not eating.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22you%22%5D+%5B%22sleep%22%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22dead%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'you sleep with the dead ', 'right': '', 'complete_match': 'you sleep with the dead ', 'testimony_id': 'irn508628', 'shelfmark': ['USHMM RG-50.462*0007'], 'token_start': 19960, 'token_end': 19965}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"you sleep with the dead\"\n",
    "fragment_4['label']= \"(..)you sleep with the dead(..)\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22when%22%5D+%5B%22you%22%5D+%5B%22got%22%5D+%5B%22up%22%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22morning%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22thought%22%5D+%5B%22you%22%5D+%5B%22sleeping%22%5D+%5B%22with%22%5D+%5B%22somebody%22%5D+%5B%22and%22%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22meantime%22%5D+%5B%22the%22%5D+%5B%22other%22%5D+%5B%22guy%22%5D+%5B%22was%22%5D+%5B%22already%22%5D+%5B%22dead%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'when you got up in the morning , you thought you sleeping with somebody and in the meantime the other guy was already dead . ', 'right': '', 'complete_match': 'when you got up in the morning , you thought you sleeping with somebody and in the meantime the other guy was already dead . ', 'testimony_id': 'irn509081', 'shelfmark': ['USHMM RG-50.233*0004'], 'token_start': 4330, 'token_end': 4355}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"when you got up in the morning, you thought you sleeping with somebody and in the meantime the other guy was already dead.\"\n",
    "fragment_5['label']= \"(..) when you got up in the morning, you thought you sleeping with somebody and in the meantime the other guy was already dead.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"n't\",\"sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"n't\"][]{0,1}[lemma=\"sleep\"])|([lemma=\"sleep\"][]{0,1}[lemma=\"n't\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=1)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"could not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22not%22%5D+%5B%22sleep%22%5D+%5B%5D+%5B%22We%22%5D+%5B%22could%22%5D+%5B%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"not sleep . We could n't . \", 'right': '', 'complete_match': \"not sleep . We could n't . \", 'testimony_id': 'usc_shoah_10272', 'shelfmark': ['USC Shoah Foundation 10272'], 'token_start': 21372, 'token_end': 21379}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"not sleep. We couldn't.\"\n",
    "fragment_1['label']=\"We couldn't even-- not sleep. We couldn't.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Well%22%5D+%5B%5D+%5B%22we%22%5D+%5B%22had%22%5D+%5B%22enough%22%5D+%5B%22time%22%5D+%5B%5D+%5B%22but%22%5D+%5B%22who%22%5D+%5B%22could%22%5D+%5B%22sleep%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Well , we had enough time , but who could sleep ? ', 'right': '', 'complete_match': 'Well , we had enough time , but who could sleep ? ', 'testimony_id': 'usc_shoah_20745', 'shelfmark': ['USC Shoah Foundation 20745'], 'token_start': 10578, 'token_end': 10590}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"Well, we had enough time, but who could sleep? \"\n",
    "fragment_2['label']=\"Well, we had enough time, but who could sleep? \"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22no%22%5D+%5B%22sleep%22%5D+%5B%22and%22%5D+%5B%22no%22%5D+%5B%22food%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And no sleep and no food . ', 'right': '', 'complete_match': 'And no sleep and no food . ', 'testimony_id': 'usc_shoah_11184', 'shelfmark': ['USC Shoah Foundation 11184'], 'token_start': 14709, 'token_end': 14716}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"And no sleep and no food. \"\n",
    "fragment_3['label']=\"And no sleep and no food. \"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22it%22%5D+%5B%5D+%5B%22snowing%22%5D+%5B%5D+%5B%22no%22%5D+%5B%22food%22%5D+%5B%5D+%5B%22no%22%5D+%5B%22sleep%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"And it 's snowing , no food , no sleep . \", 'right': '', 'complete_match': \"And it 's snowing , no food , no sleep . \", 'testimony_id': 'usc_shoah_895', 'shelfmark': ['USC Shoah Foundation 895'], 'token_start': 7284, 'token_end': 7295}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"And it's snowing, no food, no sleep. \"\n",
    "fragment_4['label']= \"And it's snowing, no food, no sleep. \"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22There%22%5D+%5B%22was%22%5D+%5B%22no%22%5D+%5B%22sleep%22%5D+%5B%5D+%5B%22not%22%5D+%5B%22much%22%5D+%5B%22sleep%22%5D+%5B%22at%22%5D+%5B%22all%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'There was no sleep , not much sleep at all . ', 'right': '', 'complete_match': 'There was no sleep , not much sleep at all . ', 'testimony_id': 'usc_shoah_11184', 'shelfmark': ['USC Shoah Foundation 11184'], 'token_start': 14829, 'token_end': 14840}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"There was no sleep, not much sleep at all.\"\n",
    "fragment_5['label']= \"There was no sleep, not much sleep at all.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"cry\",\"sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"cry\"][]{0,25}[lemma=\"sleep\"])|([lemma=\"sleep\"][]{0,25}[lemma=\"cry\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=25)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"cry to sleep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22I%22%5D+%5B%22cried%22%5D+%5B%22myself%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%22because%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22hard%22%5D+%5B%22work%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22over%22%5D+%5B%22there%22%5D+%5B%5D+%5B%22that%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22way%22%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22for%22%5D+%5B%22six%22%5D+%5B%22months%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"And I cried myself to sleep because of the hard work . And over there , that 's the way I was for six months . \", 'right': '', 'complete_match': \"And I cried myself to sleep because of the hard work . And over there , that 's the way I was for six months . \", 'testimony_id': 'irn504640', 'shelfmark': ['USHMM RG-50.030*0146'], 'token_start': 6869, 'token_end': 6895}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And I cried myself to sleep because of the hard work. And over there, that's the way I was for six months.\"\n",
    "fragment_1['label']=\"And I cried myself to sleep because of the hard work. (..) that's the way I was for six months.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22carry%22%5D+%5B%22them%22%5D+%5B%22out%22%5D+%5B%22uh%22%5D+%5B%222%22%5D+%5B%22by%22%5D+%5B%222%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22it%22%5D+%5B%22was%22%5D+%5B%22very%22%5D+%5B%5D+%5B%22very%22%5D+%5B%22heavy%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22the%22%5D+%5B%22labor%22%5D+%5B%22was%22%5D+%5B%22just%22%5D+%5B%22as%22%5D+%5B%22hard%22%5D+%5B%22as%22%5D+%5B%22those%22%5D+%5B%22tremendous%22%5D+%5B%22rocks%22%5D+%5B%22that%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22dragging%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22I%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22thought%22%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22going%22%5D+%5B%22to%22%5D+%5B%22do%22%5D+%5B%5D+%5B%22I%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22cried%22%5D+%5B%22myself%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%22every%22%5D+%5B%22night%22%5D+%5B%22because%22%5D+%5B%22I%22%5D+%5B%22thought%22%5D+%5B%22I%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22do%22%5D+%5B%22that%22%5D+%5B%22hard%22%5D+%5B%22labor%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"We had to carry them out uh 2 by 2 , and it was very , very heavy , and the labor was just as hard as those tremendous rocks that we were dragging , and I , I thought I was going to do , I , I cried myself to sleep every night because I thought I could n't do that hard labor . \", 'right': '', 'complete_match': \"We had to carry them out uh 2 by 2 , and it was very , very heavy , and the labor was just as hard as those tremendous rocks that we were dragging , and I , I thought I was going to do , I , I cried myself to sleep every night because I thought I could n't do that hard labor . \", 'testimony_id': 'irn505573', 'shelfmark': ['USHMM RG-50.042*0020'], 'token_start': 5631, 'token_end': 5697}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"We had to carry them out uh 2 by 2, and it was very, very heavy, and the labor was just as hard as those tremendous rocks that we were dragging, and I, I thought I was going to do, I, I cried myself to sleep every night because I thought I couldn't do that hard labor.\"\n",
    "fragment_2['label']=\"I cried myself to sleep every night because I thought I couldn't do that hard labor.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22just%22%5D+%5B%22cried%22%5D+%5B%22myself%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%22many%22%5D+%5B%22nights%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I just cried myself to sleep many nights ', 'right': '', 'complete_match': 'I just cried myself to sleep many nights ', 'testimony_id': 'irn84819', 'shelfmark': ['USHMM RG-50.030*0754'], 'token_start': 2445, 'token_end': 2453}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \" I just cried myself to sleep many nights\"\n",
    "fragment_3['label']=\"I just cried myself to sleep many nights\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22as%22%5D+%5B%22I%22%5D+%5B%22cried%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22cried%22%5D+%5B%22myself%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%5D+%5B%22I%22%5D+%5B%22dozed%22%5D+%5B%22off%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22I%22%5D+%5B%22saw%22%5D+%5B%22my%22%5D+%5B%22mother%22%5D+%5B%22coming%22%5D+%5B%22in%22%5D+%5B%22through%22%5D+%5B%22the%22%5D+%5B%22camp%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And as I cried , I cried myself to sleep , I dozed off , and I saw my mother coming in through the camp ', 'right': '', 'complete_match': 'And as I cried , I cried myself to sleep , I dozed off , and I saw my mother coming in through the camp ', 'testimony_id': 'irn505576', 'shelfmark': ['USHMM RG-50.042*0023'], 'token_start': 8342, 'token_end': 8367}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"And as I cried, I cried myself to sleep, I dozed off, and I saw my mother coming in through the camp\"\n",
    "fragment_4['label']= \"And as I cried, I cried myself to sleep, I dozed off, and I saw my mother coming in through the camp (..)\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"straw\",\"sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"straw\"][]{0,25}[lemma=\"sleep\"])|([lemma=\"sleep\"][]{0,25}[lemma=\"straw\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=25)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"straw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22slept%22%5D+%5B%22on%22%5D+%5B%22wooden%22%5D+%5B%22planks%22%5D+%5B%22with%22%5D+%5B%22filthy%22%5D+%5B%22straw%22%5D+%5B%22or%22%5D+%5B%22something%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we slept on wooden planks with filthy straw or something . ', 'right': '', 'complete_match': 'And we slept on wooden planks with filthy straw or something . ', 'testimony_id': 'irn504441', 'shelfmark': ['USHMM RG-50.106*0019'], 'token_start': 2924, 'token_end': 2936}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And we slept on wooden planks with filthy straw or something.\"\n",
    "fragment_1['label']=\"And we slept on wooden planks with filthy straw or something.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22we%22%5D+%5B%22slept%22%5D+%5B%22on%22%5D+%5B%22some%22%5D+%5B%22straw%22%5D+%5B%22mattresses%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And we slept on some straw mattresses . ', 'right': '', 'complete_match': 'And we slept on some straw mattresses . ', 'testimony_id': 'HVT-83', 'shelfmark': ['Fortunoff Archive HVT-83'], 'token_start': 7458, 'token_end': 7466}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"And we slept on some straw mattresses.\"\n",
    "fragment_2['label']=\"And we slept on some straw mattresses.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22slept%22%5D+%5B%22on%22%5D+%5B%22straw%22%5D+%5B%5D+%5B%22In%22%5D+%5B%22the%22%5D+%5B%22barracks%22%5D+%5B%22there%22%5D+%5B%22were%22%5D+%5B%5D%7B0%2C3%7D+%5B%22other%22%5D+%5B%22men%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'We slept on straw . In the barracks there were 50-60 other men . ', 'right': '', 'complete_match': 'We slept on straw . In the barracks there were 50-60 other men . ', 'testimony_id': 'irn507461', 'shelfmark': ['USHMM RG-50.031*0029'], 'token_start': 1110, 'token_end': 1124}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"We slept on straw. In the barracks there were 50-60 other men.\"\n",
    "fragment_3['label']=\"We slept on straw. In the barracks there were 50-60 other men.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22The%22%5D+%5B%22beds%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22bunks%22%5D+%5B%5D+%5B%22yes%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22it%22%5D+%5B%22was%22%5D+%5B%22straw%22%5D+%5B%22there%22%5D+%5B%5D+%5B%22That%22%5D+%5B%5D+%5B%22all%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"The beds , the bunks , yes . And it was straw there . That 's all . \", 'right': '', 'complete_match': \"The beds , the bunks , yes . And it was straw there . That 's all . \", 'testimony_id': 'usc_shoah_16922', 'shelfmark': ['USC Shoah Foundation 16922'], 'token_start': 7651, 'token_end': 7669}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"The beds, the bunks, yes. And it was straw there. That's all.\"\n",
    "fragment_4['label']= \"The beds, the bunks, yes. And it was straw there. That's all.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22they%22%5D+%5B%22put%22%5D+%5B%22straw%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22floor%22%5D+%5B%5D+%5B%22so%22%5D+%5B%22we%22%5D+%5B%22all%22%5D+%5B%22slept%22%5D+%5B%22on%22%5D+%5B%22straw%22%5D+%5B%5D+%5B%22called%22%5D+%5B%22paille%22%5D+%5B%22in%22%5D+%5B%22French%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And they put straw on the floor , so we all slept on straw , called paille in French . ', 'right': '', 'complete_match': 'And they put straw on the floor , so we all slept on straw , called paille in French . ', 'testimony_id': 'usc_shoah_19923', 'shelfmark': ['USC Shoah Foundation 19923'], 'token_start': 10997, 'token_end': 11017}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"And they put straw on the floor, so we all slept on straw, called paille in French.\"\n",
    "fragment_5['label']= \"And they put straw on the floor, so we all slept on straw, called paille in French.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. sleep  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"afraid\",\"sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"afraid\"][]{0,2}[lemma=\"sleep\"])|([lemma=\"sleep\"][]{0,2}[lemma=\"afraid\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=2)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"afraid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22one%22%5D+%5B%22day%22%5D+%5B%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22afraid%22%5D+%5B%5D%7B0%2C3%7D+%5B%22there%22%5D+%5B%22were%22%5D+%5B%22raids%22%5D+%5B%22all%22%5D+%5B%22over%22%5D+%5B%5D+%5B%22so%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22afraid%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%22in%22%5D+%5B%22any%22%5D+%5B%22place%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And one day , we were afraid -- there were raids all over , so we were afraid to sleep in any place . ', 'right': '', 'complete_match': 'And one day , we were afraid -- there were raids all over , so we were afraid to sleep in any place . ', 'testimony_id': 'HVT-109', 'shelfmark': ['Fortunoff Archive HVT-109'], 'token_start': 7719, 'token_end': 7743}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And one day, we were afraid-- there were raids all over, so we were afraid to sleep in any place.\"\n",
    "fragment_1['label']=\"(..) there were raids all over, so we were afraid to sleep in any place\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22remember%22%5D+%5B%22I%22%5D+%5B%22was%22%5D+%5B%22afraid%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%22in%22%5D+%5B%22my%22%5D+%5B%22bed%22%5D+%5B%22because%22%5D+%5B%22after%22%5D+%5B%22all%22%5D+%5B%5D%7B0%2C50%7D+%5B%22uh%22%5D+%5B%5D%7B0%2C50%7D+%5B%22the%22%5D+%5B%22two%22%5D+%5B%5D+%5B%22three%22%5D+%5B%22weeks%22%5D+%5B%22I%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22sleep%22%5D+%5B%22in%22%5D+%5B%22a%22%5D+%5B%22bed%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"I remember I was afraid to sleep in my bed because after all ... uh ... the two , three weeks I did n't sleep in a bed . \", 'right': '', 'complete_match': \"I remember I was afraid to sleep in my bed because after all ... uh ... the two , three weeks I did n't sleep in a bed . \", 'testimony_id': 'irn504608', 'shelfmark': ['USHMM RG-50.030*0114'], 'token_start': 2371, 'token_end': 2400}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"I remember I was afraid to sleep in my bed because after all...uh...the two, three weeks I didn't sleep in a bed.\"\n",
    "fragment_2['label']=\"I remember I was afraid to sleep in my bed (..)\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22You%22%5D+%5B%22could%22%5D+%5B%5D+%5B%22sleep%22%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22barracks%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22were%22%5D+%5B%22afraid%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%22there%22%5D+%5B%5D+%5B%22they%22%5D+%5B%5D+%5B%22going%22%5D+%5B%22to%22%5D+%5B%22come%22%5D+%5B%5D+%5B%22take%22%5D+%5B%22you%22%5D+%5B%22out%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'You could n’t sleep in the barracks , you were afraid to sleep there , they ’re going to come , take you out . ', 'right': '', 'complete_match': 'You could n’t sleep in the barracks , you were afraid to sleep there , they ’re going to come , take you out . ', 'testimony_id': 'irn510703', 'shelfmark': ['USHMM RG-50.156*0049'], 'token_start': 2382, 'token_end': 2407}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"You couldn’t sleep in the barracks, you were afraid to sleep there, they’re going to come, take you out.\"\n",
    "fragment_3['label']=\"You couldn’t sleep in the barracks, you were afraid to sleep there, they’re going to come, take you out.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22I%22%5D+%5B%22was%22%5D+%5B%22afraid%22%5D+%5B%22to%22%5D+%5B%22sleep%22%5D+%5B%5D+%5B%22if%22%5D+%5B%22my%22%5D+%5B%22dreams%22%5D+%5B%22control%22%5D+%5B%22me%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'I was afraid to sleep , if my dreams control me . ', 'right': '', 'complete_match': 'I was afraid to sleep , if my dreams control me . ', 'testimony_id': 'usc_shoah_7455', 'shelfmark': ['USC Shoah Foundation 7455'], 'token_start': 38406, 'token_end': 38418}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"I was afraid to sleep, if my dreams control me.\"\n",
    "fragment_4['label']= \"I was afraid to sleep, if my dreams control me.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
