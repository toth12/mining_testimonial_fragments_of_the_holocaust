{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining testimonial fragments of the Holocaust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience domain:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_topic_model_concordance as topic_concordancer\n",
    "from utils import blacklab, db, text\n",
    "mongo = db.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_query(lemmas,context_length=50):\n",
    "    permutations = itertools.permutations(lemmas,len(lemmas))\n",
    "    final_result = []\n",
    "    for element in list(permutations):\n",
    "        temp_result = []\n",
    "        for el in element:\n",
    "            temp_result.append('[lemma=\"'+el+'\"]')\n",
    "        temp_result = '('+('[]{0,'+str(context_length)+'}').join(temp_result)+')'\n",
    "        final_result.append(temp_result)\n",
    "    final_result = '|'.join(final_result)\n",
    "    return final_result\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import blacklab, db, text\n",
    "import requests\n",
    "import json\n",
    "def find_sentence_id(label):\n",
    "    props = {'annotators': 'tokenize'}\n",
    "\n",
    "    # set the encoding of the annotator\n",
    "    requests.encoding = 'utf-8'\n",
    "    # make a request\n",
    "    r = requests.post('http://localhost:9000/', params={'properties':\n",
    "                      json.dumps(props)},\n",
    "                      data=label.encode('utf-8'))\n",
    "    result = json.loads(r.text, encoding='utf-8')\n",
    "    query = []\n",
    "    for i, token in enumerate(result['tokens']):\n",
    "\n",
    "        if ('...'in token['word'] and ((i == 0) or\n",
    "           i == len(result['tokens']) - 1)):\n",
    "            continue\n",
    "        elif ('...'in token['word']):\n",
    "            query.append('[]{0,50}')\n",
    "        elif ('-'in token['word']):\n",
    "            query.append('[]{0,3}')\n",
    "        elif (\"n't\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'re\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"?\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\".\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\"'s\"in token['word']):\n",
    "            query.append('[]')\n",
    "        elif (\",\"in token['word']):\n",
    "            query.append('[]')\n",
    "        else:\n",
    "            query.append('[\"' + token['word'] + '\"]')\n",
    "\n",
    "    query = ' '.join(query)\n",
    "    try:\n",
    "        sentence = blacklab.search_blacklab(query, window=0,\n",
    "                                            lemma=False,\n",
    "                                            include_match=True)\n",
    "        token_end = sentence[0]['token_end']\n",
    "        token_start = sentence[0]['token_start']\n",
    "        print (sentence[0])\n",
    "        mongo = db.get_db()\n",
    "        results = mongo.tokens.find({'testimony_id':\n",
    "                                    sentence[0]['testimony_id']},\n",
    "                                    {'_id': 0})\n",
    "        tokens = list(results)[0]['tokens']\n",
    "        sentenceStart = tokens[token_start]['sentence_index']\n",
    "        sentenceEnd = tokens[token_end]['sentence_index']\n",
    "        originalsentence = sentence[0]['complete_match']\n",
    "        return (sentenceStart,sentenceEnd,sentence[0]['testimony_id'])\n",
    "    except:\n",
    "        print(\"The following query returned a null result\")\n",
    "        print(query)\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_node(label):\n",
    "    \"\"\"Generate a root node for a tree structure.\"\"\"\n",
    "    testimony_id = random.randint(1, 20)\n",
    "    node = {}\n",
    "    node['label'] = label\n",
    "    fragment = {'label': label,\n",
    "                'essay_id': random.randint(1, 20),\n",
    "                'tree': get_node(testimony_id, node, is_parent=True)}\n",
    "    fragment['tree']['label'] = label\n",
    "\n",
    "    return fragment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node(testimony_id, node, is_parent=False):\n",
    "    \"\"\"Generate a parent or leaf node for a tree structure.\"\"\"\n",
    "    if is_parent:\n",
    "        return {\n",
    "            'label': node['label'],\n",
    "            'testimony_id': random.randint(1, 20),\n",
    "            'media_index': random.randint(1, 20),\n",
    "            'media_offset': random.randint(1, 20),\n",
    "            'start_sentence_index': random.randint(1, 20),\n",
    "            'end_sentence_index': random.randint(1, 20),\n",
    "            'children': [], }\n",
    "    else:\n",
    "        return {'label': node['label'],\n",
    "                'testimony_id': node['testimony_id'],\n",
    "                'media_index': float(node['media_index']),\n",
    "                'media_offset': float(node['media_offset']),\n",
    "                'start_sentence_index': float(node['start_sentence_index']),\n",
    "                'end_sentence_index': float(node['end_sentence_index']),\n",
    "                'children': [], }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_main_node_exist(node):\n",
    "    results = mongo.fragments.find({'label':node},{'_id': 0})\n",
    "    if len(results[0])==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_main_node(label):\n",
    "    mongo.fragments.insert(create_parent_node(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_main_node(label):\n",
    "    mongo.fragments.delete_one({'label':label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_testimonial_fragments(fragments):\n",
    "    if check_if_main_node_exist(fragments['main_node']):\n",
    "        results = mongo.fragments.find({'label':fragments['main_node']},{'_id':0})[0]\n",
    "        mid_nodes = [element['label'] for element in results['tree']['children']]\n",
    "        if fragments['mid_node'] in mid_nodes:\n",
    "            print (\"mid node exists cannot be added\")\n",
    "        else:\n",
    "            \n",
    "            mid_node = get_node('r',{'label':fragments['mid_node']},is_parent=True)\n",
    "            for fragment in fragments['fragments']:\n",
    "                leaf = get_node(fragment['testimony_id'],fragment)\n",
    "                mid_node['children'].append(leaf)\n",
    "            results['tree']['children'].append(mid_node)\n",
    "            mongo.fragments.replace_one({'label':fragments['main_node']},results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the main node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "main_node = \"whip\"\n",
    "delete_main_node(main_node)\n",
    "add_main_node(main_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '[lemma=\"whip\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result = topic_concordancer.main(query,window=10,topicn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the key topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i,element in enumerate(result['topic_documents']):\n",
    "    print (i)\n",
    "    topic_words =  element['topic_words'][1]\n",
    "    print (topic_words)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i=0\n",
    "for text in result['topic_documents'][i]['texts'][0:25]:\n",
    "    print (text['matched_text_words'])\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testimonial fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"whip\",\"carry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"whip\"][]{0,10}[lemma=\"carry\"])|([lemma=\"carry\"][]{0,10}[lemma=\"whip\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"carry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22So%22%5D+%5B%22we%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22go%22%5D+%5B%22down%22%5D+%5B%5D+%5B%22400%22%5D+%5B%5D+%5B%22500%22%5D+%5B%22people%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22carry%22%5D+%5B%22that%22%5D+%5B%22cable%22%5D+%5B%22on%22%5D+%5B%22their%22%5D+%5B%5D%7B0%2C3%7D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22was%22%5D+%5B%22going%22%5D+%5B%22with%22%5D+%5B%22whips%22%5D+%5B%5D+%5B%22leather%22%5D+%5B%22whips%22%5D+%5B%5D+%5B%22eh%22%5D+%5B%22%21%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'So we had to go down , 400 , 500 people , and carry that cable on their -- and they was going with whips , leather whips , eh ! ', 'right': '', 'complete_match': 'So we had to go down , 400 , 500 people , and carry that cable on their -- and they was going with whips , leather whips , eh ! ', 'testimony_id': 'usc_shoah_6009', 'shelfmark': ['USC Shoah Foundation 6009'], 'token_start': 8714, 'token_end': 8745}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"So we had to go down, 400, 500 people, and carry that cable on their-- and they was going with whips, leather whips, eh!\"\n",
    "fragment_1['label']=\"So we had to go down, 400, 500 people, and carry that cable on their-- and they was going with whips, leather whips, eh! \"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22carry%22%5D+%5B%22small%22%5D+%5B%22bombs%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22the%22%5D+%5B%22SS%22%5D+%5B%22were%22%5D+%5B%22standing%22%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22whips%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22if%22%5D+%5B%22you%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22pick%22%5D+%5B%22it%22%5D+%5B%22up%22%5D+%5B%22and%22%5D+%5B%22take%22%5D+%5B%22it%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22train%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22were%22%5D+%5B%22whipped%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"We had to carry small bombs . And the SS were standing with the whips , and if you did n't pick it up and take it on the train , you were whipped . \", 'right': '', 'complete_match': \"We had to carry small bombs . And the SS were standing with the whips , and if you did n't pick it up and take it on the train , you were whipped . \", 'testimony_id': 'irn504671', 'shelfmark': ['USHMM RG-50.030*0168'], 'token_start': 1796, 'token_end': 1831}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"We had to carry small bombs. And the SS were standing with the whips, and if you didn't pick it up and take it on the train, you were whipped.\"\n",
    "fragment_2['label']=\"We had to carry small bombs. And the SS were standing with the whips, and if you didn't pick it up (..), you were whipped.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22whatever%22%5D+%5B%22they%22%5D+%5B%22had%22%5D+%5B%22left%22%5D+%5B%5D%7B0%2C3%7D+%5B%22some%22%5D+%5B%22people%22%5D+%5B%22took%22%5D+%5B%22their%22%5D+%5B%22suitcases%22%5D+%5B%5D+%5B%22They%22%5D+%5B%22were%22%5D+%5B%22kicked%22%5D+%5B%22and%22%5D+%5B%22whipped%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And whatever they had left -- some people took their suitcases . They were kicked and whipped . ', 'right': '', 'complete_match': 'And whatever they had left -- some people took their suitcases . They were kicked and whipped . ', 'testimony_id': 'usc_shoah_9262', 'shelfmark': ['USC Shoah Foundation 9262'], 'token_start': 23194, 'token_end': 23212}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"And whatever they had left-- some people took their suitcases. They were kicked and whipped.\"\n",
    "fragment_3['label']=\"And whatever they had left-- some people took their suitcases. They were kicked and whipped.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"kill\",\"whip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"kill\"][]{0,10}[lemma=\"whip\"])|([lemma=\"whip\"][]{0,10}[lemma=\"kill\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"kill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Except%22%5D+%5B%22that%22%5D+%5B%22girl%22%5D+%5B%22that%22%5D+%5B%22had%22%5D+%5B%22a%22%5D+%5B%5D%7B0%2C3%7D+%5B%22one%22%5D+%5B%22leg%22%5D+%5B%5D%7B0%2C3%7D+%5B%22she%22%5D+%5B%5D+%5B%22ah%22%5D+%5B%5D+%5B%22they%22%5D+%5B%5D%7B0%2C3%7D+%5B%22for%22%5D+%5B%22some%22%5D+%5B%22reason%22%5D+%5B%5D%7B0%2C3%7D+%5B%22they%22%5D+%5B%22beat%22%5D+%5B%22her%22%5D+%5B%22with%22%5D+%5B%5D%7B0%2C3%7D+%5B%22whips%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Except that girl that had a – one leg — she , ah , they – for some reason – they beat her with twenty-five whips ', 'right': '', 'complete_match': 'Except that girl that had a – one leg — she , ah , they – for some reason – they beat her with twenty-five whips ', 'testimony_id': 'irn510737', 'shelfmark': ['USHMM RG-50.154*0026'], 'token_start': 10109, 'token_end': 10135}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"Except that girl that had a – one leg—she, ah, they – for some reason – they beat her with twenty-five whips\"\n",
    "fragment_1['label']=\"Except that girl that had a – one leg—she,(..) they beat her with twenty-five whips(..)\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22one%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22SSs%22%5D+%5B%22called%22%5D+%5B%5D+%5B%22uh%22%5D+%5B%5D+%5B%22told%22%5D+%5B%22once%22%5D+%5B%22that%22%5D+%5B%22he%22%5D+%5B%22could%22%5D+%5B%22kill%22%5D+%5B%5D+%5B%22he%22%5D+%5B%22had%22%5D+%5B%22a%22%5D+%5B%22very%22%5D+%5B%22big%22%5D+%5B%22whip%22%5D+%5B%22made%22%5D+%5B%5D+%5B%22that%22%5D+%5B%22he%22%5D+%5B%22could%22%5D+%5B%22kill%22%5D+%5B%22a%22%5D+%5B%22Jew%22%5D+%5B%22in%22%5D+%5B%2210%22%5D+%5B%22whips%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'one of the SSs called , uh , told once that he could kill , he had a very big whip made , that he could kill a Jew in 10 whips ', 'right': '', 'complete_match': 'one of the SSs called , uh , told once that he could kill , he had a very big whip made , that he could kill a Jew in 10 whips ', 'testimony_id': 'irn505564', 'shelfmark': ['USHMM RG-50.042*0010'], 'token_start': 14425, 'token_end': 14457}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"one of the SSs called, uh, told once that he could kill, he had a very big whip made, that he could kill a Jew in 10 whips\"\n",
    "fragment_2['label']=\"(..) one of the SSs called, uh, told once that he could kill, he had a very big whip made, that he could kill a Jew in 10 whips (..)\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22he%22%5D+%5B%22humiliated%22%5D+%5B%22people%22%5D+%5B%22by%22%5D+%5B%22lashing%22%5D+%5B%5D%7B0%2C3%7D+%5B%22beating%22%5D+%5B%22a%22%5D+%5B%22person%22%5D+%5B%22with%22%5D+%5B%22a%22%5D+%5B%22whip%22%5D+%5B%5D%7B0%2C3%7D+%5B%22in%22%5D+%5B%22front%22%5D+%5B%22of%22%5D+%5B%22all%22%5D+%5B%22the%22%5D+%5B%22workers%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22on%22%5D+%5B%22one%22%5D+%5B%22occasion%22%5D+%5B%5D+%5B%22as%22%5D+%5B%22I%22%5D+%5B%22mentioned%22%5D+%5B%22before%22%5D+%5B%5D+%5B%22he%22%5D+%5B%22had%22%5D+%5B%22killed%22%5D+%5B%22a%22%5D+%5B%22person%22%5D+%5B%22not%22%5D+%5B%22too%22%5D+%5B%22far%22%5D+%5B%22from%22%5D+%5B%22myself%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'he humiliated people by lashing -- beating a person with a whip -- in front of all the workers . And on one occasion , as I mentioned before , he had killed a person not too far from myself . ', 'right': '', 'complete_match': 'he humiliated people by lashing -- beating a person with a whip -- in front of all the workers . And on one occasion , as I mentioned before , he had killed a person not too far from myself . ', 'testimony_id': 'usc_shoah_7358', 'shelfmark': ['USC Shoah Foundation 7358'], 'token_start': 5125, 'token_end': 5166}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"he humiliated people by lashing-- beating a person with a whip-- in front of all the workers. And on one occasion, as I mentioned before, he had killed a person not too far from myself.\"\n",
    "fragment_3['label']=\"he humiliated people by lashing-- beating a person with a whip-- (..) And on one occasion (..) he had killed a person not too far from myself.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22so%22%5D+%5B%22he%22%5D+%5B%22says%22%5D+%5B%22to%22%5D+%5B%22him%22%5D+%5B%22that%22%5D+%5B%22he%22%5D+%5B%22just%22%5D+%5B%22killed%22%5D+%5B%22a%22%5D+%5B%22Jew%22%5D+%5B%22with%22%5D+%5B%2212%22%5D+%5B%22hits%22%5D+%5B%22with%22%5D+%5B%22his%22%5D+%5B%22whip%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'so he says to him that he just killed a Jew with 12 hits with his whip . ', 'right': '', 'complete_match': 'so he says to him that he just killed a Jew with 12 hits with his whip . ', 'testimony_id': 'irn504562', 'shelfmark': ['USHMM RG-50.030*0066'], 'token_start': 6707, 'token_end': 6725}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"so he says to him that he just killed a Jew with 12 hits with his whip.\"\n",
    "fragment_4['label']= \"(..) so he says to him that he just killed a Jew with 12 hits with his whip.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"march\",\"woman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"march\"][]{0,10}[lemma=\"woman\"])|([lemma=\"woman\"][]{0,10}[lemma=\"march\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"march\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22the%22%5D+%5B%22SS%22%5D+%5B%22lifted%22%5D+%5B%22their%22%5D+%5B%22whips%22%5D+%5B%5D+%5B%22It%22%5D+%5B%22was%22%5D+%5B%22forward%22%5D+%5B%22march%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22we%22%5D+%5B%22started%22%5D+%5B%22to%22%5D+%5B%22march%22%5D+%5B%5D+%5B%5D%7B0%2C3%7D+%5B%22PAUSES%22%5D+%5B%22FOR%22%5D+%5B%225%22%5D+%5B%22SECONDS%22%5D+%5B%5D%7B0%2C3%7D+%5B%22We%22%5D+%5B%22left%22%5D+%5B%22a%22%5D+%5B%22bloody%22%5D+%5B%22trail%22%5D+%5B%22in%22%5D+%5B%5D+%5B%22in%22%5D+%5B%22the%22%5D+%5B%22snow%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And the SS lifted their whips . It was forward march . And we started to march . [ PAUSES FOR 5 SECONDS ] We left a bloody trail in , in the snow . ', 'right': '', 'complete_match': 'And the SS lifted their whips . It was forward march . And we started to march . [ PAUSES FOR 5 SECONDS ] We left a bloody trail in , in the snow . ', 'testimony_id': 'usc_shoah_9725', 'shelfmark': ['USC Shoah Foundation 9725'], 'token_start': 18403, 'token_end': 18438}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And the SS lifted their whips. It was forward march. And we started to march. [PAUSES FOR 5 SECONDS] We left a bloody trail in, in the snow.\"\n",
    "fragment_1['label']=\"And the SS lifted their whips. It was forward march. And we started to march. We left a bloody trail in, in the snow.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22were%22%5D+%5B%22marched%22%5D+%5B%22through%22%5D+%5B%22the%22%5D+%5B%22town%22%5D+%5B%22under%22%5D+%5B%22the%22%5D+%5B%22whips%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22SS%22%5D+%5B%22uh%22%5D+%5B%22to%22%5D+%5B%22a%22%5D+%5B%22certain%22%5D+%5B%22collection%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'We were marched through the town under the whips of the SS uh to a certain collection ', 'right': '', 'complete_match': 'We were marched through the town under the whips of the SS uh to a certain collection ', 'testimony_id': 'irn504599', 'shelfmark': ['USHMM RG-50.030*0105'], 'token_start': 6838, 'token_end': 6855}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"We were marched through the town under the whips of the SS uh to a certain collection\"\n",
    "fragment_2['label']=\"We were marched through the town under the whips of the SS uh to a certain collection (..).\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22side%22%5D+%5B%22were%22%5D+%5B%22the%22%5D+%5B%22SS%22%5D+%5B%22men%22%5D+%5B%22and%22%5D+%5B%22the%22%5D+%5B%22SS%22%5D+%5B%22women%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22lifted%22%5D+%5B%22their%22%5D+%5B%22whips%22%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22said%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And on the side were the SS men and the SS women , and they lifted their whips and they said ', 'right': '', 'complete_match': 'And on the side were the SS men and the SS women , and they lifted their whips and they said ', 'testimony_id': 'irn507376', 'shelfmark': ['USHMM RG-50.042*0001'], 'token_start': 13993, 'token_end': 14014}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = 'And on the side were the SS men and the SS women, and they lifted their whips and they said'\n",
    "fragment_3['label']='And on the side were the SS men and the SS women, and they lifted their whips and they said, \"Forward march.\"'\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22were%22%5D+%5B%22marched%22%5D+%5B%22every%22%5D+%5B%22morning%22%5D+%5B%22into%22%5D+%5B%22fields%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22there%22%5D+%5B%22was%22%5D+%5B%22always%22%5D+%5B%22a%22%5D+%5B%22whip%22%5D+%5B%22above%22%5D+%5B%22your%22%5D+%5B%22head%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'We were marched every morning into fields . And there was always a whip above your head . ', 'right': '', 'complete_match': 'We were marched every morning into fields . And there was always a whip above your head . ', 'testimony_id': 'usc_shoah_27443', 'shelfmark': ['USC Shoah Foundation 27443'], 'token_start': 9844, 'token_end': 9862}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"We were marched every morning into fields. And there was always a whip above your head.\"\n",
    "fragment_4['label']= \"We were marched every morning into fields. And there was always a whip above your head.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22some%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22whips%22%5D+%5B%22hit%22%5D+%5B%22us%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22so%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22marching%22%5D+%5B%22quickly%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And some of the whips hit us . And so we were marching quickly . ', 'right': '', 'complete_match': 'And some of the whips hit us . And so we were marching quickly . ', 'testimony_id': 'usc_shoah_2916', 'shelfmark': ['USC Shoah Foundation 2916'], 'token_start': 11302, 'token_end': 11317}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \" And some of the whips hit us. And so we were marching quickly.\"\n",
    "fragment_5['label']= \" And some of the whips hit us. And so we were marching quickly. \"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"whip\",\"run\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"whip\"][]{0,10}[lemma=\"run\"])|([lemma=\"run\"][]{0,10}[lemma=\"whip\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22had%22%5D+%5B%22whips%22%5D+%5B%22and%22%5D+%5B%22were%22%5D+%5B%22yelling%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They had whips and were yelling ', 'right': '', 'complete_match': 'They had whips and were yelling ', 'testimony_id': 'irn503624', 'shelfmark': ['USHMM RG-50.005*0028'], 'token_start': 6481, 'token_end': 6487}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = 'They had whips and were yelling'\n",
    "fragment_1['label']='They had whips and were yelling,\"Run, run, fast, fast.\"'\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Just%22%5D+%5B%22whip%22%5D+%5B%22and%22%5D+%5B%22run%22%5D+%5B%5D+%5B%22Whip%22%5D+%5B%22you%22%5D+%5B%22and%22%5D+%5B%22run%22%5D+%5B%5D+%5B%22Always%22%5D+%5B%22run%22%5D+%5B%5D+%5B%22run%22%5D+%5B%5D+%5B%22run%22%5D+%5B%5D+%5B%22run%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'Just whip and run . Whip you and run . Always run , run , run , run . ', 'right': '', 'complete_match': 'Just whip and run . Whip you and run . Always run , run , run , run . ', 'testimony_id': 'HVT-158', 'shelfmark': ['Fortunoff Archive HVT-158'], 'token_start': 9438, 'token_end': 9457}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"Just whip and run. Whip you and run. Always run, run, run, run.\"\n",
    "fragment_2['label']=\"Just whip and run. Whip you and run. Always run, run, run, run.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22So%22%5D+%5B%22they%22%5D+%5B%22whipped%22%5D+%5B%22preferably%22%5D+%5B%22those%22%5D+%5B%22with%22%5D+%5B%22loads%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22so%22%5D+%5B%22they%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22run%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'So they whipped preferably those with loads , and so they had to run . ', 'right': '', 'complete_match': 'So they whipped preferably those with loads , and so they had to run . ', 'testimony_id': 'usc_shoah_15694', 'shelfmark': ['USC Shoah Foundation 15694'], 'token_start': 25120, 'token_end': 25135}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"So they whipped preferably those with loads, and so they had to run.\"\n",
    "fragment_3['label']=\"So they whipped preferably those with loads, and so they had to run. \"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22once%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22running%22%5D+%5B%5D+%5B%22he%22%5D+%5B%22made%22%5D+%5B%22us%22%5D+%5B%22run%22%5D+%5B%22in%22%5D+%5B%22a%22%5D+%5B%22circle%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22ice%22%5D+%5B%22and%22%5D+%5B%22whipped%22%5D+%5B%22us%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'once we were running , he made us run in a circle on the ice and whipped us ', 'right': '', 'complete_match': 'once we were running , he made us run in a circle on the ice and whipped us ', 'testimony_id': 'usc_shoah_21582', 'shelfmark': ['USC Shoah Foundation 21582'], 'token_start': 26674, 'token_end': 26692}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"once we were running, he made us run in a circle on the ice and whipped us\"\n",
    "fragment_4['label']= \"(..)once we were running, he made us run in a circle on the ice and whipped us (..).\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22while%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22running%22%5D+%5B%22out%22%5D+%5B%22of%22%5D+%5B%22the%22%5D+%5B%22houses%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22beating%22%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22whips%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'while we were running out of the houses , they were beating with the whips ', 'right': '', 'complete_match': 'while we were running out of the houses , they were beating with the whips ', 'testimony_id': 'HVT-91', 'shelfmark': ['Fortunoff Archive HVT-91'], 'token_start': 730, 'token_end': 745}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"while we were running out of the houses, they were beating with the whips\"\n",
    "fragment_5['label']= \"(..)while we were running out of the houses, they were beating with the whips (..)\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"walk\",\"whip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"walk\"][]{0,10}[lemma=\"whip\"])|([lemma=\"whip\"][]{0,10}[lemma=\"walk\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"walk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22were%22%5D+%5B%22horse%22%5D+%5B%22whipped%22%5D+%5B%22all%22%5D+%5B%22the%22%5D+%5B%22way%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22camp%22%5D+%5B%5D+%5B%22walking%22%5D+%5B%223%22%5D+%5B%22or%22%5D+%5B%224%22%5D+%5B%5D+%5B%222%22%5D+%5B%22or%22%5D+%5B%223%22%5D+%5B%22miles%22%5D+%5B%22to%22%5D+%5B%22the%22%5D+%5B%22camp%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'We were horse whipped all the way to the camp , walking 3 or 4 , 2 or 3 miles to the camp ', 'right': '', 'complete_match': 'We were horse whipped all the way to the camp , walking 3 or 4 , 2 or 3 miles to the camp ', 'testimony_id': 'irn505558', 'shelfmark': ['USHMM RG-50.042*0004'], 'token_start': 10226, 'token_end': 10249}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"We were horse whipped all the way to the camp, walking 3 or 4, 2 or 3 miles to the camp\"\n",
    "fragment_1['label']=\"We were horse whipped all the way to the camp, walking 3 or 4, 2 or 3 miles to the camp (..).\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22We%22%5D+%5B%22were%22%5D+%5B%22walking%22%5D+%5B%22in%22%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22stand%22%5D+%5B%22with%22%5D+%5B%22the%22%5D+%5B%22long%22%5D+%5B%22whips%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'We were walking in and they were stand with the long whips ', 'right': '', 'complete_match': 'We were walking in and they were stand with the long whips ', 'testimony_id': 'irn506639', 'shelfmark': ['USHMM RG-50.106*0128'], 'token_start': 8626, 'token_end': 8638}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"We were walking in and they were stand with the long whips\"\n",
    "fragment_2['label']=\"We were walking in and they were stand with the long whips and they’re beating on us (..).\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22SS%22%5D+%5B%22men%22%5D+%5B%22hitting%22%5D+%5B%22them%22%5D+%5B%22with%22%5D+%5B%22whips%22%5D+%5B%22and%22%5D+%5B%22urging%22%5D+%5B%22them%22%5D+%5B%22to%22%5D+%5B%22walk%22%5D+%5B%22faster%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'SS men hitting them with whips and urging them to walk faster . ', 'right': '', 'complete_match': 'SS men hitting them with whips and urging them to walk faster . ', 'testimony_id': 'usc_shoah_9701', 'shelfmark': ['USC Shoah Foundation 9701'], 'token_start': 5072, 'token_end': 5085}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"SS men hitting them with whips and urging them to walk faster.\"\n",
    "fragment_3['label']=\"(..) SS men hitting them with whips and urging them to walk faster.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22on%22%5D+%5B%22each%22%5D+%5B%22between%22%5D+%5B%22the%22%5D+%5B%22walking%22%5D+%5B%22back%22%5D+%5B%22and%22%5D+%5B%22forth%22%5D+%5B%22work%22%5D+%5B%22and%22%5D+%5B%22whip%22%5D+%5B%22and%22%5D+%5B%22whip%22%5D+%5B%22and%22%5D+%5B%22whip%22%5D+%5B%5D%7B0%2C3%7D+%5B%22it%22%5D+%5B%5D+%5B%22just%22%5D+%5B%22like%22%5D+%5B%5D+%5B%22like%22%5D+%5B%22a%22%5D+%5B%5D%7B0%2C3%7D+%5B%22something%22%5D+%5B%22on%22%5D+%5B%22the%22%5D+%5B%22air%22%5D+%5B%5D+%5B%22sometime%22%5D+%5B%22get%22%5D+%5B%22you%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'on each between the walking back and forth work and whip and whip and whip --- it ’s just like , like a --- something on the air , sometime get you . ', 'right': '', 'complete_match': 'on each between the walking back and forth work and whip and whip and whip --- it ’s just like , like a --- something on the air , sometime get you . ', 'testimony_id': 'irn510737', 'shelfmark': ['USHMM RG-50.154*0026'], 'token_start': 6759, 'token_end': 6792}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"on each between the walking back and forth work and whip and whip and whip --- it’s just like, like a --- something on the air, sometime get you.\"\n",
    "fragment_4['label']= \"(..) on each between the walking back and forth work and whip and whip and whip --- it’s just like, like a --- something on the air, sometime get you.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22Sundays%22%5D+%5B%5D+%5B%22where%22%5D+%5B%22we%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22work%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22we%22%5D+%5B%22walked%22%5D+%5B%22around%22%5D+%5B%22outside%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22went%22%5D+%5B%22around%22%5D+%5B%22with%22%5D+%5B%22whips%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22whip%22%5D+%5B%22up%22%5D+%5B%5D+%5B%22just%22%5D+%5B%22walking%22%5D+%5B%22there%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"Sundays , where we did n't work , and we walked around outside , the went around with whips , and they whip up , just walking there . \", 'right': '', 'complete_match': \"Sundays , where we did n't work , and we walked around outside , the went around with whips , and they whip up , just walking there . \", 'testimony_id': 'usc_shoah_2684', 'shelfmark': ['USC Shoah Foundation 2684'], 'token_start': 15592, 'token_end': 15621}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"Sundays, where we didn't work, and we walked around outside, the went around with whips, and they whip up, just walking there.\"\n",
    "fragment_5['label']= \"Sundays, where we didn't work, and we walked around outside, the went around with whips, and they whip up, just walking there.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"count\",\"whip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"count\"][]{0,10}[lemma=\"whip\"])|([lemma=\"whip\"][]{0,10}[lemma=\"count\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22But%22%5D+%5B%22I%22%5D+%5B%22got%22%5D+%5B%22twenty%22%5D+%5B%5D%7B0%2C3%7D+%5B%22five%22%5D+%5B%22and%22%5D+%5B%22you%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22count%22%5D+%5B%5D+%5B%22You%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22count%22%5D+%5B%22you%22%5D+%5B%22got%22%5D+%5B%22the%22%5D+%5B%22whip%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'But I got twenty - five and you had to count . You did n’t count you got the whip . ', 'right': '', 'complete_match': 'But I got twenty - five and you had to count . You did n’t count you got the whip . ', 'testimony_id': 'irn509193', 'shelfmark': ['USHMM RG-50.233*0111'], 'token_start': 15136, 'token_end': 15157}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"But I got twenty- five and you had to count. You didn’t count you got the whip.\"\n",
    "fragment_1['label']=\"But I got twenty- five and you had to count. You didn’t count you got the whip.\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22they%22%5D+%5B%22told%22%5D+%5B%22him%22%5D+%5B%22that%22%5D+%5B%22he%22%5D+%5B%22would%22%5D+%5B%22get%22%5D+%5B%22now%22%5D+%5B%22a%22%5D+%5B%22hundred%22%5D+%5B%22whips%22%5D+%5B%22and%22%5D+%5B%22he%22%5D+%5B%22has%22%5D+%5B%22to%22%5D+%5B%22count%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'they told him that he would get now a hundred whips and he has to count . ', 'right': '', 'complete_match': 'they told him that he would get now a hundred whips and he has to count . ', 'testimony_id': 'irn508479', 'shelfmark': ['USHMM RG-50.030*0411'], 'token_start': 31655, 'token_end': 31672}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"they told him that he would get now a hundred whips and he has to count.\"\n",
    "fragment_2['label']=\"(..) they told him that he would get now a hundred whips and he has to count.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22neat%22%5D+%5B%5D%7B0%2C3%7D+%5B%22they%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22pull%22%5D+%5B%22out%22%5D+%5B%22their%22%5D+%5B%22pants%22%5D+%5B%5D+%5B%22and%22%5D+%5B%22they%22%5D+%5B%22beat%22%5D+%5B%22them%22%5D+%5B%5D+%5B%22whipped%22%5D+%5B%22them%22%5D+%5B%5D+%5B%22put%22%5D+%5B%22down%22%5D+%5B%2225%22%5D+%5B%5D+%5B%22They%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22count%22%5D+%5B%22it%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They neat -- they had to pull out their pants , and they beat them , whipped them , put down 25 . They had to count it . ', 'right': '', 'complete_match': 'They neat -- they had to pull out their pants , and they beat them , whipped them , put down 25 . They had to count it . ', 'testimony_id': 'usc_shoah_1491', 'shelfmark': ['USC Shoah Foundation 1491'], 'token_start': 8115, 'token_end': 8144}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"They neat-- they had to pull out their pants, and they beat them, whipped them, put down 25. They had to count it.\"\n",
    "fragment_3['label']=\"They neat-- they had to pull out their pants, and they beat them, whipped them, put down 25. They had to count it.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22they%22%5D+%5B%22used%22%5D+%5B%22to%22%5D+%5B%22have%22%5D+%5B%22to%22%5D+%5B%22count%22%5D+%5B%22them%22%5D+%5B%2225%22%5D+%5B%5D+%5B%22They%22%5D+%5B%22used%22%5D+%5B%22to%22%5D+%5B%22get%22%5D+%5B%2225%22%5D+%5B%22whips%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And they used to have to count them 25 . They used to get 25 whips . ', 'right': '', 'complete_match': 'And they used to have to count them 25 . They used to get 25 whips . ', 'testimony_id': 'usc_shoah_1496', 'shelfmark': ['USC Shoah Foundation 1496'], 'token_start': 5046, 'token_end': 5063}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \" And they used to have to count them 25. They used to get 25 whips.\"\n",
    "fragment_4['label']= \" And they used to have to count them 25. They used to get 25 whips.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22used%22%5D+%5B%22to%22%5D+%5B%22get%22%5D+%5B%2225%22%5D+%5B%22whips%22%5D+%5B%5D+%5B%22And%22%5D+%5B%22if%22%5D+%5B%22they%22%5D+%5B%22missed%22%5D+%5B%22it%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22had%22%5D+%5B%22to%22%5D+%5B%22count%22%5D+%5B%22all%22%5D+%5B%22over%22%5D+%5B%22again%22%5D+%5B%5D+%5B%22The%22%5D+%5B%22whips%22%5D+%5B%22would%22%5D+%5B%22cut%22%5D+%5B%22the%22%5D+%5B%22flesh%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They used to get 25 whips . And if they missed it , they had to count all over again . The whips would cut the flesh . ', 'right': '', 'complete_match': 'They used to get 25 whips . And if they missed it , they had to count all over again . The whips would cut the flesh . ', 'testimony_id': 'usc_shoah_1496', 'shelfmark': ['USC Shoah Foundation 1496'], 'token_start': 5056, 'token_end': 5084}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"They used to get 25 whips. And if they missed it, they had to count all over again. The whips would cut the flesh.\"\n",
    "fragment_5['label']= \"They used to get 25 whips. And if they missed it, they had to count all over again. The whips would cut the flesh.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [\"whip\",\"work\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([lemma=\"whip\"][]{0,10}[lemma=\"work\"])|([lemma=\"work\"][]{0,10}[lemma=\"whip\"])\n"
     ]
    }
   ],
   "source": [
    "query = create_contextual_query(lemmas,context_length=10)\n",
    "print (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_term = \"work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = {}\n",
    "fragments['main_node'] = main_node\n",
    "fragments['mid_node'] = domain_term\n",
    "fragments['fragments'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22And%22%5D+%5B%22if%22%5D+%5B%22there%22%5D+%5B%22were%22%5D+%5B%22any%22%5D+%5B%22complaints%22%5D+%5B%22about%22%5D+%5B%22anybody%22%5D+%5B%22during%22%5D+%5B%22the%22%5D+%5B%22work%22%5D+%5B%22of%22%5D+%5B%22any%22%5D+%5B%22kind%22%5D+%5B%5D+%5B%22the%22%5D+%5B%22person%22%5D+%5B%22was%22%5D+%5B%22whipped%22%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'And if there were any complaints about anybody during the work of any kind , the person was whipped ', 'right': '', 'complete_match': 'And if there were any complaints about anybody during the work of any kind , the person was whipped ', 'testimony_id': 'irn504840', 'shelfmark': ['USHMM RG-50.030*0346'], 'token_start': 16270, 'token_end': 16289}\n"
     ]
    }
   ],
   "source": [
    "fragment_1 = {}\n",
    "fragment_1['original_sentence'] = \"And if there were any complaints about anybody during the work of any kind, the person was whipped\"\n",
    "fragment_1['label']=\"And if there were any complaints about anybody during the work of any kind, the person was whipped (..).\"\n",
    "indices = find_sentence_id(fragment_1['original_sentence'])\n",
    "fragment_1['start_sentence_index']=indices[0]\n",
    "fragment_1['end_sentence_index']=indices[1]\n",
    "fragment_1['media_offset'] = 0\n",
    "fragment_1['media_index'] = 0\n",
    "fragment_1['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22They%22%5D+%5B%22went%22%5D+%5B%22behind%22%5D+%5B%22us%22%5D+%5B%22and%22%5D+%5B%22if%22%5D+%5B%22we%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22work%22%5D+%5B%22hard%22%5D+%5B%22enough%22%5D+%5B%22to%22%5D+%5B%22suit%22%5D+%5B%22them%22%5D+%5B%22we%22%5D+%5B%22were%22%5D+%5B%22whipped%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'They went behind us and if we did n’t work hard enough to suit them we were whipped . ', 'right': '', 'complete_match': 'They went behind us and if we did n’t work hard enough to suit them we were whipped . ', 'testimony_id': 'irn510486', 'shelfmark': ['USHMM RG-50.322*0032'], 'token_start': 10858, 'token_end': 10877}\n"
     ]
    }
   ],
   "source": [
    "fragment_2 = {}\n",
    "fragment_2['original_sentence'] = \"They went behind us and if we didn’t work hard enough to suit them we were whipped.\"\n",
    "fragment_2['label']=\"They went behind us and if we didn’t work hard enough to suit them we were whipped.\"\n",
    "indices = find_sentence_id(fragment_2['original_sentence'])\n",
    "fragment_2['start_sentence_index']=indices[0]\n",
    "fragment_2['end_sentence_index']=indices[1]\n",
    "fragment_2['media_offset'] = 0\n",
    "fragment_2['media_index'] = 0\n",
    "fragment_2['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22You%22%5D+%5B%22were%22%5D+%5B%22not%22%5D+%5B%22allowed%22%5D+%5B%22to%22%5D+%5B%22pick%22%5D+%5B%22up%22%5D+%5B%22your%22%5D+%5B%22head%22%5D+%5B%22from%22%5D+%5B%22the%22%5D+%5B%22work%22%5D+%5B%22that%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22If%22%5D+%5B%22you%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22you%22%5D+%5B%22had%22%5D+%5B%22a%22%5D+%5B%22whip%22%5D+%5B%22around%22%5D+%5B%22your%22%5D+%5B%22neck%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'You were not allowed to pick up your head from the work that did . If you did , you had a whip around your neck . ', 'right': '', 'complete_match': 'You were not allowed to pick up your head from the work that did . If you did , you had a whip around your neck . ', 'testimony_id': 'usc_shoah_27443', 'shelfmark': ['USC Shoah Foundation 27443'], 'token_start': 9862, 'token_end': 9889}\n"
     ]
    }
   ],
   "source": [
    "fragment_3 = {}\n",
    "fragment_3['original_sentence'] = \"You were not allowed to pick up your head from the work that did. If you did, you had a whip around your neck.\"\n",
    "fragment_3['label']=\"You were not allowed to pick up your head from the work that did. If you did, you had a whip around your neck.\"\n",
    "indices = find_sentence_id(fragment_3['original_sentence'])\n",
    "fragment_3['start_sentence_index']=indices[0]\n",
    "fragment_3['end_sentence_index']=indices[1]\n",
    "fragment_3['media_offset'] = 0\n",
    "fragment_3['media_index'] = 0\n",
    "fragment_3['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22But%22%5D+%5B%22they%22%5D+%5B%22were%22%5D+%5B%22like%22%5D+%5B%5D%7B0%2C3%7D+%5B%22they%22%5D+%5B%22used%22%5D+%5B%22like%22%5D+%5B%22those%22%5D+%5B%22whips%22%5D+%5B%5D%7B0%2C3%7D+%5B%22work%22%5D+%5B%5D+%5B%22work%22%5D+%5B%5D+%5B%22work%22%5D+%5B%5D+%5B%22work%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': 'But they were like -- they used like those whips -- work , work , work , work . ', 'right': '', 'complete_match': 'But they were like -- they used like those whips -- work , work , work , work . ', 'testimony_id': 'usc_shoah_3474', 'shelfmark': ['USC Shoah Foundation 3474'], 'token_start': 3769, 'token_end': 3788}\n"
     ]
    }
   ],
   "source": [
    "fragment_4 = {}\n",
    "fragment_4['original_sentence'] = \"But they were like-- they used like those whips-- work, work, work, work.\"\n",
    "fragment_4['label']= \"But they were like-- they used like those whips-- work, work, work, work.\"\n",
    "indices = find_sentence_id(fragment_4['original_sentence'])\n",
    "fragment_4['start_sentence_index']=indices[0]\n",
    "fragment_4['end_sentence_index']=indices[1]\n",
    "fragment_4['media_offset'] = 0\n",
    "fragment_4['media_index'] = 0\n",
    "fragment_4['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8080/blacklab-server-2.1.0/lts/hits?patt=%5B%22you%22%5D+%5B%22did%22%5D+%5B%5D+%5B%22work%22%5D+%5B%22fast%22%5D+%5B%22enough%22%5D+%5B%5D+%5B%22they%22%5D+%5B%22come%22%5D+%5B%22with%22%5D+%5B%22a%22%5D+%5B%22whip%22%5D+%5B%22or%22%5D+%5B%22something%22%5D+%5B%22like%22%5D+%5B%22that%22%5D+%5B%5D&waitfortotal=true&outputformat=json&prettyprint=no&wordsaroundhit=0\n",
      "{'left': '', 'match_word': \"you did n't work fast enough , they come with a whip or something like that . \", 'right': '', 'complete_match': \"you did n't work fast enough , they come with a whip or something like that . \", 'testimony_id': 'usc_shoah_8072', 'shelfmark': ['USC Shoah Foundation 8072'], 'token_start': 18671, 'token_end': 18688}\n"
     ]
    }
   ],
   "source": [
    "fragment_5 = {}\n",
    "fragment_5['original_sentence'] = \"you didn't work fast enough, they come with a whip or something like that.\"\n",
    "fragment_5['label']= \"(..)you didn't work fast enough, they come with a whip or something like that.\"\n",
    "indices = find_sentence_id(fragment_5['original_sentence'])\n",
    "fragment_5['start_sentence_index']=indices[0]\n",
    "fragment_5['end_sentence_index']=indices[1]\n",
    "fragment_5['media_offset'] = 0\n",
    "fragment_5['media_index'] = 0\n",
    "fragment_5['testimony_id'] = indices[2]\n",
    "fragments['fragments'].append(fragment_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_testimonial_fragments(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
